{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7032355718c249698c43242b1569e8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_722714584d834d54868f54081b2f5458",
              "IPY_MODEL_7e638aea8e0e4cea8685a567f1424378",
              "IPY_MODEL_59a80e72bb4146279a60ac310efabff5"
            ],
            "layout": "IPY_MODEL_cfc3b4088f0547319cc97f4ddd3927f6"
          }
        },
        "722714584d834d54868f54081b2f5458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_113c2ef36a7a4aa9824529475cc9cd15",
            "placeholder": "​",
            "style": "IPY_MODEL_1301ad61aeeb4c9ca41a87cec3d80e7a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7e638aea8e0e4cea8685a567f1424378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2235267506c3479b95ac330143094714",
            "max": 373,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c68d00e1cc8430283ae029bea37fe78",
            "value": 373
          }
        },
        "59a80e72bb4146279a60ac310efabff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_945adfbbd5974a83985920f347e436b1",
            "placeholder": "​",
            "style": "IPY_MODEL_e65c3e8bfeeb429dbe22159222deeb90",
            "value": " 373/373 [00:00&lt;00:00, 16.2kB/s]"
          }
        },
        "cfc3b4088f0547319cc97f4ddd3927f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113c2ef36a7a4aa9824529475cc9cd15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1301ad61aeeb4c9ca41a87cec3d80e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2235267506c3479b95ac330143094714": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c68d00e1cc8430283ae029bea37fe78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "945adfbbd5974a83985920f347e436b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e65c3e8bfeeb429dbe22159222deeb90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0a9b4afba2b457f8cb8d8fb0da36942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f3ecfe57c56417eb4ebf67d1dea3b2d",
              "IPY_MODEL_872e539aa73f44cfb1b45e46b8af2ab6",
              "IPY_MODEL_bdc9cf2a820744e8988269c7e9060742"
            ],
            "layout": "IPY_MODEL_7fbf0a39f0b445b5af020018bb813d4e"
          }
        },
        "5f3ecfe57c56417eb4ebf67d1dea3b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca62c926763f4d8eae95aa1dee323496",
            "placeholder": "​",
            "style": "IPY_MODEL_45ad06874e7b40a1984da829bfd20bc5",
            "value": "vocab.txt: 100%"
          }
        },
        "872e539aa73f44cfb1b45e46b8af2ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e98ac1ae9bc42c4940986f77ae2156f",
            "max": 995526,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11a61ff19892431183f3ef45c241698b",
            "value": 995526
          }
        },
        "bdc9cf2a820744e8988269c7e9060742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13dcda64de1741bc92ff87d82b879c5a",
            "placeholder": "​",
            "style": "IPY_MODEL_66aafacf9818442fabbd1a5dfc26d1d9",
            "value": " 996k/996k [00:00&lt;00:00, 5.04MB/s]"
          }
        },
        "7fbf0a39f0b445b5af020018bb813d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca62c926763f4d8eae95aa1dee323496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45ad06874e7b40a1984da829bfd20bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e98ac1ae9bc42c4940986f77ae2156f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11a61ff19892431183f3ef45c241698b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13dcda64de1741bc92ff87d82b879c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66aafacf9818442fabbd1a5dfc26d1d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7537f476d62c4edfa7ada4f4677245d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6ceb2945b154c6fb7a682a5e9c153fa",
              "IPY_MODEL_9c35160a77604ee99bc91ea61dd3acee",
              "IPY_MODEL_5d69ac4537404207990ab49f12a3e750"
            ],
            "layout": "IPY_MODEL_45d973762de04b12b4cee76138176838"
          }
        },
        "a6ceb2945b154c6fb7a682a5e9c153fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0f020db08834e7c9bd0ac05223e0e19",
            "placeholder": "​",
            "style": "IPY_MODEL_4b1fd450e1914a888f5634ff7b00ccf6",
            "value": "tokenizer.json: 100%"
          }
        },
        "9c35160a77604ee99bc91ea61dd3acee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a0d8804514e4463bffae4be493209e5",
            "max": 2919615,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d67422af1fa64012854adad67a22a8f9",
            "value": 2919615
          }
        },
        "5d69ac4537404207990ab49f12a3e750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_562e75957f014469a0231272a08845aa",
            "placeholder": "​",
            "style": "IPY_MODEL_4fd60da48eb24bc5aa9ef3ac62bcd6be",
            "value": " 2.92M/2.92M [00:00&lt;00:00, 11.2MB/s]"
          }
        },
        "45d973762de04b12b4cee76138176838": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0f020db08834e7c9bd0ac05223e0e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1fd450e1914a888f5634ff7b00ccf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a0d8804514e4463bffae4be493209e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d67422af1fa64012854adad67a22a8f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "562e75957f014469a0231272a08845aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fd60da48eb24bc5aa9ef3ac62bcd6be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6104d41624c64eb69210c3dbe71fdda3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b5a97882b43475b9b7ff8953c3ef1b1",
              "IPY_MODEL_ef6d7b62b6b4485ea20b6b228c664ed9",
              "IPY_MODEL_d5d3530882864020bac4547e4605197f"
            ],
            "layout": "IPY_MODEL_4c98faf916b54415b050b24d4e905ec2"
          }
        },
        "2b5a97882b43475b9b7ff8953c3ef1b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5d354b41ac4e1c996d38a3aea564e6",
            "placeholder": "​",
            "style": "IPY_MODEL_69f1570053294edaa25202a89ee35369",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "ef6d7b62b6b4485ea20b6b228c664ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9361c1921b5c44e0b2c2bef222aa569c",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e8ad080c6544d2ebf8b7f0eae3612fb",
            "value": 125
          }
        },
        "d5d3530882864020bac4547e4605197f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ea299e332b64e1ab205ded794c7e3b6",
            "placeholder": "​",
            "style": "IPY_MODEL_232df0e9e80e40d09a1fa27362ead5fc",
            "value": " 125/125 [00:00&lt;00:00, 10.2kB/s]"
          }
        },
        "4c98faf916b54415b050b24d4e905ec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c5d354b41ac4e1c996d38a3aea564e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69f1570053294edaa25202a89ee35369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9361c1921b5c44e0b2c2bef222aa569c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e8ad080c6544d2ebf8b7f0eae3612fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ea299e332b64e1ab205ded794c7e3b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "232df0e9e80e40d09a1fa27362ead5fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6493f02ad3e644dda7b217063d0b45bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_875904bfbe1b4ad09d297a83acb27622",
              "IPY_MODEL_3d6947214d16470bbc2e1e4547292bc0",
              "IPY_MODEL_0645b9158f264822a54cf3a9527ddbbc"
            ],
            "layout": "IPY_MODEL_05d12aaf5b7841e19f46419bef0030ee"
          }
        },
        "875904bfbe1b4ad09d297a83acb27622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_922bc33d383a4a56b44c0972b517ab68",
            "placeholder": "​",
            "style": "IPY_MODEL_fb4e906f36394329a0e0ac0d5ca58614",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3d6947214d16470bbc2e1e4547292bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a295b2d2798b41e08707fec31d2e3f3d",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b098d01b0c774239981535bf2ff97111",
            "value": 25
          }
        },
        "0645b9158f264822a54cf3a9527ddbbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_131503859e634097a841f66192b34cbc",
            "placeholder": "​",
            "style": "IPY_MODEL_a7fef936064646349761c8ab40707610",
            "value": " 25.0/25.0 [00:00&lt;00:00, 2.08kB/s]"
          }
        },
        "05d12aaf5b7841e19f46419bef0030ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "922bc33d383a4a56b44c0972b517ab68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb4e906f36394329a0e0ac0d5ca58614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a295b2d2798b41e08707fec31d2e3f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b098d01b0c774239981535bf2ff97111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "131503859e634097a841f66192b34cbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7fef936064646349761c8ab40707610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9d1137d7fb44008b3ce580a6cdbc3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fb8291bc3df4819bfdc8958350db4f3",
              "IPY_MODEL_c71116a9119b4e568cefcd6b8013603e",
              "IPY_MODEL_e5210fd37cab4792af87bdc56d7d3236"
            ],
            "layout": "IPY_MODEL_7ff4e5b1df43451e9a1a01301aa55c66"
          }
        },
        "0fb8291bc3df4819bfdc8958350db4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_711983895ea2473fa054eb1b90413530",
            "placeholder": "​",
            "style": "IPY_MODEL_bb2bc29c2ba340879cbda20195cb17cb",
            "value": "config.json: 100%"
          }
        },
        "c71116a9119b4e568cefcd6b8013603e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef083008d47640d9b68b65ad8491b3a8",
            "max": 684,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d1105545eb3425b97f4b30f479b9fd5",
            "value": 684
          }
        },
        "e5210fd37cab4792af87bdc56d7d3236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7555460763f64cdaabff6c09a1814ff6",
            "placeholder": "​",
            "style": "IPY_MODEL_97e427a387fd46a0a2345fb11ad281a9",
            "value": " 684/684 [00:00&lt;00:00, 45.7kB/s]"
          }
        },
        "7ff4e5b1df43451e9a1a01301aa55c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711983895ea2473fa054eb1b90413530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb2bc29c2ba340879cbda20195cb17cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef083008d47640d9b68b65ad8491b3a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d1105545eb3425b97f4b30f479b9fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7555460763f64cdaabff6c09a1814ff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97e427a387fd46a0a2345fb11ad281a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c0d7e35eaad4767a2cdacf393b9b994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13857956f50949f0bf90547dcfd3cd34",
              "IPY_MODEL_d499a5fe2928435692de2a1a08fba4f3",
              "IPY_MODEL_e801ffedab3344e99a8067361b25dc2a"
            ],
            "layout": "IPY_MODEL_a89449cc0c904de58b9a4d2a6d0de806"
          }
        },
        "13857956f50949f0bf90547dcfd3cd34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9a820699fa74ca7bc0d01b3e1e783d8",
            "placeholder": "​",
            "style": "IPY_MODEL_c8de952bf59c44fdb8cdd553d9a856bb",
            "value": "spiece.model: 100%"
          }
        },
        "d499a5fe2928435692de2a1a08fba4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_468d7103c43542a98dd6256b583d2fd0",
            "max": 760289,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d37c8705051d416a98a7ae1428315e23",
            "value": 760289
          }
        },
        "e801ffedab3344e99a8067361b25dc2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32ea2b394ef34146b72fbb03e58390f6",
            "placeholder": "​",
            "style": "IPY_MODEL_50421cb3f07046bfa349c66e89d6d1f7",
            "value": " 760k/760k [00:00&lt;00:00, 17.8MB/s]"
          }
        },
        "a89449cc0c904de58b9a4d2a6d0de806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9a820699fa74ca7bc0d01b3e1e783d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8de952bf59c44fdb8cdd553d9a856bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "468d7103c43542a98dd6256b583d2fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37c8705051d416a98a7ae1428315e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32ea2b394ef34146b72fbb03e58390f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50421cb3f07046bfa349c66e89d6d1f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85e4b67fd0314938bead37023dbf819f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10b9ebc19bd04c95ab4566d6420ca40f",
              "IPY_MODEL_c1735b17cd7942c0930340e739be904b",
              "IPY_MODEL_6ed292820d6e4fdeb03cf6edae59788b"
            ],
            "layout": "IPY_MODEL_6ce25c66cadc4f12af9f7d54cc8fe2dc"
          }
        },
        "10b9ebc19bd04c95ab4566d6420ca40f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20119087de364bfab19b6c4ee517bcbd",
            "placeholder": "​",
            "style": "IPY_MODEL_76f6caf9c6134ca98dec0f94df93f6f4",
            "value": "tokenizer.json: 100%"
          }
        },
        "c1735b17cd7942c0930340e739be904b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce5dd2e167074119976b52a3cd4270e9",
            "max": 1312669,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_311e2f40b0de4b6c89896333fbab0d2d",
            "value": 1312669
          }
        },
        "6ed292820d6e4fdeb03cf6edae59788b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17f07a93688448218acfcbbdf4130c13",
            "placeholder": "​",
            "style": "IPY_MODEL_0015b5c0660e425c812a2570bc49c402",
            "value": " 1.31M/1.31M [00:00&lt;00:00, 4.04MB/s]"
          }
        },
        "6ce25c66cadc4f12af9f7d54cc8fe2dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20119087de364bfab19b6c4ee517bcbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76f6caf9c6134ca98dec0f94df93f6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce5dd2e167074119976b52a3cd4270e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "311e2f40b0de4b6c89896333fbab0d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17f07a93688448218acfcbbdf4130c13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0015b5c0660e425c812a2570bc49c402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4edfb6e775f5466b9e1fb068750c9c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3681476488544e61965bf98c63070b0c",
              "IPY_MODEL_055f8dc311b04c70b65efb51cdd51ca1",
              "IPY_MODEL_94303e647ee54731a9ea72c44b341be8"
            ],
            "layout": "IPY_MODEL_f21520b665e04421968cfbea30b01882"
          }
        },
        "3681476488544e61965bf98c63070b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b351f8a7724f48a8aa129e60ce4bdc",
            "placeholder": "​",
            "style": "IPY_MODEL_fae1ef33f8c7427aab2d71fbaac2fae6",
            "value": "config.json: 100%"
          }
        },
        "055f8dc311b04c70b65efb51cdd51ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4217df52a76c4b59860cf49c9376f277",
            "max": 759,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ded2ecfce1564065a94bfa1c20296868",
            "value": 759
          }
        },
        "94303e647ee54731a9ea72c44b341be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c5158870ad42ca8a9f5bdc1b31476c",
            "placeholder": "​",
            "style": "IPY_MODEL_da7c4160cd62475ba6823349001550b8",
            "value": " 759/759 [00:00&lt;00:00, 57.3kB/s]"
          }
        },
        "f21520b665e04421968cfbea30b01882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b351f8a7724f48a8aa129e60ce4bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fae1ef33f8c7427aab2d71fbaac2fae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4217df52a76c4b59860cf49c9376f277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ded2ecfce1564065a94bfa1c20296868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70c5158870ad42ca8a9f5bdc1b31476c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da7c4160cd62475ba6823349001550b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f3afa38ca0044da81aafb35c731ae6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a913ba9d55054f43bae070d8a650adab",
              "IPY_MODEL_d8ffc63ef61b4331829942e8996bc1f9",
              "IPY_MODEL_dcd79a3f7e5f43608824ebd88f749a91"
            ],
            "layout": "IPY_MODEL_1b1a1a5d087141568f91f5fc6ff3b404"
          }
        },
        "a913ba9d55054f43bae070d8a650adab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56e019159b8848a78217f25723dbc046",
            "placeholder": "​",
            "style": "IPY_MODEL_e202fa6e1ebc40be9548f71f6286a960",
            "value": "model.safetensors: 100%"
          }
        },
        "d8ffc63ef61b4331829942e8996bc1f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b64b9d61604483c8c33013f7b81f5e4",
            "max": 541320452,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2abae425e66c46b694d10340a6b4790f",
            "value": 541320452
          }
        },
        "dcd79a3f7e5f43608824ebd88f749a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_956fed4991144fbbb838eebf942cb9c6",
            "placeholder": "​",
            "style": "IPY_MODEL_ad39be40f0ee44489c8d890fe5135810",
            "value": " 541M/541M [00:02&lt;00:00, 264MB/s]"
          }
        },
        "1b1a1a5d087141568f91f5fc6ff3b404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e019159b8848a78217f25723dbc046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e202fa6e1ebc40be9548f71f6286a960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b64b9d61604483c8c33013f7b81f5e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2abae425e66c46b694d10340a6b4790f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "956fed4991144fbbb838eebf942cb9c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad39be40f0ee44489c8d890fe5135810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f40e92ec24642e8a289d2c5c1b02eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d3a31f3182d4ca9bf5ad084d2533e01",
              "IPY_MODEL_86cdf50d7beb427fab6033e7c7972edd",
              "IPY_MODEL_bef8a86ffe8243029f513d15206cf3b7"
            ],
            "layout": "IPY_MODEL_98dbd8e7421d40e1aaa63053014966a2"
          }
        },
        "3d3a31f3182d4ca9bf5ad084d2533e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cb66fc91bc84277ba30d08e4c09ab31",
            "placeholder": "​",
            "style": "IPY_MODEL_4d3fb7ff4d524f95ae1b4627159df42d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "86cdf50d7beb427fab6033e7c7972edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b8d0894c4724f769956356984064775",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2738df88fe30487ead8bf7cfede20db2",
            "value": 29
          }
        },
        "bef8a86ffe8243029f513d15206cf3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20c1cbf2ae194e2da78d4a3d42352d99",
            "placeholder": "​",
            "style": "IPY_MODEL_b393624f55b44e968bf815c9967bdd62",
            "value": " 29.0/29.0 [00:00&lt;00:00, 729B/s]"
          }
        },
        "98dbd8e7421d40e1aaa63053014966a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cb66fc91bc84277ba30d08e4c09ab31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d3fb7ff4d524f95ae1b4627159df42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b8d0894c4724f769956356984064775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2738df88fe30487ead8bf7cfede20db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20c1cbf2ae194e2da78d4a3d42352d99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b393624f55b44e968bf815c9967bdd62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7de1702c43349eb8625fc930146534b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb6d5bd7aed64006921bda04719ea38b",
              "IPY_MODEL_39f1dd7df39e40f783158435d64ca571",
              "IPY_MODEL_923095fdbc0648649f82a6da4f95c651"
            ],
            "layout": "IPY_MODEL_fafd1cdc4faf423fb1bf275a173c9991"
          }
        },
        "bb6d5bd7aed64006921bda04719ea38b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b066dc790944b1a6c476b39004163b",
            "placeholder": "​",
            "style": "IPY_MODEL_24e60449f11d4451bf96d5e31c5f765a",
            "value": "config.json: 100%"
          }
        },
        "39f1dd7df39e40f783158435d64ca571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e31ce0e3e0c24efab19755146557c0c3",
            "max": 473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08d8d8f707774ff187a45d6bdca009c1",
            "value": 473
          }
        },
        "923095fdbc0648649f82a6da4f95c651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d7b726b24142989457560cac919754",
            "placeholder": "​",
            "style": "IPY_MODEL_d9b7ec92f65a434f865a4bdaec2fca07",
            "value": " 473/473 [00:00&lt;00:00, 20.6kB/s]"
          }
        },
        "fafd1cdc4faf423fb1bf275a173c9991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44b066dc790944b1a6c476b39004163b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24e60449f11d4451bf96d5e31c5f765a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e31ce0e3e0c24efab19755146557c0c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08d8d8f707774ff187a45d6bdca009c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7d7b726b24142989457560cac919754": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9b7ec92f65a434f865a4bdaec2fca07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96ad30b108664fcc98702b300c2fe35d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b069bc47f9a4702aa9056303a01a19f",
              "IPY_MODEL_7ffa5e6e63a24ec19112fe926b56c11f",
              "IPY_MODEL_14cc0c337bfb4b13ac8b2ef3a1ecd79f"
            ],
            "layout": "IPY_MODEL_ddf3bda8826849f28c2336710e5dd3d9"
          }
        },
        "1b069bc47f9a4702aa9056303a01a19f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a363c10de174b7a8069f20fe9499f64",
            "placeholder": "​",
            "style": "IPY_MODEL_bd8d20f4c6f8406596af7d17a8f2dff7",
            "value": "vocab.txt: 100%"
          }
        },
        "7ffa5e6e63a24ec19112fe926b56c11f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd25814dfd134ce58fb66a2ce813a914",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb824df5e0bd462bbe840ef769b926a3",
            "value": 213450
          }
        },
        "14cc0c337bfb4b13ac8b2ef3a1ecd79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a05df1cfc5b34ff0b9b15a656c49685f",
            "placeholder": "​",
            "style": "IPY_MODEL_f1bf817ca15a417f93854b08bd6b97c0",
            "value": " 213k/213k [00:00&lt;00:00, 1.67MB/s]"
          }
        },
        "ddf3bda8826849f28c2336710e5dd3d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a363c10de174b7a8069f20fe9499f64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd8d20f4c6f8406596af7d17a8f2dff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd25814dfd134ce58fb66a2ce813a914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb824df5e0bd462bbe840ef769b926a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a05df1cfc5b34ff0b9b15a656c49685f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1bf817ca15a417f93854b08bd6b97c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef5c10c3e4f84754b2035421d11ae21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff94b2268aff448caaa4e700fdb0246f",
              "IPY_MODEL_53f13d4f6a2d4c36bfe10f53d70008b8",
              "IPY_MODEL_b12b8209d2c84841a8c5508b05084905"
            ],
            "layout": "IPY_MODEL_2cbc8e28a6b047908010feb9402ee245"
          }
        },
        "ff94b2268aff448caaa4e700fdb0246f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaab07d484414d73adb4f06893d4dcd9",
            "placeholder": "​",
            "style": "IPY_MODEL_d661a4922b9d4e80828c1c7985ea903c",
            "value": "tokenizer.json: 100%"
          }
        },
        "53f13d4f6a2d4c36bfe10f53d70008b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9634b1e138544d33973ce219382a2888",
            "max": 435797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cda65cfffc7a4b30ab755f01fade070a",
            "value": 435797
          }
        },
        "b12b8209d2c84841a8c5508b05084905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_055711691a954d9d9f029b5be3a07db5",
            "placeholder": "​",
            "style": "IPY_MODEL_25f8787dd5514b4bba4921d99c9ab050",
            "value": " 436k/436k [00:00&lt;00:00, 3.38MB/s]"
          }
        },
        "2cbc8e28a6b047908010feb9402ee245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaab07d484414d73adb4f06893d4dcd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d661a4922b9d4e80828c1c7985ea903c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9634b1e138544d33973ce219382a2888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cda65cfffc7a4b30ab755f01fade070a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "055711691a954d9d9f029b5be3a07db5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f8787dd5514b4bba4921d99c9ab050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7a63c3c056f4d55ac6bdbc5f013b3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_338da88b5d5e463f9696d16cf4147fa9",
              "IPY_MODEL_338aa283492e4460ba4c539c0834a8b6",
              "IPY_MODEL_d2005e49e86142759ad62cfc8954faaf"
            ],
            "layout": "IPY_MODEL_298e578810b5421f8265f9b39b10e5c2"
          }
        },
        "338da88b5d5e463f9696d16cf4147fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6c3649e8f914395ab23ec3686cec349",
            "placeholder": "​",
            "style": "IPY_MODEL_623cbbf19da44a6595f18938187953bd",
            "value": "model.safetensors: 100%"
          }
        },
        "338aa283492e4460ba4c539c0834a8b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_591aaaa364e74b9e8b0514af7a142de5",
            "max": 260782156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9496fc5c7cb40fcabe2fce52181cb30",
            "value": 260782156
          }
        },
        "d2005e49e86142759ad62cfc8954faaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_275c349a8a9044a99263a17c0486cd31",
            "placeholder": "​",
            "style": "IPY_MODEL_6506bc49f55a4df78fe04ca34006e1e2",
            "value": " 261M/261M [00:01&lt;00:00, 183MB/s]"
          }
        },
        "298e578810b5421f8265f9b39b10e5c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c3649e8f914395ab23ec3686cec349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623cbbf19da44a6595f18938187953bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "591aaaa364e74b9e8b0514af7a142de5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9496fc5c7cb40fcabe2fce52181cb30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "275c349a8a9044a99263a17c0486cd31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6506bc49f55a4df78fe04ca34006e1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FCYOZdFfUUFo"
      },
      "outputs": [],
      "source": [
        "#\n",
        "from transformers import pipeline,AutoModel,AutoTokenizer\n",
        "from transformers.pipelines import PIPELINE_REGISTRY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anItFxfyVDcO",
        "outputId": "495f8645-8ed4-4f43-dc3e-e6f96ceb2740"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__annotations__',\n",
              " '__builtins__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__closure__',\n",
              " '__code__',\n",
              " '__defaults__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__get__',\n",
              " '__getattribute__',\n",
              " '__globals__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__kwdefaults__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__name__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__qualname__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4xjQeAsY7aR",
        "outputId": "d4be9b1a-19a5-41ab-c56f-cc58653c8cce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function pipeline in module transformers.pipelines:\n",
            "\n",
            "pipeline(task: str = None, model: Union[str, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel'), NoneType] = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, ForwardRef('PreTrainedTokenizerFast'), NoneType] = None, feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None, image_processor: Union[str, transformers.image_processing_utils.BaseImageProcessor, NoneType] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Union[str, bool, NoneType] = None, device: Union[int, str, ForwardRef('torch.device'), NoneType] = None, device_map=None, torch_dtype=None, trust_remote_code: Optional[bool] = None, model_kwargs: Dict[str, Any] = None, pipeline_class: Optional[Any] = None, **kwargs) -> transformers.pipelines.base.Pipeline\n",
            "    Utility factory method to build a [`Pipeline`].\n",
            "    \n",
            "    Pipelines are made of:\n",
            "    \n",
            "        - A [tokenizer](tokenizer) in charge of mapping raw textual input to token.\n",
            "        - A [model](model) to make predictions from the inputs.\n",
            "        - Some (optional) post processing for enhancing model's output.\n",
            "    \n",
            "    Args:\n",
            "        task (`str`):\n",
            "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
            "    \n",
            "            - `\"audio-classification\"`: will return a [`AudioClassificationPipeline`].\n",
            "            - `\"automatic-speech-recognition\"`: will return a [`AutomaticSpeechRecognitionPipeline`].\n",
            "            - `\"conversational\"`: will return a [`ConversationalPipeline`].\n",
            "            - `\"depth-estimation\"`: will return a [`DepthEstimationPipeline`].\n",
            "            - `\"document-question-answering\"`: will return a [`DocumentQuestionAnsweringPipeline`].\n",
            "            - `\"feature-extraction\"`: will return a [`FeatureExtractionPipeline`].\n",
            "            - `\"fill-mask\"`: will return a [`FillMaskPipeline`]:.\n",
            "            - `\"image-classification\"`: will return a [`ImageClassificationPipeline`].\n",
            "            - `\"image-feature-extraction\"`: will return an [`ImageFeatureExtractionPipeline`].\n",
            "            - `\"image-segmentation\"`: will return a [`ImageSegmentationPipeline`].\n",
            "            - `\"image-to-image\"`: will return a [`ImageToImagePipeline`].\n",
            "            - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n",
            "            - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`].\n",
            "            - `\"object-detection\"`: will return a [`ObjectDetectionPipeline`].\n",
            "            - `\"question-answering\"`: will return a [`QuestionAnsweringPipeline`].\n",
            "            - `\"summarization\"`: will return a [`SummarizationPipeline`].\n",
            "            - `\"table-question-answering\"`: will return a [`TableQuestionAnsweringPipeline`].\n",
            "            - `\"text2text-generation\"`: will return a [`Text2TextGenerationPipeline`].\n",
            "            - `\"text-classification\"` (alias `\"sentiment-analysis\"` available): will return a\n",
            "              [`TextClassificationPipeline`].\n",
            "            - `\"text-generation\"`: will return a [`TextGenerationPipeline`]:.\n",
            "            - `\"text-to-audio\"` (alias `\"text-to-speech\"` available): will return a [`TextToAudioPipeline`]:.\n",
            "            - `\"token-classification\"` (alias `\"ner\"` available): will return a [`TokenClassificationPipeline`].\n",
            "            - `\"translation\"`: will return a [`TranslationPipeline`].\n",
            "            - `\"translation_xx_to_yy\"`: will return a [`TranslationPipeline`].\n",
            "            - `\"video-classification\"`: will return a [`VideoClassificationPipeline`].\n",
            "            - `\"visual-question-answering\"`: will return a [`VisualQuestionAnsweringPipeline`].\n",
            "            - `\"zero-shot-classification\"`: will return a [`ZeroShotClassificationPipeline`].\n",
            "            - `\"zero-shot-image-classification\"`: will return a [`ZeroShotImageClassificationPipeline`].\n",
            "            - `\"zero-shot-audio-classification\"`: will return a [`ZeroShotAudioClassificationPipeline`].\n",
            "            - `\"zero-shot-object-detection\"`: will return a [`ZeroShotObjectDetectionPipeline`].\n",
            "    \n",
            "        model (`str` or [`PreTrainedModel`] or [`TFPreTrainedModel`], *optional*):\n",
            "            The model that will be used by the pipeline to make predictions. This can be a model identifier or an\n",
            "            actual instance of a pretrained model inheriting from [`PreTrainedModel`] (for PyTorch) or\n",
            "            [`TFPreTrainedModel`] (for TensorFlow).\n",
            "    \n",
            "            If not provided, the default for the `task` will be loaded.\n",
            "        config (`str` or [`PretrainedConfig`], *optional*):\n",
            "            The configuration that will be used by the pipeline to instantiate the model. This can be a model\n",
            "            identifier or an actual pretrained model configuration inheriting from [`PretrainedConfig`].\n",
            "    \n",
            "            If not provided, the default configuration file for the requested model will be used. That means that if\n",
            "            `model` is given, its default configuration will be used. However, if `model` is not supplied, this\n",
            "            `task`'s default model's config is used instead.\n",
            "        tokenizer (`str` or [`PreTrainedTokenizer`], *optional*):\n",
            "            The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
            "            identifier or an actual pretrained tokenizer inheriting from [`PreTrainedTokenizer`].\n",
            "    \n",
            "            If not provided, the default tokenizer for the given `model` will be loaded (if it is a string). If `model`\n",
            "            is not specified or not a string, then the default tokenizer for `config` is loaded (if it is a string).\n",
            "            However, if `config` is also not given or not a string, then the default tokenizer for the given `task`\n",
            "            will be loaded.\n",
            "        feature_extractor (`str` or [`PreTrainedFeatureExtractor`], *optional*):\n",
            "            The feature extractor that will be used by the pipeline to encode data for the model. This can be a model\n",
            "            identifier or an actual pretrained feature extractor inheriting from [`PreTrainedFeatureExtractor`].\n",
            "    \n",
            "            Feature extractors are used for non-NLP models, such as Speech or Vision models as well as multi-modal\n",
            "            models. Multi-modal models will also require a tokenizer to be passed.\n",
            "    \n",
            "            If not provided, the default feature extractor for the given `model` will be loaded (if it is a string). If\n",
            "            `model` is not specified or not a string, then the default feature extractor for `config` is loaded (if it\n",
            "            is a string). However, if `config` is also not given or not a string, then the default feature extractor\n",
            "            for the given `task` will be loaded.\n",
            "        framework (`str`, *optional*):\n",
            "            The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
            "            installed.\n",
            "    \n",
            "            If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
            "            both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
            "            provided.\n",
            "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
            "            When passing a task name or a string model identifier: The specific model version to use. It can be a\n",
            "            branch name, a tag name, or a commit id, since we use a git-based system for storing models and other\n",
            "            artifacts on huggingface.co, so `revision` can be any identifier allowed by git.\n",
            "        use_fast (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not to use a Fast tokenizer if possible (a [`PreTrainedTokenizerFast`]).\n",
            "        use_auth_token (`str` or *bool*, *optional*):\n",
            "            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
            "            when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
            "        device (`int` or `str` or `torch.device`):\n",
            "            Defines the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank like `1`) on which this\n",
            "            pipeline will be allocated.\n",
            "        device_map (`str` or `Dict[str, Union[int, str, torch.device]`, *optional*):\n",
            "            Sent directly as `model_kwargs` (just a simpler shortcut). When `accelerate` library is present, set\n",
            "            `device_map=\"auto\"` to compute the most optimized `device_map` automatically (see\n",
            "            [here](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)\n",
            "            for more information).\n",
            "    \n",
            "            <Tip warning={true}>\n",
            "    \n",
            "            Do not use `device_map` AND `device` at the same time as they will conflict\n",
            "    \n",
            "            </Tip>\n",
            "    \n",
            "        torch_dtype (`str` or `torch.dtype`, *optional*):\n",
            "            Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
            "            (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`).\n",
            "        trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to allow for custom code defined on the Hub in their own modeling, configuration,\n",
            "            tokenization or even pipeline files. This option should only be set to `True` for repositories you trust\n",
            "            and in which you have read the code, as it will execute code present on the Hub on your local machine.\n",
            "        model_kwargs (`Dict[str, Any]`, *optional*):\n",
            "            Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,\n",
            "            **model_kwargs)` function.\n",
            "        kwargs (`Dict[str, Any]`, *optional*):\n",
            "            Additional keyword arguments passed along to the specific pipeline init (see the documentation for the\n",
            "            corresponding pipeline class for possible values).\n",
            "    \n",
            "    Returns:\n",
            "        [`Pipeline`]: A suitable pipeline for the task.\n",
            "    \n",
            "    Examples:\n",
            "    \n",
            "    ```python\n",
            "    >>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
            "    \n",
            "    >>> # Sentiment analysis pipeline\n",
            "    >>> analyzer = pipeline(\"sentiment-analysis\")\n",
            "    \n",
            "    >>> # Question answering pipeline, specifying the checkpoint identifier\n",
            "    >>> oracle = pipeline(\n",
            "    ...     \"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", tokenizer=\"google-bert/bert-base-cased\"\n",
            "    ... )\n",
            "    \n",
            "    >>> # Named entity recognition pipeline, passing in a specific model and tokenizer\n",
            "    >>> model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
            "    >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
            "    >>> recognizer = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
            "    ```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(PIPELINE_REGISTRY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnrpXhPNaat7",
        "outputId": "d78ae84f-f6c9-4089-bbcb-b895d27c201f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on PipelineRegistry in module transformers.pipelines.base object:\n",
            "\n",
            "class PipelineRegistry(builtins.object)\n",
            " |  PipelineRegistry(supported_tasks: Dict[str, Any], task_aliases: Dict[str, str]) -> None\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, supported_tasks: Dict[str, Any], task_aliases: Dict[str, str]) -> None\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  check_task(self, task: str) -> Tuple[str, Dict, Any]\n",
            " |  \n",
            " |  get_supported_tasks(self) -> List[str]\n",
            " |  \n",
            " |  register_pipeline(self, task: str, pipeline_class: type, pt_model: Union[type, Tuple[type], NoneType] = None, tf_model: Union[type, Tuple[type], NoneType] = None, default: Optional[Dict] = None, type: Optional[str] = None) -> None\n",
            " |  \n",
            " |  to_dict(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(PIPELINE_REGISTRY.supported_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFWzvhFWbTw_",
        "outputId": "14745fa5-d836-4108-f115-ae1c7e424ae7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on dict object:\n",
            "\n",
            "class dict(object)\n",
            " |  dict() -> new empty dictionary\n",
            " |  dict(mapping) -> new dictionary initialized from a mapping object's\n",
            " |      (key, value) pairs\n",
            " |  dict(iterable) -> new dictionary initialized as if via:\n",
            " |      d = {}\n",
            " |      for k, v in iterable:\n",
            " |          d[k] = v\n",
            " |  dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
            " |      in the keyword argument list.  For example:  dict(one=1, two=2)\n",
            " |  \n",
            " |  Built-in subclasses:\n",
            " |      StgDict\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      True if the dictionary has the specified key, else False.\n",
            " |  \n",
            " |  __delitem__(self, key, /)\n",
            " |      Delete self[key].\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __getitem__(...)\n",
            " |      x.__getitem__(y) <==> x[y]\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __init__(self, /, *args, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __ior__(self, value, /)\n",
            " |      Return self|=value.\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __or__(self, value, /)\n",
            " |      Return self|value.\n",
            " |  \n",
            " |  __repr__(self, /)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __reversed__(self, /)\n",
            " |      Return a reverse iterator over the dict keys.\n",
            " |  \n",
            " |  __ror__(self, value, /)\n",
            " |      Return value|self.\n",
            " |  \n",
            " |  __setitem__(self, key, value, /)\n",
            " |      Set self[key] to value.\n",
            " |  \n",
            " |  __sizeof__(...)\n",
            " |      D.__sizeof__() -> size of D in memory, in bytes\n",
            " |  \n",
            " |  clear(...)\n",
            " |      D.clear() -> None.  Remove all items from D.\n",
            " |  \n",
            " |  copy(...)\n",
            " |      D.copy() -> a shallow copy of D\n",
            " |  \n",
            " |  get(self, key, default=None, /)\n",
            " |      Return the value for key if key is in the dictionary, else default.\n",
            " |  \n",
            " |  items(...)\n",
            " |      D.items() -> a set-like object providing a view on D's items\n",
            " |  \n",
            " |  keys(...)\n",
            " |      D.keys() -> a set-like object providing a view on D's keys\n",
            " |  \n",
            " |  pop(...)\n",
            " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
            " |      \n",
            " |      If the key is not found, return the default if given; otherwise,\n",
            " |      raise a KeyError.\n",
            " |  \n",
            " |  popitem(self, /)\n",
            " |      Remove and return a (key, value) pair as a 2-tuple.\n",
            " |      \n",
            " |      Pairs are returned in LIFO (last-in, first-out) order.\n",
            " |      Raises KeyError if the dict is empty.\n",
            " |  \n",
            " |  setdefault(self, key, default=None, /)\n",
            " |      Insert key with a value of default if key is not in the dictionary.\n",
            " |      \n",
            " |      Return the value for key if key is in the dictionary, else default.\n",
            " |  \n",
            " |  update(...)\n",
            " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
            " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
            " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
            " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
            " |  \n",
            " |  values(...)\n",
            " |      D.values() -> an object providing a view on D's values\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  __class_getitem__(...) from builtins.type\n",
            " |      See PEP 585\n",
            " |  \n",
            " |  fromkeys(iterable, value=None, /) from builtins.type\n",
            " |      Create a new dictionary with keys from iterable and values set to value.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __hash__ = None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PIPELINE_REGISTRY.supported_tasks.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vds7svcjbjcI",
        "outputId": "61a2927e-dbfa-4716-c750-5efcc91328fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['audio-classification', 'automatic-speech-recognition', 'text-to-audio', 'feature-extraction', 'text-classification', 'token-classification', 'question-answering', 'table-question-answering', 'visual-question-answering', 'document-question-answering', 'fill-mask', 'summarization', 'translation', 'text2text-generation', 'text-generation', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-audio-classification', 'conversational', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-text', 'object-detection', 'zero-shot-object-detection', 'depth-estimation', 'video-classification', 'mask-generation', 'image-to-image'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(AutoModel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25MoYVDhVHBw",
        "outputId": "2e8b5562-61df-412e-a573-26f9387e5041"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_model_mapping',\n",
              " 'from_config',\n",
              " 'from_pretrained',\n",
              " 'register']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(AutoModel.from_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ9Cx36vVuRq",
        "outputId": "f5660eba-6bae-48db-8996-ad1119aa49b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method from_config in module transformers.models.auto.auto_factory:\n",
            "\n",
            "from_config(**kwargs) method of builtins.type instance\n",
            "    Instantiates one of the base model classes of the library from a configuration.\n",
            "    \n",
            "    Note:\n",
            "        Loading a model from its configuration file does **not** load the model weights. It only affects the\n",
            "        model's configuration. Use [`~AutoModel.from_pretrained`] to load the model weights.\n",
            "    \n",
            "    Args:\n",
            "        config ([`PretrainedConfig`]):\n",
            "            The model class to instantiate is selected based on the configuration class:\n",
            "    \n",
            "            - [`ASTConfig`] configuration class: [`ASTModel`] (Audio Spectrogram Transformer model)\n",
            "            - [`AlbertConfig`] configuration class: [`AlbertModel`] (ALBERT model)\n",
            "            - [`AlignConfig`] configuration class: [`AlignModel`] (ALIGN model)\n",
            "            - [`AltCLIPConfig`] configuration class: [`AltCLIPModel`] (AltCLIP model)\n",
            "            - [`AutoformerConfig`] configuration class: [`AutoformerModel`] (Autoformer model)\n",
            "            - [`BarkConfig`] configuration class: [`BarkModel`] (Bark model)\n",
            "            - [`BartConfig`] configuration class: [`BartModel`] (BART model)\n",
            "            - [`BeitConfig`] configuration class: [`BeitModel`] (BEiT model)\n",
            "            - [`BertConfig`] configuration class: [`BertModel`] (BERT model)\n",
            "            - [`BertGenerationConfig`] configuration class: [`BertGenerationEncoder`] (Bert Generation model)\n",
            "            - [`BigBirdConfig`] configuration class: [`BigBirdModel`] (BigBird model)\n",
            "            - [`BigBirdPegasusConfig`] configuration class: [`BigBirdPegasusModel`] (BigBird-Pegasus model)\n",
            "            - [`BioGptConfig`] configuration class: [`BioGptModel`] (BioGpt model)\n",
            "            - [`BitConfig`] configuration class: [`BitModel`] (BiT model)\n",
            "            - [`BlenderbotConfig`] configuration class: [`BlenderbotModel`] (Blenderbot model)\n",
            "            - [`BlenderbotSmallConfig`] configuration class: [`BlenderbotSmallModel`] (BlenderbotSmall model)\n",
            "            - [`Blip2Config`] configuration class: [`Blip2Model`] (BLIP-2 model)\n",
            "            - [`BlipConfig`] configuration class: [`BlipModel`] (BLIP model)\n",
            "            - [`BloomConfig`] configuration class: [`BloomModel`] (BLOOM model)\n",
            "            - [`BridgeTowerConfig`] configuration class: [`BridgeTowerModel`] (BridgeTower model)\n",
            "            - [`BrosConfig`] configuration class: [`BrosModel`] (BROS model)\n",
            "            - [`CLIPConfig`] configuration class: [`CLIPModel`] (CLIP model)\n",
            "            - [`CLIPSegConfig`] configuration class: [`CLIPSegModel`] (CLIPSeg model)\n",
            "            - [`CLIPVisionConfig`] configuration class: [`CLIPVisionModel`] (CLIPVisionModel model)\n",
            "            - [`CTRLConfig`] configuration class: [`CTRLModel`] (CTRL model)\n",
            "            - [`CamembertConfig`] configuration class: [`CamembertModel`] (CamemBERT model)\n",
            "            - [`CanineConfig`] configuration class: [`CanineModel`] (CANINE model)\n",
            "            - [`ChineseCLIPConfig`] configuration class: [`ChineseCLIPModel`] (Chinese-CLIP model)\n",
            "            - [`ChineseCLIPVisionConfig`] configuration class: [`ChineseCLIPVisionModel`] (ChineseCLIPVisionModel model)\n",
            "            - [`ClapConfig`] configuration class: [`ClapModel`] (CLAP model)\n",
            "            - [`ClvpConfig`] configuration class: [`ClvpModelForConditionalGeneration`] (CLVP model)\n",
            "            - [`CodeGenConfig`] configuration class: [`CodeGenModel`] (CodeGen model)\n",
            "            - [`ConditionalDetrConfig`] configuration class: [`ConditionalDetrModel`] (Conditional DETR model)\n",
            "            - [`ConvBertConfig`] configuration class: [`ConvBertModel`] (ConvBERT model)\n",
            "            - [`ConvNextConfig`] configuration class: [`ConvNextModel`] (ConvNeXT model)\n",
            "            - [`ConvNextV2Config`] configuration class: [`ConvNextV2Model`] (ConvNeXTV2 model)\n",
            "            - [`CpmAntConfig`] configuration class: [`CpmAntModel`] (CPM-Ant model)\n",
            "            - [`CvtConfig`] configuration class: [`CvtModel`] (CvT model)\n",
            "            - [`DPRConfig`] configuration class: [`DPRQuestionEncoder`] (DPR model)\n",
            "            - [`DPTConfig`] configuration class: [`DPTModel`] (DPT model)\n",
            "            - [`Data2VecAudioConfig`] configuration class: [`Data2VecAudioModel`] (Data2VecAudio model)\n",
            "            - [`Data2VecTextConfig`] configuration class: [`Data2VecTextModel`] (Data2VecText model)\n",
            "            - [`Data2VecVisionConfig`] configuration class: [`Data2VecVisionModel`] (Data2VecVision model)\n",
            "            - [`DebertaConfig`] configuration class: [`DebertaModel`] (DeBERTa model)\n",
            "            - [`DebertaV2Config`] configuration class: [`DebertaV2Model`] (DeBERTa-v2 model)\n",
            "            - [`DecisionTransformerConfig`] configuration class: [`DecisionTransformerModel`] (Decision Transformer model)\n",
            "            - [`DeformableDetrConfig`] configuration class: [`DeformableDetrModel`] (Deformable DETR model)\n",
            "            - [`DeiTConfig`] configuration class: [`DeiTModel`] (DeiT model)\n",
            "            - [`DetaConfig`] configuration class: [`DetaModel`] (DETA model)\n",
            "            - [`DetrConfig`] configuration class: [`DetrModel`] (DETR model)\n",
            "            - [`DinatConfig`] configuration class: [`DinatModel`] (DiNAT model)\n",
            "            - [`Dinov2Config`] configuration class: [`Dinov2Model`] (DINOv2 model)\n",
            "            - [`DistilBertConfig`] configuration class: [`DistilBertModel`] (DistilBERT model)\n",
            "            - [`DonutSwinConfig`] configuration class: [`DonutSwinModel`] (DonutSwin model)\n",
            "            - [`EfficientFormerConfig`] configuration class: [`EfficientFormerModel`] (EfficientFormer model)\n",
            "            - [`EfficientNetConfig`] configuration class: [`EfficientNetModel`] (EfficientNet model)\n",
            "            - [`ElectraConfig`] configuration class: [`ElectraModel`] (ELECTRA model)\n",
            "            - [`EncodecConfig`] configuration class: [`EncodecModel`] (EnCodec model)\n",
            "            - [`ErnieConfig`] configuration class: [`ErnieModel`] (ERNIE model)\n",
            "            - [`ErnieMConfig`] configuration class: [`ErnieMModel`] (ErnieM model)\n",
            "            - [`EsmConfig`] configuration class: [`EsmModel`] (ESM model)\n",
            "            - [`FNetConfig`] configuration class: [`FNetModel`] (FNet model)\n",
            "            - [`FSMTConfig`] configuration class: [`FSMTModel`] (FairSeq Machine-Translation model)\n",
            "            - [`FalconConfig`] configuration class: [`FalconModel`] (Falcon model)\n",
            "            - [`FastSpeech2ConformerConfig`] configuration class: [`FastSpeech2ConformerModel`] (FastSpeech2Conformer model)\n",
            "            - [`FlaubertConfig`] configuration class: [`FlaubertModel`] (FlauBERT model)\n",
            "            - [`FlavaConfig`] configuration class: [`FlavaModel`] (FLAVA model)\n",
            "            - [`FocalNetConfig`] configuration class: [`FocalNetModel`] (FocalNet model)\n",
            "            - [`FunnelConfig`] configuration class: [`FunnelModel`] or [`FunnelBaseModel`] (Funnel Transformer model)\n",
            "            - [`GLPNConfig`] configuration class: [`GLPNModel`] (GLPN model)\n",
            "            - [`GPT2Config`] configuration class: [`GPT2Model`] (OpenAI GPT-2 model)\n",
            "            - [`GPTBigCodeConfig`] configuration class: [`GPTBigCodeModel`] (GPTBigCode model)\n",
            "            - [`GPTJConfig`] configuration class: [`GPTJModel`] (GPT-J model)\n",
            "            - [`GPTNeoConfig`] configuration class: [`GPTNeoModel`] (GPT Neo model)\n",
            "            - [`GPTNeoXConfig`] configuration class: [`GPTNeoXModel`] (GPT NeoX model)\n",
            "            - [`GPTNeoXJapaneseConfig`] configuration class: [`GPTNeoXJapaneseModel`] (GPT NeoX Japanese model)\n",
            "            - [`GPTSanJapaneseConfig`] configuration class: [`GPTSanJapaneseForConditionalGeneration`] (GPTSAN-japanese model)\n",
            "            - [`GemmaConfig`] configuration class: [`GemmaModel`] (Gemma model)\n",
            "            - [`GitConfig`] configuration class: [`GitModel`] (GIT model)\n",
            "            - [`GraphormerConfig`] configuration class: [`GraphormerModel`] (Graphormer model)\n",
            "            - [`GroupViTConfig`] configuration class: [`GroupViTModel`] (GroupViT model)\n",
            "            - [`HubertConfig`] configuration class: [`HubertModel`] (Hubert model)\n",
            "            - [`IBertConfig`] configuration class: [`IBertModel`] (I-BERT model)\n",
            "            - [`IdeficsConfig`] configuration class: [`IdeficsModel`] (IDEFICS model)\n",
            "            - [`ImageGPTConfig`] configuration class: [`ImageGPTModel`] (ImageGPT model)\n",
            "            - [`InformerConfig`] configuration class: [`InformerModel`] (Informer model)\n",
            "            - [`JukeboxConfig`] configuration class: [`JukeboxModel`] (Jukebox model)\n",
            "            - [`Kosmos2Config`] configuration class: [`Kosmos2Model`] (KOSMOS-2 model)\n",
            "            - [`LEDConfig`] configuration class: [`LEDModel`] (LED model)\n",
            "            - [`LayoutLMConfig`] configuration class: [`LayoutLMModel`] (LayoutLM model)\n",
            "            - [`LayoutLMv2Config`] configuration class: [`LayoutLMv2Model`] (LayoutLMv2 model)\n",
            "            - [`LayoutLMv3Config`] configuration class: [`LayoutLMv3Model`] (LayoutLMv3 model)\n",
            "            - [`LevitConfig`] configuration class: [`LevitModel`] (LeViT model)\n",
            "            - [`LiltConfig`] configuration class: [`LiltModel`] (LiLT model)\n",
            "            - [`LlamaConfig`] configuration class: [`LlamaModel`] (LLaMA model)\n",
            "            - [`LongT5Config`] configuration class: [`LongT5Model`] (LongT5 model)\n",
            "            - [`LongformerConfig`] configuration class: [`LongformerModel`] (Longformer model)\n",
            "            - [`LukeConfig`] configuration class: [`LukeModel`] (LUKE model)\n",
            "            - [`LxmertConfig`] configuration class: [`LxmertModel`] (LXMERT model)\n",
            "            - [`M2M100Config`] configuration class: [`M2M100Model`] (M2M100 model)\n",
            "            - [`MBartConfig`] configuration class: [`MBartModel`] (mBART model)\n",
            "            - [`MCTCTConfig`] configuration class: [`MCTCTModel`] (M-CTC-T model)\n",
            "            - [`MPNetConfig`] configuration class: [`MPNetModel`] (MPNet model)\n",
            "            - [`MT5Config`] configuration class: [`MT5Model`] (MT5 model)\n",
            "            - [`MarianConfig`] configuration class: [`MarianModel`] (Marian model)\n",
            "            - [`MarkupLMConfig`] configuration class: [`MarkupLMModel`] (MarkupLM model)\n",
            "            - [`Mask2FormerConfig`] configuration class: [`Mask2FormerModel`] (Mask2Former model)\n",
            "            - [`MaskFormerConfig`] configuration class: [`MaskFormerModel`] (MaskFormer model)\n",
            "            - [`MaskFormerSwinConfig`] configuration class: [`MaskFormerSwinModel`] (MaskFormerSwin model)\n",
            "            - [`MegaConfig`] configuration class: [`MegaModel`] (MEGA model)\n",
            "            - [`MegatronBertConfig`] configuration class: [`MegatronBertModel`] (Megatron-BERT model)\n",
            "            - [`MgpstrConfig`] configuration class: [`MgpstrForSceneTextRecognition`] (MGP-STR model)\n",
            "            - [`MistralConfig`] configuration class: [`MistralModel`] (Mistral model)\n",
            "            - [`MixtralConfig`] configuration class: [`MixtralModel`] (Mixtral model)\n",
            "            - [`MobileBertConfig`] configuration class: [`MobileBertModel`] (MobileBERT model)\n",
            "            - [`MobileNetV1Config`] configuration class: [`MobileNetV1Model`] (MobileNetV1 model)\n",
            "            - [`MobileNetV2Config`] configuration class: [`MobileNetV2Model`] (MobileNetV2 model)\n",
            "            - [`MobileViTConfig`] configuration class: [`MobileViTModel`] (MobileViT model)\n",
            "            - [`MobileViTV2Config`] configuration class: [`MobileViTV2Model`] (MobileViTV2 model)\n",
            "            - [`MptConfig`] configuration class: [`MptModel`] (MPT model)\n",
            "            - [`MraConfig`] configuration class: [`MraModel`] (MRA model)\n",
            "            - [`MvpConfig`] configuration class: [`MvpModel`] (MVP model)\n",
            "            - [`NatConfig`] configuration class: [`NatModel`] (NAT model)\n",
            "            - [`NezhaConfig`] configuration class: [`NezhaModel`] (Nezha model)\n",
            "            - [`NllbMoeConfig`] configuration class: [`NllbMoeModel`] (NLLB-MOE model)\n",
            "            - [`NystromformerConfig`] configuration class: [`NystromformerModel`] (Nyströmformer model)\n",
            "            - [`OPTConfig`] configuration class: [`OPTModel`] (OPT model)\n",
            "            - [`OneFormerConfig`] configuration class: [`OneFormerModel`] (OneFormer model)\n",
            "            - [`OpenAIGPTConfig`] configuration class: [`OpenAIGPTModel`] (OpenAI GPT model)\n",
            "            - [`OpenLlamaConfig`] configuration class: [`OpenLlamaModel`] (OpenLlama model)\n",
            "            - [`OwlViTConfig`] configuration class: [`OwlViTModel`] (OWL-ViT model)\n",
            "            - [`Owlv2Config`] configuration class: [`Owlv2Model`] (OWLv2 model)\n",
            "            - [`PLBartConfig`] configuration class: [`PLBartModel`] (PLBart model)\n",
            "            - [`PatchTSMixerConfig`] configuration class: [`PatchTSMixerModel`] (PatchTSMixer model)\n",
            "            - [`PatchTSTConfig`] configuration class: [`PatchTSTModel`] (PatchTST model)\n",
            "            - [`PegasusConfig`] configuration class: [`PegasusModel`] (Pegasus model)\n",
            "            - [`PegasusXConfig`] configuration class: [`PegasusXModel`] (PEGASUS-X model)\n",
            "            - [`PerceiverConfig`] configuration class: [`PerceiverModel`] (Perceiver model)\n",
            "            - [`PersimmonConfig`] configuration class: [`PersimmonModel`] (Persimmon model)\n",
            "            - [`PhiConfig`] configuration class: [`PhiModel`] (Phi model)\n",
            "            - [`PoolFormerConfig`] configuration class: [`PoolFormerModel`] (PoolFormer model)\n",
            "            - [`ProphetNetConfig`] configuration class: [`ProphetNetModel`] (ProphetNet model)\n",
            "            - [`PvtConfig`] configuration class: [`PvtModel`] (PVT model)\n",
            "            - [`QDQBertConfig`] configuration class: [`QDQBertModel`] (QDQBert model)\n",
            "            - [`Qwen2Config`] configuration class: [`Qwen2Model`] (Qwen2 model)\n",
            "            - [`ReformerConfig`] configuration class: [`ReformerModel`] (Reformer model)\n",
            "            - [`RegNetConfig`] configuration class: [`RegNetModel`] (RegNet model)\n",
            "            - [`RemBertConfig`] configuration class: [`RemBertModel`] (RemBERT model)\n",
            "            - [`ResNetConfig`] configuration class: [`ResNetModel`] (ResNet model)\n",
            "            - [`RetriBertConfig`] configuration class: [`RetriBertModel`] (RetriBERT model)\n",
            "            - [`RoCBertConfig`] configuration class: [`RoCBertModel`] (RoCBert model)\n",
            "            - [`RoFormerConfig`] configuration class: [`RoFormerModel`] (RoFormer model)\n",
            "            - [`RobertaConfig`] configuration class: [`RobertaModel`] (RoBERTa model)\n",
            "            - [`RobertaPreLayerNormConfig`] configuration class: [`RobertaPreLayerNormModel`] (RoBERTa-PreLayerNorm model)\n",
            "            - [`RwkvConfig`] configuration class: [`RwkvModel`] (RWKV model)\n",
            "            - [`SEWConfig`] configuration class: [`SEWModel`] (SEW model)\n",
            "            - [`SEWDConfig`] configuration class: [`SEWDModel`] (SEW-D model)\n",
            "            - [`SamConfig`] configuration class: [`SamModel`] (SAM model)\n",
            "            - [`SeamlessM4TConfig`] configuration class: [`SeamlessM4TModel`] (SeamlessM4T model)\n",
            "            - [`SeamlessM4Tv2Config`] configuration class: [`SeamlessM4Tv2Model`] (SeamlessM4Tv2 model)\n",
            "            - [`SegformerConfig`] configuration class: [`SegformerModel`] (SegFormer model)\n",
            "            - [`SiglipConfig`] configuration class: [`SiglipModel`] (SigLIP model)\n",
            "            - [`SiglipVisionConfig`] configuration class: [`SiglipVisionModel`] (SiglipVisionModel model)\n",
            "            - [`Speech2TextConfig`] configuration class: [`Speech2TextModel`] (Speech2Text model)\n",
            "            - [`SpeechT5Config`] configuration class: [`SpeechT5Model`] (SpeechT5 model)\n",
            "            - [`SplinterConfig`] configuration class: [`SplinterModel`] (Splinter model)\n",
            "            - [`SqueezeBertConfig`] configuration class: [`SqueezeBertModel`] (SqueezeBERT model)\n",
            "            - [`StableLmConfig`] configuration class: [`StableLmModel`] (StableLm model)\n",
            "            - [`SwiftFormerConfig`] configuration class: [`SwiftFormerModel`] (SwiftFormer model)\n",
            "            - [`Swin2SRConfig`] configuration class: [`Swin2SRModel`] (Swin2SR model)\n",
            "            - [`SwinConfig`] configuration class: [`SwinModel`] (Swin Transformer model)\n",
            "            - [`Swinv2Config`] configuration class: [`Swinv2Model`] (Swin Transformer V2 model)\n",
            "            - [`SwitchTransformersConfig`] configuration class: [`SwitchTransformersModel`] (SwitchTransformers model)\n",
            "            - [`T5Config`] configuration class: [`T5Model`] (T5 model)\n",
            "            - [`TableTransformerConfig`] configuration class: [`TableTransformerModel`] (Table Transformer model)\n",
            "            - [`TapasConfig`] configuration class: [`TapasModel`] (TAPAS model)\n",
            "            - [`TimeSeriesTransformerConfig`] configuration class: [`TimeSeriesTransformerModel`] (Time Series Transformer model)\n",
            "            - [`TimesformerConfig`] configuration class: [`TimesformerModel`] (TimeSformer model)\n",
            "            - [`TimmBackboneConfig`] configuration class: [`TimmBackbone`] (TimmBackbone model)\n",
            "            - [`TrajectoryTransformerConfig`] configuration class: [`TrajectoryTransformerModel`] (Trajectory Transformer model)\n",
            "            - [`TransfoXLConfig`] configuration class: [`TransfoXLModel`] (Transformer-XL model)\n",
            "            - [`TvltConfig`] configuration class: [`TvltModel`] (TVLT model)\n",
            "            - [`TvpConfig`] configuration class: [`TvpModel`] (TVP model)\n",
            "            - [`UMT5Config`] configuration class: [`UMT5Model`] (UMT5 model)\n",
            "            - [`UniSpeechConfig`] configuration class: [`UniSpeechModel`] (UniSpeech model)\n",
            "            - [`UniSpeechSatConfig`] configuration class: [`UniSpeechSatModel`] (UniSpeechSat model)\n",
            "            - [`UnivNetConfig`] configuration class: [`UnivNetModel`] (UnivNet model)\n",
            "            - [`VanConfig`] configuration class: [`VanModel`] (VAN model)\n",
            "            - [`ViTConfig`] configuration class: [`ViTModel`] (ViT model)\n",
            "            - [`ViTHybridConfig`] configuration class: [`ViTHybridModel`] (ViT Hybrid model)\n",
            "            - [`ViTMAEConfig`] configuration class: [`ViTMAEModel`] (ViTMAE model)\n",
            "            - [`ViTMSNConfig`] configuration class: [`ViTMSNModel`] (ViTMSN model)\n",
            "            - [`VideoMAEConfig`] configuration class: [`VideoMAEModel`] (VideoMAE model)\n",
            "            - [`ViltConfig`] configuration class: [`ViltModel`] (ViLT model)\n",
            "            - [`VisionTextDualEncoderConfig`] configuration class: [`VisionTextDualEncoderModel`] (VisionTextDualEncoder model)\n",
            "            - [`VisualBertConfig`] configuration class: [`VisualBertModel`] (VisualBERT model)\n",
            "            - [`VitDetConfig`] configuration class: [`VitDetModel`] (VitDet model)\n",
            "            - [`VitsConfig`] configuration class: [`VitsModel`] (VITS model)\n",
            "            - [`VivitConfig`] configuration class: [`VivitModel`] (ViViT model)\n",
            "            - [`Wav2Vec2BertConfig`] configuration class: [`Wav2Vec2BertModel`] (Wav2Vec2-BERT model)\n",
            "            - [`Wav2Vec2Config`] configuration class: [`Wav2Vec2Model`] (Wav2Vec2 model)\n",
            "            - [`Wav2Vec2ConformerConfig`] configuration class: [`Wav2Vec2ConformerModel`] (Wav2Vec2-Conformer model)\n",
            "            - [`WavLMConfig`] configuration class: [`WavLMModel`] (WavLM model)\n",
            "            - [`WhisperConfig`] configuration class: [`WhisperModel`] (Whisper model)\n",
            "            - [`XCLIPConfig`] configuration class: [`XCLIPModel`] (X-CLIP model)\n",
            "            - [`XGLMConfig`] configuration class: [`XGLMModel`] (XGLM model)\n",
            "            - [`XLMConfig`] configuration class: [`XLMModel`] (XLM model)\n",
            "            - [`XLMProphetNetConfig`] configuration class: [`XLMProphetNetModel`] (XLM-ProphetNet model)\n",
            "            - [`XLMRobertaConfig`] configuration class: [`XLMRobertaModel`] (XLM-RoBERTa model)\n",
            "            - [`XLMRobertaXLConfig`] configuration class: [`XLMRobertaXLModel`] (XLM-RoBERTa-XL model)\n",
            "            - [`XLNetConfig`] configuration class: [`XLNetModel`] (XLNet model)\n",
            "            - [`XmodConfig`] configuration class: [`XmodModel`] (X-MOD model)\n",
            "            - [`YolosConfig`] configuration class: [`YolosModel`] (YOLOS model)\n",
            "            - [`YosoConfig`] configuration class: [`YosoModel`] (YOSO model)\n",
            "    \n",
            "    Examples:\n",
            "    \n",
            "    ```python\n",
            "    >>> from transformers import AutoConfig, AutoModel\n",
            "    \n",
            "    >>> # Download configuration from huggingface.co and cache.\n",
            "    >>> config = AutoConfig.from_pretrained(\"google-bert/bert-base-cased\")\n",
            "    >>> model = AutoModel.from_config(config)\n",
            "    ```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(AutoModel.from_pretrained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvhZYjWTVzhp",
        "outputId": "4f393d76-1cbe-4f86-a131-84f21d552789"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method from_pretrained in module transformers.models.auto.auto_factory:\n",
            "\n",
            "from_pretrained(*model_args, **kwargs) method of builtins.type instance\n",
            "    Instantiate one of the base model classes of the library from a pretrained model.\n",
            "    \n",
            "    The model class to instantiate is selected based on the `model_type` property of the config object (either\n",
            "    passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n",
            "    falling back to using pattern matching on `pretrained_model_name_or_path`:\n",
            "    \n",
            "        - **albert** -- [`AlbertModel`] (ALBERT model)\n",
            "        - **align** -- [`AlignModel`] (ALIGN model)\n",
            "        - **altclip** -- [`AltCLIPModel`] (AltCLIP model)\n",
            "        - **audio-spectrogram-transformer** -- [`ASTModel`] (Audio Spectrogram Transformer model)\n",
            "        - **autoformer** -- [`AutoformerModel`] (Autoformer model)\n",
            "        - **bark** -- [`BarkModel`] (Bark model)\n",
            "        - **bart** -- [`BartModel`] (BART model)\n",
            "        - **beit** -- [`BeitModel`] (BEiT model)\n",
            "        - **bert** -- [`BertModel`] (BERT model)\n",
            "        - **bert-generation** -- [`BertGenerationEncoder`] (Bert Generation model)\n",
            "        - **big_bird** -- [`BigBirdModel`] (BigBird model)\n",
            "        - **bigbird_pegasus** -- [`BigBirdPegasusModel`] (BigBird-Pegasus model)\n",
            "        - **biogpt** -- [`BioGptModel`] (BioGpt model)\n",
            "        - **bit** -- [`BitModel`] (BiT model)\n",
            "        - **blenderbot** -- [`BlenderbotModel`] (Blenderbot model)\n",
            "        - **blenderbot-small** -- [`BlenderbotSmallModel`] (BlenderbotSmall model)\n",
            "        - **blip** -- [`BlipModel`] (BLIP model)\n",
            "        - **blip-2** -- [`Blip2Model`] (BLIP-2 model)\n",
            "        - **bloom** -- [`BloomModel`] (BLOOM model)\n",
            "        - **bridgetower** -- [`BridgeTowerModel`] (BridgeTower model)\n",
            "        - **bros** -- [`BrosModel`] (BROS model)\n",
            "        - **camembert** -- [`CamembertModel`] (CamemBERT model)\n",
            "        - **canine** -- [`CanineModel`] (CANINE model)\n",
            "        - **chinese_clip** -- [`ChineseCLIPModel`] (Chinese-CLIP model)\n",
            "        - **chinese_clip_vision_model** -- [`ChineseCLIPVisionModel`] (ChineseCLIPVisionModel model)\n",
            "        - **clap** -- [`ClapModel`] (CLAP model)\n",
            "        - **clip** -- [`CLIPModel`] (CLIP model)\n",
            "        - **clip_vision_model** -- [`CLIPVisionModel`] (CLIPVisionModel model)\n",
            "        - **clipseg** -- [`CLIPSegModel`] (CLIPSeg model)\n",
            "        - **clvp** -- [`ClvpModelForConditionalGeneration`] (CLVP model)\n",
            "        - **code_llama** -- [`LlamaModel`] (CodeLlama model)\n",
            "        - **codegen** -- [`CodeGenModel`] (CodeGen model)\n",
            "        - **conditional_detr** -- [`ConditionalDetrModel`] (Conditional DETR model)\n",
            "        - **convbert** -- [`ConvBertModel`] (ConvBERT model)\n",
            "        - **convnext** -- [`ConvNextModel`] (ConvNeXT model)\n",
            "        - **convnextv2** -- [`ConvNextV2Model`] (ConvNeXTV2 model)\n",
            "        - **cpmant** -- [`CpmAntModel`] (CPM-Ant model)\n",
            "        - **ctrl** -- [`CTRLModel`] (CTRL model)\n",
            "        - **cvt** -- [`CvtModel`] (CvT model)\n",
            "        - **data2vec-audio** -- [`Data2VecAudioModel`] (Data2VecAudio model)\n",
            "        - **data2vec-text** -- [`Data2VecTextModel`] (Data2VecText model)\n",
            "        - **data2vec-vision** -- [`Data2VecVisionModel`] (Data2VecVision model)\n",
            "        - **deberta** -- [`DebertaModel`] (DeBERTa model)\n",
            "        - **deberta-v2** -- [`DebertaV2Model`] (DeBERTa-v2 model)\n",
            "        - **decision_transformer** -- [`DecisionTransformerModel`] (Decision Transformer model)\n",
            "        - **deformable_detr** -- [`DeformableDetrModel`] (Deformable DETR model)\n",
            "        - **deit** -- [`DeiTModel`] (DeiT model)\n",
            "        - **deta** -- [`DetaModel`] (DETA model)\n",
            "        - **detr** -- [`DetrModel`] (DETR model)\n",
            "        - **dinat** -- [`DinatModel`] (DiNAT model)\n",
            "        - **dinov2** -- [`Dinov2Model`] (DINOv2 model)\n",
            "        - **distilbert** -- [`DistilBertModel`] (DistilBERT model)\n",
            "        - **donut-swin** -- [`DonutSwinModel`] (DonutSwin model)\n",
            "        - **dpr** -- [`DPRQuestionEncoder`] (DPR model)\n",
            "        - **dpt** -- [`DPTModel`] (DPT model)\n",
            "        - **efficientformer** -- [`EfficientFormerModel`] (EfficientFormer model)\n",
            "        - **efficientnet** -- [`EfficientNetModel`] (EfficientNet model)\n",
            "        - **electra** -- [`ElectraModel`] (ELECTRA model)\n",
            "        - **encodec** -- [`EncodecModel`] (EnCodec model)\n",
            "        - **ernie** -- [`ErnieModel`] (ERNIE model)\n",
            "        - **ernie_m** -- [`ErnieMModel`] (ErnieM model)\n",
            "        - **esm** -- [`EsmModel`] (ESM model)\n",
            "        - **falcon** -- [`FalconModel`] (Falcon model)\n",
            "        - **fastspeech2_conformer** -- [`FastSpeech2ConformerModel`] (FastSpeech2Conformer model)\n",
            "        - **flaubert** -- [`FlaubertModel`] (FlauBERT model)\n",
            "        - **flava** -- [`FlavaModel`] (FLAVA model)\n",
            "        - **fnet** -- [`FNetModel`] (FNet model)\n",
            "        - **focalnet** -- [`FocalNetModel`] (FocalNet model)\n",
            "        - **fsmt** -- [`FSMTModel`] (FairSeq Machine-Translation model)\n",
            "        - **funnel** -- [`FunnelModel`] or [`FunnelBaseModel`] (Funnel Transformer model)\n",
            "        - **gemma** -- [`GemmaModel`] (Gemma model)\n",
            "        - **git** -- [`GitModel`] (GIT model)\n",
            "        - **glpn** -- [`GLPNModel`] (GLPN model)\n",
            "        - **gpt-sw3** -- [`GPT2Model`] (GPT-Sw3 model)\n",
            "        - **gpt2** -- [`GPT2Model`] (OpenAI GPT-2 model)\n",
            "        - **gpt_bigcode** -- [`GPTBigCodeModel`] (GPTBigCode model)\n",
            "        - **gpt_neo** -- [`GPTNeoModel`] (GPT Neo model)\n",
            "        - **gpt_neox** -- [`GPTNeoXModel`] (GPT NeoX model)\n",
            "        - **gpt_neox_japanese** -- [`GPTNeoXJapaneseModel`] (GPT NeoX Japanese model)\n",
            "        - **gptj** -- [`GPTJModel`] (GPT-J model)\n",
            "        - **gptsan-japanese** -- [`GPTSanJapaneseForConditionalGeneration`] (GPTSAN-japanese model)\n",
            "        - **graphormer** -- [`GraphormerModel`] (Graphormer model)\n",
            "        - **groupvit** -- [`GroupViTModel`] (GroupViT model)\n",
            "        - **hubert** -- [`HubertModel`] (Hubert model)\n",
            "        - **ibert** -- [`IBertModel`] (I-BERT model)\n",
            "        - **idefics** -- [`IdeficsModel`] (IDEFICS model)\n",
            "        - **imagegpt** -- [`ImageGPTModel`] (ImageGPT model)\n",
            "        - **informer** -- [`InformerModel`] (Informer model)\n",
            "        - **jukebox** -- [`JukeboxModel`] (Jukebox model)\n",
            "        - **kosmos-2** -- [`Kosmos2Model`] (KOSMOS-2 model)\n",
            "        - **layoutlm** -- [`LayoutLMModel`] (LayoutLM model)\n",
            "        - **layoutlmv2** -- [`LayoutLMv2Model`] (LayoutLMv2 model)\n",
            "        - **layoutlmv3** -- [`LayoutLMv3Model`] (LayoutLMv3 model)\n",
            "        - **led** -- [`LEDModel`] (LED model)\n",
            "        - **levit** -- [`LevitModel`] (LeViT model)\n",
            "        - **lilt** -- [`LiltModel`] (LiLT model)\n",
            "        - **llama** -- [`LlamaModel`] (LLaMA model)\n",
            "        - **longformer** -- [`LongformerModel`] (Longformer model)\n",
            "        - **longt5** -- [`LongT5Model`] (LongT5 model)\n",
            "        - **luke** -- [`LukeModel`] (LUKE model)\n",
            "        - **lxmert** -- [`LxmertModel`] (LXMERT model)\n",
            "        - **m2m_100** -- [`M2M100Model`] (M2M100 model)\n",
            "        - **marian** -- [`MarianModel`] (Marian model)\n",
            "        - **markuplm** -- [`MarkupLMModel`] (MarkupLM model)\n",
            "        - **mask2former** -- [`Mask2FormerModel`] (Mask2Former model)\n",
            "        - **maskformer** -- [`MaskFormerModel`] (MaskFormer model)\n",
            "        - **maskformer-swin** -- [`MaskFormerSwinModel`] (MaskFormerSwin model)\n",
            "        - **mbart** -- [`MBartModel`] (mBART model)\n",
            "        - **mctct** -- [`MCTCTModel`] (M-CTC-T model)\n",
            "        - **mega** -- [`MegaModel`] (MEGA model)\n",
            "        - **megatron-bert** -- [`MegatronBertModel`] (Megatron-BERT model)\n",
            "        - **mgp-str** -- [`MgpstrForSceneTextRecognition`] (MGP-STR model)\n",
            "        - **mistral** -- [`MistralModel`] (Mistral model)\n",
            "        - **mixtral** -- [`MixtralModel`] (Mixtral model)\n",
            "        - **mobilebert** -- [`MobileBertModel`] (MobileBERT model)\n",
            "        - **mobilenet_v1** -- [`MobileNetV1Model`] (MobileNetV1 model)\n",
            "        - **mobilenet_v2** -- [`MobileNetV2Model`] (MobileNetV2 model)\n",
            "        - **mobilevit** -- [`MobileViTModel`] (MobileViT model)\n",
            "        - **mobilevitv2** -- [`MobileViTV2Model`] (MobileViTV2 model)\n",
            "        - **mpnet** -- [`MPNetModel`] (MPNet model)\n",
            "        - **mpt** -- [`MptModel`] (MPT model)\n",
            "        - **mra** -- [`MraModel`] (MRA model)\n",
            "        - **mt5** -- [`MT5Model`] (MT5 model)\n",
            "        - **mvp** -- [`MvpModel`] (MVP model)\n",
            "        - **nat** -- [`NatModel`] (NAT model)\n",
            "        - **nezha** -- [`NezhaModel`] (Nezha model)\n",
            "        - **nllb-moe** -- [`NllbMoeModel`] (NLLB-MOE model)\n",
            "        - **nystromformer** -- [`NystromformerModel`] (Nyströmformer model)\n",
            "        - **oneformer** -- [`OneFormerModel`] (OneFormer model)\n",
            "        - **open-llama** -- [`OpenLlamaModel`] (OpenLlama model)\n",
            "        - **openai-gpt** -- [`OpenAIGPTModel`] (OpenAI GPT model)\n",
            "        - **opt** -- [`OPTModel`] (OPT model)\n",
            "        - **owlv2** -- [`Owlv2Model`] (OWLv2 model)\n",
            "        - **owlvit** -- [`OwlViTModel`] (OWL-ViT model)\n",
            "        - **patchtsmixer** -- [`PatchTSMixerModel`] (PatchTSMixer model)\n",
            "        - **patchtst** -- [`PatchTSTModel`] (PatchTST model)\n",
            "        - **pegasus** -- [`PegasusModel`] (Pegasus model)\n",
            "        - **pegasus_x** -- [`PegasusXModel`] (PEGASUS-X model)\n",
            "        - **perceiver** -- [`PerceiverModel`] (Perceiver model)\n",
            "        - **persimmon** -- [`PersimmonModel`] (Persimmon model)\n",
            "        - **phi** -- [`PhiModel`] (Phi model)\n",
            "        - **plbart** -- [`PLBartModel`] (PLBart model)\n",
            "        - **poolformer** -- [`PoolFormerModel`] (PoolFormer model)\n",
            "        - **prophetnet** -- [`ProphetNetModel`] (ProphetNet model)\n",
            "        - **pvt** -- [`PvtModel`] (PVT model)\n",
            "        - **qdqbert** -- [`QDQBertModel`] (QDQBert model)\n",
            "        - **qwen2** -- [`Qwen2Model`] (Qwen2 model)\n",
            "        - **reformer** -- [`ReformerModel`] (Reformer model)\n",
            "        - **regnet** -- [`RegNetModel`] (RegNet model)\n",
            "        - **rembert** -- [`RemBertModel`] (RemBERT model)\n",
            "        - **resnet** -- [`ResNetModel`] (ResNet model)\n",
            "        - **retribert** -- [`RetriBertModel`] (RetriBERT model)\n",
            "        - **roberta** -- [`RobertaModel`] (RoBERTa model)\n",
            "        - **roberta-prelayernorm** -- [`RobertaPreLayerNormModel`] (RoBERTa-PreLayerNorm model)\n",
            "        - **roc_bert** -- [`RoCBertModel`] (RoCBert model)\n",
            "        - **roformer** -- [`RoFormerModel`] (RoFormer model)\n",
            "        - **rwkv** -- [`RwkvModel`] (RWKV model)\n",
            "        - **sam** -- [`SamModel`] (SAM model)\n",
            "        - **seamless_m4t** -- [`SeamlessM4TModel`] (SeamlessM4T model)\n",
            "        - **seamless_m4t_v2** -- [`SeamlessM4Tv2Model`] (SeamlessM4Tv2 model)\n",
            "        - **segformer** -- [`SegformerModel`] (SegFormer model)\n",
            "        - **sew** -- [`SEWModel`] (SEW model)\n",
            "        - **sew-d** -- [`SEWDModel`] (SEW-D model)\n",
            "        - **siglip** -- [`SiglipModel`] (SigLIP model)\n",
            "        - **siglip_vision_model** -- [`SiglipVisionModel`] (SiglipVisionModel model)\n",
            "        - **speech_to_text** -- [`Speech2TextModel`] (Speech2Text model)\n",
            "        - **speecht5** -- [`SpeechT5Model`] (SpeechT5 model)\n",
            "        - **splinter** -- [`SplinterModel`] (Splinter model)\n",
            "        - **squeezebert** -- [`SqueezeBertModel`] (SqueezeBERT model)\n",
            "        - **stablelm** -- [`StableLmModel`] (StableLm model)\n",
            "        - **swiftformer** -- [`SwiftFormerModel`] (SwiftFormer model)\n",
            "        - **swin** -- [`SwinModel`] (Swin Transformer model)\n",
            "        - **swin2sr** -- [`Swin2SRModel`] (Swin2SR model)\n",
            "        - **swinv2** -- [`Swinv2Model`] (Swin Transformer V2 model)\n",
            "        - **switch_transformers** -- [`SwitchTransformersModel`] (SwitchTransformers model)\n",
            "        - **t5** -- [`T5Model`] (T5 model)\n",
            "        - **table-transformer** -- [`TableTransformerModel`] (Table Transformer model)\n",
            "        - **tapas** -- [`TapasModel`] (TAPAS model)\n",
            "        - **time_series_transformer** -- [`TimeSeriesTransformerModel`] (Time Series Transformer model)\n",
            "        - **timesformer** -- [`TimesformerModel`] (TimeSformer model)\n",
            "        - **timm_backbone** -- [`TimmBackbone`] (TimmBackbone model)\n",
            "        - **trajectory_transformer** -- [`TrajectoryTransformerModel`] (Trajectory Transformer model)\n",
            "        - **transfo-xl** -- [`TransfoXLModel`] (Transformer-XL model)\n",
            "        - **tvlt** -- [`TvltModel`] (TVLT model)\n",
            "        - **tvp** -- [`TvpModel`] (TVP model)\n",
            "        - **umt5** -- [`UMT5Model`] (UMT5 model)\n",
            "        - **unispeech** -- [`UniSpeechModel`] (UniSpeech model)\n",
            "        - **unispeech-sat** -- [`UniSpeechSatModel`] (UniSpeechSat model)\n",
            "        - **univnet** -- [`UnivNetModel`] (UnivNet model)\n",
            "        - **van** -- [`VanModel`] (VAN model)\n",
            "        - **videomae** -- [`VideoMAEModel`] (VideoMAE model)\n",
            "        - **vilt** -- [`ViltModel`] (ViLT model)\n",
            "        - **vision-text-dual-encoder** -- [`VisionTextDualEncoderModel`] (VisionTextDualEncoder model)\n",
            "        - **visual_bert** -- [`VisualBertModel`] (VisualBERT model)\n",
            "        - **vit** -- [`ViTModel`] (ViT model)\n",
            "        - **vit_hybrid** -- [`ViTHybridModel`] (ViT Hybrid model)\n",
            "        - **vit_mae** -- [`ViTMAEModel`] (ViTMAE model)\n",
            "        - **vit_msn** -- [`ViTMSNModel`] (ViTMSN model)\n",
            "        - **vitdet** -- [`VitDetModel`] (VitDet model)\n",
            "        - **vits** -- [`VitsModel`] (VITS model)\n",
            "        - **vivit** -- [`VivitModel`] (ViViT model)\n",
            "        - **wav2vec2** -- [`Wav2Vec2Model`] (Wav2Vec2 model)\n",
            "        - **wav2vec2-bert** -- [`Wav2Vec2BertModel`] (Wav2Vec2-BERT model)\n",
            "        - **wav2vec2-conformer** -- [`Wav2Vec2ConformerModel`] (Wav2Vec2-Conformer model)\n",
            "        - **wavlm** -- [`WavLMModel`] (WavLM model)\n",
            "        - **whisper** -- [`WhisperModel`] (Whisper model)\n",
            "        - **xclip** -- [`XCLIPModel`] (X-CLIP model)\n",
            "        - **xglm** -- [`XGLMModel`] (XGLM model)\n",
            "        - **xlm** -- [`XLMModel`] (XLM model)\n",
            "        - **xlm-prophetnet** -- [`XLMProphetNetModel`] (XLM-ProphetNet model)\n",
            "        - **xlm-roberta** -- [`XLMRobertaModel`] (XLM-RoBERTa model)\n",
            "        - **xlm-roberta-xl** -- [`XLMRobertaXLModel`] (XLM-RoBERTa-XL model)\n",
            "        - **xlnet** -- [`XLNetModel`] (XLNet model)\n",
            "        - **xmod** -- [`XmodModel`] (X-MOD model)\n",
            "        - **yolos** -- [`YolosModel`] (YOLOS model)\n",
            "        - **yoso** -- [`YosoModel`] (YOSO model)\n",
            "    \n",
            "    The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are\n",
            "    deactivated). To train the model, you should first set it back in training mode with `model.train()`\n",
            "    \n",
            "    Args:\n",
            "        pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
            "            Can be either:\n",
            "    \n",
            "                - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
            "                - A path to a *directory* containing model weights saved using\n",
            "                  [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
            "                - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
            "                  this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
            "                  `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
            "                  PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
            "        model_args (additional positional arguments, *optional*):\n",
            "            Will be passed along to the underlying model `__init__()` method.\n",
            "        config ([`PretrainedConfig`], *optional*):\n",
            "            Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
            "            be automatically loaded when:\n",
            "    \n",
            "                - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
            "                  model).\n",
            "                - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
            "                  save directory.\n",
            "                - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
            "                  configuration JSON file named *config.json* is found in the directory.\n",
            "        state_dict (*Dict[str, torch.Tensor]*, *optional*):\n",
            "            A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
            "    \n",
            "            This option can be used if you want to create a model from a pretrained configuration but load your own\n",
            "            weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
            "            [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
            "        cache_dir (`str` or `os.PathLike`, *optional*):\n",
            "            Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
            "            standard cache should not be used.\n",
            "        from_tf (`bool`, *optional*, defaults to `False`):\n",
            "            Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
            "            `pretrained_model_name_or_path` argument).\n",
            "        force_download (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            "            cached versions if they exist.\n",
            "        resume_download (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
            "            file exists.\n",
            "        proxies (`Dict[str, str]`, *optional*):\n",
            "            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
            "            'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            "        output_loading_info(`bool`, *optional*, defaults to `False`):\n",
            "            Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
            "        local_files_only(`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to only look at local files (e.g., not try downloading the model).\n",
            "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
            "            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
            "            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
            "            identifier allowed by git.\n",
            "        trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
            "            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
            "            execute code present on the Hub on your local machine.\n",
            "        code_revision (`str`, *optional*, defaults to `\"main\"`):\n",
            "            The specific revision to use for the code on the Hub, if the code leaves in a different repository than\n",
            "            the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\n",
            "            system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\n",
            "            allowed by git.\n",
            "        kwargs (additional keyword arguments, *optional*):\n",
            "            Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
            "            `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
            "            automatically loaded:\n",
            "    \n",
            "                - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
            "                  underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
            "                  already been done)\n",
            "                - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
            "                  initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
            "                  corresponds to a configuration attribute will be used to override said attribute with the\n",
            "                  supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
            "                  will be passed to the underlying model's `__init__` function.\n",
            "    \n",
            "    Examples:\n",
            "    \n",
            "    ```python\n",
            "    >>> from transformers import AutoConfig, AutoModel\n",
            "    \n",
            "    >>> # Download model and configuration from huggingface.co and cache.\n",
            "    >>> model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n",
            "    \n",
            "    >>> # Update configuration during loading\n",
            "    >>> model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\", output_attentions=True)\n",
            "    >>> model.config.output_attentions\n",
            "    True\n",
            "    \n",
            "    >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
            "    >>> config = AutoConfig.from_pretrained(\"./tf_model/bert_tf_model_config.json\")\n",
            "    >>> model = AutoModel.from_pretrained(\n",
            "    ...     \"./tf_model/bert_tf_checkpoint.ckpt.index\", from_tf=True, config=config\n",
            "    ... )\n",
            "    ```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(AutoModel.register)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cm3e9lLV3gQ",
        "outputId": "476892f7-2e0a-4508-a41c-9c145afd55fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method register in module transformers.models.auto.auto_factory:\n",
            "\n",
            "register(config_class, model_class, exist_ok=False) method of builtins.type instance\n",
            "    Register a new model for this class.\n",
            "    \n",
            "    Args:\n",
            "        config_class ([`PretrainedConfig`]):\n",
            "            The configuration corresponding to the model to register.\n",
            "        model_class ([`PreTrainedModel`]):\n",
            "            The model to register.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "7032355718c249698c43242b1569e8b7",
            "722714584d834d54868f54081b2f5458",
            "7e638aea8e0e4cea8685a567f1424378",
            "59a80e72bb4146279a60ac310efabff5",
            "cfc3b4088f0547319cc97f4ddd3927f6",
            "113c2ef36a7a4aa9824529475cc9cd15",
            "1301ad61aeeb4c9ca41a87cec3d80e7a",
            "2235267506c3479b95ac330143094714",
            "7c68d00e1cc8430283ae029bea37fe78",
            "945adfbbd5974a83985920f347e436b1",
            "e65c3e8bfeeb429dbe22159222deeb90",
            "e0a9b4afba2b457f8cb8d8fb0da36942",
            "5f3ecfe57c56417eb4ebf67d1dea3b2d",
            "872e539aa73f44cfb1b45e46b8af2ab6",
            "bdc9cf2a820744e8988269c7e9060742",
            "7fbf0a39f0b445b5af020018bb813d4e",
            "ca62c926763f4d8eae95aa1dee323496",
            "45ad06874e7b40a1984da829bfd20bc5",
            "6e98ac1ae9bc42c4940986f77ae2156f",
            "11a61ff19892431183f3ef45c241698b",
            "13dcda64de1741bc92ff87d82b879c5a",
            "66aafacf9818442fabbd1a5dfc26d1d9",
            "7537f476d62c4edfa7ada4f4677245d9",
            "a6ceb2945b154c6fb7a682a5e9c153fa",
            "9c35160a77604ee99bc91ea61dd3acee",
            "5d69ac4537404207990ab49f12a3e750",
            "45d973762de04b12b4cee76138176838",
            "c0f020db08834e7c9bd0ac05223e0e19",
            "4b1fd450e1914a888f5634ff7b00ccf6",
            "5a0d8804514e4463bffae4be493209e5",
            "d67422af1fa64012854adad67a22a8f9",
            "562e75957f014469a0231272a08845aa",
            "4fd60da48eb24bc5aa9ef3ac62bcd6be",
            "6104d41624c64eb69210c3dbe71fdda3",
            "2b5a97882b43475b9b7ff8953c3ef1b1",
            "ef6d7b62b6b4485ea20b6b228c664ed9",
            "d5d3530882864020bac4547e4605197f",
            "4c98faf916b54415b050b24d4e905ec2",
            "0c5d354b41ac4e1c996d38a3aea564e6",
            "69f1570053294edaa25202a89ee35369",
            "9361c1921b5c44e0b2c2bef222aa569c",
            "7e8ad080c6544d2ebf8b7f0eae3612fb",
            "8ea299e332b64e1ab205ded794c7e3b6",
            "232df0e9e80e40d09a1fa27362ead5fc"
          ]
        },
        "id": "M67KnvtYc1lL",
        "outputId": "7f75440e-aa5f-4ad4-93d3-f691320c9c15"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/373 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7032355718c249698c43242b1569e8b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0a9b4afba2b457f8cb8d8fb0da36942"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7537f476d62c4edfa7ada4f4677245d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6104d41624c64eb69210c3dbe71fdda3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep7YN8DjdrnT",
        "outputId": "9d4d7f86-70d4-4f8a-dad2-58d7b9db50d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on DistilBertTokenizerFast in module transformers.models.distilbert.tokenization_distilbert_fast object:\n",
            "\n",
            "class DistilBertTokenizerFast(transformers.tokenization_utils_fast.PreTrainedTokenizerFast)\n",
            " |  DistilBertTokenizerFast(vocab_file=None, tokenizer_file=None, do_lower_case=True, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, strip_accents=None, **kwargs)\n",
            " |  \n",
            " |  Construct a \"fast\" DistilBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n",
            " |  \n",
            " |  This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
            " |  refer to this superclass for more information regarding those methods.\n",
            " |  \n",
            " |  Args:\n",
            " |      vocab_file (`str`):\n",
            " |          File containing the vocabulary.\n",
            " |      do_lower_case (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether or not to lowercase the input when tokenizing.\n",
            " |      unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n",
            " |          The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
            " |          token instead.\n",
            " |      sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n",
            " |          The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
            " |          sequence classification or for a text and a question for question answering. It is also used as the last\n",
            " |          token of a sequence built with special tokens.\n",
            " |      pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n",
            " |          The token used for padding, for example when batching sequences of different lengths.\n",
            " |      cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n",
            " |          The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
            " |          instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
            " |      mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n",
            " |          The token used for masking values. This is the token used when training this model with masked language\n",
            " |          modeling. This is the token which the model will try to predict.\n",
            " |      clean_text (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether or not to clean the text before tokenization by removing any control characters and replacing all\n",
            " |          whitespaces by the classic one.\n",
            " |      tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n",
            " |          issue](https://github.com/huggingface/transformers/issues/328)).\n",
            " |      strip_accents (`bool`, *optional*):\n",
            " |          Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
            " |          value for `lowercase` (as in the original BERT).\n",
            " |      wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n",
            " |          The prefix for subwords.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DistilBertTokenizerFast\n",
            " |      transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
            " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
            " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
            " |      transformers.utils.hub.PushToHubMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, vocab_file=None, tokenizer_file=None, do_lower_case=True, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, strip_accents=None, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None)\n",
            " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
            " |      adding special tokens. A BERT sequence has the following format:\n",
            " |      \n",
            " |      - single sequence: `[CLS] X [SEP]`\n",
            " |      - pair of sequences: `[CLS] A [SEP] B [SEP]`\n",
            " |      \n",
            " |      Args:\n",
            " |          token_ids_0 (`List[int]`):\n",
            " |              List of IDs to which the special tokens will be added.\n",
            " |          token_ids_1 (`List[int]`, *optional*):\n",
            " |              Optional second list of IDs for sequence pairs.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n",
            " |  \n",
            " |  create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]\n",
            " |      Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n",
            " |      pair mask has the following format:\n",
            " |      \n",
            " |      ```\n",
            " |      0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " |      | first sequence    | second sequence |\n",
            " |      ```\n",
            " |      \n",
            " |      If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n",
            " |      \n",
            " |      Args:\n",
            " |          token_ids_0 (`List[int]`):\n",
            " |              List of IDs.\n",
            " |          token_ids_1 (`List[int]`, *optional*):\n",
            " |              Optional second list of IDs for sequence pairs.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n",
            " |  \n",
            " |  save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]\n",
            " |      Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
            " |      \n",
            " |      This method won't save the configuration and special token mappings of the tokenizer. Use\n",
            " |      [`~PreTrainedTokenizerFast._save_pretrained`] to save the whole state of the tokenizer.\n",
            " |      \n",
            " |      Args:\n",
            " |          save_directory (`str`):\n",
            " |              The directory in which to save the vocabulary.\n",
            " |          filename_prefix (`str`, *optional*):\n",
            " |              An optional prefix to add to the named of the saved files.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `Tuple(str)`: Paths to the files saved.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  max_model_input_sizes = {'distilbert-base-cased': 512, 'distilbert-bas...\n",
            " |  \n",
            " |  model_input_names = ['input_ids', 'attention_mask']\n",
            " |  \n",
            " |  pretrained_init_configuration = {'distilbert-base-cased': {'do_lower_c...\n",
            " |  \n",
            " |  pretrained_vocab_files_map = {'tokenizer_file': {'distilbert-base-case...\n",
            " |  \n",
            " |  slow_tokenizer_class = <class 'transformers.models.distilbert.tokeniza...\n",
            " |      Construct a DistilBERT tokenizer. Based on WordPiece.\n",
            " |      \n",
            " |      This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n",
            " |      this superclass for more information regarding those methods.\n",
            " |      \n",
            " |      Args:\n",
            " |          vocab_file (`str`):\n",
            " |              File containing the vocabulary.\n",
            " |          do_lower_case (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to lowercase the input when tokenizing.\n",
            " |          do_basic_tokenize (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to do basic tokenization before WordPiece.\n",
            " |          never_split (`Iterable`, *optional*):\n",
            " |              Collection of tokens which will never be split during tokenization. Only has an effect when\n",
            " |              `do_basic_tokenize=True`\n",
            " |          unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n",
            " |              The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
            " |              token instead.\n",
            " |          sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n",
            " |              The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
            " |              sequence classification or for a text and a question for question answering. It is also used as the last\n",
            " |              token of a sequence built with special tokens.\n",
            " |          pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n",
            " |              The token used for padding, for example when batching sequences of different lengths.\n",
            " |          cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n",
            " |              The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
            " |              instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
            " |          mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n",
            " |              The token used for masking values. This is the token used when training this model with masked language\n",
            " |              modeling. This is the token which the model will try to predict.\n",
            " |          tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to tokenize Chinese characters.\n",
            " |      \n",
            " |              This should likely be deactivated for Japanese (see this\n",
            " |              [issue](https://github.com/huggingface/transformers/issues/328)).\n",
            " |          strip_accents (`bool`, *optional*):\n",
            " |              Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
            " |              value for `lowercase` (as in the original BERT).\n",
            " |  \n",
            " |  \n",
            " |  vocab_files_names = {'tokenizer_file': 'tokenizer.json', 'vocab_file':...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |      Size of the full vocabulary with the added tokens.\n",
            " |  \n",
            " |  convert_ids_to_tokens(self, ids: Union[int, List[int]], skip_special_tokens: bool = False) -> Union[str, List[str]]\n",
            " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
            " |      added tokens.\n",
            " |      \n",
            " |      Args:\n",
            " |          ids (`int` or `List[int]`):\n",
            " |              The token id (or token ids) to convert to tokens.\n",
            " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to remove special tokens in the decoding.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `str` or `List[str]`: The decoded token(s).\n",
            " |  \n",
            " |  convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]\n",
            " |      Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
            " |      vocabulary.\n",
            " |      \n",
            " |      Args:\n",
            " |          tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int` or `List[int]`: The token id or list of token ids.\n",
            " |  \n",
            " |  convert_tokens_to_string(self, tokens: List[str]) -> str\n",
            " |      Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n",
            " |      often want to remove sub-word tokenization artifacts at the same time.\n",
            " |      \n",
            " |      Args:\n",
            " |          tokens (`List[str]`): The token to join in a string.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `str`: The joined tokens.\n",
            " |  \n",
            " |  get_added_vocab(self) -> Dict[str, int]\n",
            " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `Dict[str, int]`: The added tokens.\n",
            " |  \n",
            " |  get_vocab(self) -> Dict[str, int]\n",
            " |      Returns the vocabulary as a dictionary of token to index.\n",
            " |      \n",
            " |      `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the\n",
            " |      vocab.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `Dict[str, int]`: The vocabulary.\n",
            " |  \n",
            " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
            " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put\n",
            " |      this inside your training loop.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          pair (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
            " |              sequence.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: Number of special tokens added to sequences.\n",
            " |  \n",
            " |  set_truncation_and_padding(self, padding_strategy: transformers.utils.generic.PaddingStrategy, truncation_strategy: transformers.tokenization_utils_base.TruncationStrategy, max_length: int, stride: int, pad_to_multiple_of: Optional[int])\n",
            " |      Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n",
            " |      library) and restore the tokenizer settings afterwards.\n",
            " |      \n",
            " |      The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a\n",
            " |      padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed\n",
            " |      section.\n",
            " |      \n",
            " |      Args:\n",
            " |          padding_strategy ([`~utils.PaddingStrategy`]):\n",
            " |              The kind of padding that will be applied to the input\n",
            " |          truncation_strategy ([`~tokenization_utils_base.TruncationStrategy`]):\n",
            " |              The kind of truncation that will be applied to the input\n",
            " |          max_length (`int`):\n",
            " |              The maximum size of a sequence.\n",
            " |          stride (`int`):\n",
            " |              The stride to use when handling overflow.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
            " |              the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).\n",
            " |  \n",
            " |  tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]\n",
            " |      Converts a string into a sequence of tokens, replacing unknown tokens with the `unk_token`.\n",
            " |      \n",
            " |      Args:\n",
            " |          text (`str`):\n",
            " |              The sequence to be encoded.\n",
            " |          pair (`str`, *optional*):\n",
            " |              A second sequence to be encoded with the first.\n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to add the special tokens associated with the corresponding model.\n",
            " |          kwargs (additional keyword arguments, *optional*):\n",
            " |              Will be passed to the underlying model specific encode method. See details in\n",
            " |              [`~PreTrainedTokenizerBase.__call__`]\n",
            " |      \n",
            " |      Returns:\n",
            " |          `List[str]`: The list of tokens.\n",
            " |  \n",
            " |  train_new_from_iterator(self, text_iterator, vocab_size, length=None, new_special_tokens=None, special_tokens_map=None, **kwargs)\n",
            " |      Trains a tokenizer on a new corpus with the same defaults (in terms of special tokens or tokenization pipeline)\n",
            " |      as the current one.\n",
            " |      \n",
            " |      Args:\n",
            " |          text_iterator (generator of `List[str]`):\n",
            " |              The training corpus. Should be a generator of batches of texts, for instance a list of lists of texts\n",
            " |              if you have everything in memory.\n",
            " |          vocab_size (`int`):\n",
            " |              The size of the vocabulary you want for your tokenizer.\n",
            " |          length (`int`, *optional*):\n",
            " |              The total number of sequences in the iterator. This is used to provide meaningful progress tracking\n",
            " |          new_special_tokens (list of `str` or `AddedToken`, *optional*):\n",
            " |              A list of new special tokens to add to the tokenizer you are training.\n",
            " |          special_tokens_map (`Dict[str, str]`, *optional*):\n",
            " |              If you want to rename some of the special tokens this tokenizer uses, pass along a mapping old special\n",
            " |              token name to new special token name in this argument.\n",
            " |          kwargs (`Dict[str, Any]`, *optional*):\n",
            " |              Additional keyword arguments passed along to the trainer from the 🤗 Tokenizers library.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`PreTrainedTokenizerFast`]: A new tokenizer of the same type as the original one, trained on\n",
            " |          `text_iterator`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
            " |  \n",
            " |  added_tokens_decoder\n",
            " |      Returns the added tokens in the vocabulary as a dictionary of index to AddedToken.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `Dict[str, int]`: The added tokens.\n",
            " |  \n",
            " |  added_tokens_encoder\n",
            " |      Returns the sorted mapping from string to index. The added tokens encoder is cached for performance\n",
            " |      optimisation in `self._added_tokens_encoder` for the slow tokenizers.\n",
            " |  \n",
            " |  backend_tokenizer\n",
            " |      `tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.\n",
            " |  \n",
            " |  can_save_slow_tokenizer\n",
            " |      `bool`: Whether or not the slow tokenizer can be saved. Usually for sentencepiece based slow tokenizer, this\n",
            " |      can only be `True` if the original `\"sentencepiece.model\"` was not deleted.\n",
            " |  \n",
            " |  decoder\n",
            " |      `tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.\n",
            " |  \n",
            " |  is_fast\n",
            " |  \n",
            " |  vocab\n",
            " |  \n",
            " |  vocab_size\n",
            " |      `int`: Size of the base vocabulary (without the added tokens).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |  \n",
            " |  __call__(self, text: Union[str, List[str], List[List[str]]] = None, text_pair: Union[str, List[str], List[List[str]], NoneType] = None, text_target: Union[str, List[str], List[List[str]]] = None, text_pair_target: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
            " |      sequences.\n",
            " |      \n",
            " |      Args:\n",
            " |          text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
            " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            " |          text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
            " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            " |          text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
            " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
            " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
            " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            " |          text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
            " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
            " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
            " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            " |      \n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
            " |              automatically.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence if provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
            " |      \n",
            " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
            " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            " |              argument defines the number of overlapping tokens.\n",
            " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            " |              which it will tokenize. This is useful for NER or token classification.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |      \n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |      \n",
            " |          return_token_type_ids (`bool`, *optional*):\n",
            " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |      \n",
            " |              [What are token type IDs?](../glossary#token-type-ids)\n",
            " |          return_attention_mask (`bool`, *optional*):\n",
            " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |      \n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            " |              of returning overflowing tokens.\n",
            " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return special tokens mask information.\n",
            " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return `(char_start, char_end)` for each token.\n",
            " |      \n",
            " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
            " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the lengths of the encoded inputs.\n",
            " |          verbose (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to print more information and warnings.\n",
            " |          **kwargs: passed to the `self.tokenize()` method\n",
            " |      \n",
            " |      Return:\n",
            " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            " |      \n",
            " |          - **input_ids** -- List of token ids to be fed to a model.\n",
            " |      \n",
            " |            [What are input IDs?](../glossary#input-ids)\n",
            " |      \n",
            " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            " |      \n",
            " |            [What are token type IDs?](../glossary#token-type-ids)\n",
            " |      \n",
            " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            " |      \n",
            " |            [What are attention masks?](../glossary#attention-mask)\n",
            " |      \n",
            " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  apply_chat_template(self, conversation: Union[List[Dict[str, str]], ForwardRef('Conversation')], chat_template: Optional[str] = None, add_generation_prompt: bool = False, tokenize: bool = True, padding: bool = False, truncation: bool = False, max_length: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_dict: bool = False, **tokenizer_kwargs) -> Union[str, List[int]]\n",
            " |      Converts a Conversation object or a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\n",
            " |      ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to\n",
            " |      determine the format and control tokens to use when converting. When chat_template is None, it will fall back\n",
            " |      to the default_chat_template specified at the class level.\n",
            " |      \n",
            " |      Args:\n",
            " |          conversation (Union[List[Dict[str, str]], \"Conversation\"]): A Conversation object or list of dicts\n",
            " |              with \"role\" and \"content\" keys, representing the chat history so far.\n",
            " |          chat_template (str, *optional*): A Jinja template to use for this conversion. If\n",
            " |              this is not passed, the model's default chat template will be used instead.\n",
            " |          add_generation_prompt (bool, *optional*): Whether to end the prompt with the token(s) that indicate\n",
            " |              the start of an assistant message. This is useful when you want to generate a response from the model.\n",
            " |              Note that this argument will be passed to the chat template, and so it must be supported in the\n",
            " |              template for this argument to have any effect.\n",
            " |          tokenize (`bool`, defaults to `True`):\n",
            " |              Whether to tokenize the output. If `False`, the output will be a string.\n",
            " |          padding (`bool`, defaults to `False`):\n",
            " |              Whether to pad sequences to the maximum length. Has no effect if tokenize is `False`.\n",
            " |          truncation (`bool`, defaults to `False`):\n",
            " |              Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\n",
            " |              not specified, the tokenizer's `max_length` attribute will be used as a default.\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable\n",
            " |              values are:\n",
            " |              - `'tf'`: Return TensorFlow `tf.Tensor` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return NumPy `np.ndarray` objects.\n",
            " |              - `'jax'`: Return JAX `jnp.ndarray` objects.\n",
            " |          return_dict (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n",
            " |          **tokenizer_kwargs: Additional kwargs to pass to the tokenizer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `List[int]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n",
            " |          output is ready to pass to the model, either directly or via methods like `generate()`.\n",
            " |  \n",
            " |  as_target_tokenizer(self)\n",
            " |      Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
            " |      sequence-to-sequence models that need a slightly different processing for the labels.\n",
            " |  \n",
            " |  batch_decode(self, sequences: Union[List[int], List[List[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> List[str]\n",
            " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
            " |      \n",
            " |      Args:\n",
            " |          sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
            " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
            " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to remove special tokens in the decoding.\n",
            " |          clean_up_tokenization_spaces (`bool`, *optional*):\n",
            " |              Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
            " |              `self.clean_up_tokenization_spaces`.\n",
            " |          kwargs (additional keyword arguments, *optional*):\n",
            " |              Will be passed to the underlying model specific decode method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `List[str]`: The list of decoded sentences.\n",
            " |  \n",
            " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      This method is deprecated, `__call__` should be used instead.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          batch_text_or_text_pairs (`List[str]`, `List[Tuple[str, str]]`, `List[List[str]]`, `List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also `List[List[int]]`, `List[Tuple[List[int], List[int]]]`):\n",
            " |              Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
            " |              string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
            " |              details in `encode_plus`).\n",
            " |      \n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
            " |              automatically.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence if provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
            " |      \n",
            " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
            " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            " |              argument defines the number of overlapping tokens.\n",
            " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            " |              which it will tokenize. This is useful for NER or token classification.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |      \n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |      \n",
            " |          return_token_type_ids (`bool`, *optional*):\n",
            " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |      \n",
            " |              [What are token type IDs?](../glossary#token-type-ids)\n",
            " |          return_attention_mask (`bool`, *optional*):\n",
            " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |      \n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            " |              of returning overflowing tokens.\n",
            " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return special tokens mask information.\n",
            " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return `(char_start, char_end)` for each token.\n",
            " |      \n",
            " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
            " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the lengths of the encoded inputs.\n",
            " |          verbose (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to print more information and warnings.\n",
            " |          **kwargs: passed to the `self.tokenize()` method\n",
            " |      \n",
            " |      Return:\n",
            " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            " |      \n",
            " |          - **input_ids** -- List of token ids to be fed to a model.\n",
            " |      \n",
            " |            [What are input IDs?](../glossary#input-ids)\n",
            " |      \n",
            " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            " |      \n",
            " |            [What are token type IDs?](../glossary#token-type-ids)\n",
            " |      \n",
            " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            " |      \n",
            " |            [What are attention masks?](../glossary#attention-mask)\n",
            " |      \n",
            " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
            " |  \n",
            " |  decode(self, token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str\n",
            " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
            " |      tokens and clean up tokenization spaces.\n",
            " |      \n",
            " |      Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
            " |      \n",
            " |      Args:\n",
            " |          token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
            " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
            " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to remove special tokens in the decoding.\n",
            " |          clean_up_tokenization_spaces (`bool`, *optional*):\n",
            " |              Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
            " |              `self.clean_up_tokenization_spaces`.\n",
            " |          kwargs (additional keyword arguments, *optional*):\n",
            " |              Will be passed to the underlying model specific decode method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `str`: The decoded sentence.\n",
            " |  \n",
            " |  encode(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -> List[int]\n",
            " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
            " |      \n",
            " |      Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.\n",
            " |      \n",
            " |      Args:\n",
            " |          text (`str`, `List[str]` or `List[int]`):\n",
            " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
            " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
            " |              method).\n",
            " |          text_pair (`str`, `List[str]` or `List[int]`, *optional*):\n",
            " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
            " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
            " |              method).\n",
            " |      \n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
            " |              automatically.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence if provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
            " |      \n",
            " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
            " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            " |              argument defines the number of overlapping tokens.\n",
            " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            " |              which it will tokenize. This is useful for NER or token classification.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |      \n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |      \n",
            " |          **kwargs: Passed along to the `.tokenize()` method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `List[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`: The tokenized ids of the text.\n",
            " |  \n",
            " |  encode_plus(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      This method is deprecated, `__call__` should be used instead.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          text (`str`, `List[str]` or `List[int]` (the latter only for not-fast tokenizers)):\n",
            " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
            " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
            " |              method).\n",
            " |          text_pair (`str`, `List[str]` or `List[int]`, *optional*):\n",
            " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
            " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
            " |              method).\n",
            " |      \n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
            " |              automatically.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence if provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
            " |      \n",
            " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
            " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            " |              argument defines the number of overlapping tokens.\n",
            " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            " |              which it will tokenize. This is useful for NER or token classification.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |      \n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |      \n",
            " |          return_token_type_ids (`bool`, *optional*):\n",
            " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |      \n",
            " |              [What are token type IDs?](../glossary#token-type-ids)\n",
            " |          return_attention_mask (`bool`, *optional*):\n",
            " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |      \n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            " |              of returning overflowing tokens.\n",
            " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return special tokens mask information.\n",
            " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return `(char_start, char_end)` for each token.\n",
            " |      \n",
            " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
            " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the lengths of the encoded inputs.\n",
            " |          verbose (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to print more information and warnings.\n",
            " |          **kwargs: passed to the `self.tokenize()` method\n",
            " |      \n",
            " |      Return:\n",
            " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            " |      \n",
            " |          - **input_ids** -- List of token ids to be fed to a model.\n",
            " |      \n",
            " |            [What are input IDs?](../glossary#input-ids)\n",
            " |      \n",
            " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            " |      \n",
            " |            [What are token type IDs?](../glossary#token-type-ids)\n",
            " |      \n",
            " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            " |      \n",
            " |            [What are attention masks?](../glossary#attention-mask)\n",
            " |      \n",
            " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
            " |  \n",
            " |  get_special_tokens_mask(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]\n",
            " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
            " |      special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n",
            " |      \n",
            " |      Args:\n",
            " |          token_ids_0 (`List[int]`):\n",
            " |              List of ids of the first sequence.\n",
            " |          token_ids_1 (`List[int]`, *optional*):\n",
            " |              List of ids of the second sequence.\n",
            " |          already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the token list is already formatted with special tokens for the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
            " |  \n",
            " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = True, max_length: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
            " |      in the batch.\n",
            " |      \n",
            " |      Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,\n",
            " |      `self.pad_token_id` and `self.pad_token_type_id`).\n",
            " |      \n",
            " |      Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the\n",
            " |      text followed by a call to the `pad` method to get a padded encoding.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
            " |      result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of\n",
            " |      PyTorch tensors, you will lose the specific device of your tensors however.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, List[int]]`, `Dict[str, List[List[int]]` or `List[Dict[str, List[int]]]`):\n",
            " |              Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, List[int]]`) or a batch of\n",
            " |              tokenized inputs (list of [`BatchEncoding`], *Dict[str, List[List[int]]]* or *List[Dict[str,\n",
            " |              List[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n",
            " |              collate function.\n",
            " |      \n",
            " |              Instead of `List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors), see\n",
            " |              the note above for the return type.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
            " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
            " |               index) among:\n",
            " |      \n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence if provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Maximum length of the returned list and optionally padding length (see above).\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value.\n",
            " |      \n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          return_attention_mask (`bool`, *optional*):\n",
            " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |      \n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |      \n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |          verbose (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to print more information and warnings.\n",
            " |  \n",
            " |  prepare_for_model(self, ids: List[int], pair_ids: Optional[List[int]] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
            " |      adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
            " |      manages a moving window (with user defined stride) for overflowing tokens. Please Note, for *pair_ids*\n",
            " |      different than `None` and *truncation_strategy = longest_first* or `True`, it is not possible to return\n",
            " |      overflowing tokens. Such a combination of arguments will raise an error.\n",
            " |      \n",
            " |      Args:\n",
            " |          ids (`List[int]`):\n",
            " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
            " |              `convert_tokens_to_ids` methods.\n",
            " |          pair_ids (`List[int]`, *optional*):\n",
            " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
            " |              and `convert_tokens_to_ids` methods.\n",
            " |      \n",
            " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
            " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
            " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
            " |              automatically.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence if provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
            " |      \n",
            " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
            " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            " |              argument defines the number of overlapping tokens.\n",
            " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            " |              which it will tokenize. This is useful for NER or token classification.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta).\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |      \n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |      \n",
            " |          return_token_type_ids (`bool`, *optional*):\n",
            " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |      \n",
            " |              [What are token type IDs?](../glossary#token-type-ids)\n",
            " |          return_attention_mask (`bool`, *optional*):\n",
            " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            " |      \n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            " |              of returning overflowing tokens.\n",
            " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return special tokens mask information.\n",
            " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return `(char_start, char_end)` for each token.\n",
            " |      \n",
            " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
            " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the lengths of the encoded inputs.\n",
            " |          verbose (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to print more information and warnings.\n",
            " |          **kwargs: passed to the `self.tokenize()` method\n",
            " |      \n",
            " |      Return:\n",
            " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            " |      \n",
            " |          - **input_ids** -- List of token ids to be fed to a model.\n",
            " |      \n",
            " |            [What are input IDs?](../glossary#input-ids)\n",
            " |      \n",
            " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            " |      \n",
            " |            [What are token type IDs?](../glossary#token-type-ids)\n",
            " |      \n",
            " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            " |      \n",
            " |            [What are attention masks?](../glossary#attention-mask)\n",
            " |      \n",
            " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            " |            `return_overflowing_tokens=True`).\n",
            " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
            " |  \n",
            " |  prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Optional[List[str]] = None, max_length: Optional[int] = None, max_target_length: Optional[int] = None, padding: str = 'longest', return_tensors: str = None, truncation: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
            " |      Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          src_texts (`List[str]`):\n",
            " |              List of documents to summarize or source language texts.\n",
            " |          tgt_texts (`list`, *optional*):\n",
            " |              List of summaries or target language texts.\n",
            " |          max_length (`int`, *optional*):\n",
            " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
            " |              left unset or set to `None`, this will use the predefined model maximum length if a maximum length is\n",
            " |              required by one of the truncation/padding parameters. If the model has no specific maximum input length\n",
            " |              (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            " |          max_target_length (`int`, *optional*):\n",
            " |              Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
            " |              to `None`, this will use the max_length value.\n",
            " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            " |              Activates and controls padding. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            " |                sequence if provided).\n",
            " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            " |                acceptable input length for the model if that argument is not provided.\n",
            " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            " |                lengths).\n",
            " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            " |      \n",
            " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
            " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `True`):\n",
            " |              Activates and controls truncation. Accepts the following values:\n",
            " |      \n",
            " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            " |                sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            " |                greater than the model maximum admissible input size).\n",
            " |          **kwargs:\n",
            " |              Additional keyword arguments passed along to `self.__call__`.\n",
            " |      \n",
            " |      Return:\n",
            " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            " |      \n",
            " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
            " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
            " |          - **labels** -- List of token ids for tgt_texts.\n",
            " |      \n",
            " |          The full set of keys `[input_ids, attention_mask, labels]`, will only be returned if tgt_texts is passed.\n",
            " |          Otherwise, input_ids, attention_mask will be the only keys.\n",
            " |  \n",
            " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: str = None, commit_description: str = None, tags: Optional[List[str]] = None, **deprecated_kwargs) -> str\n",
            " |      Upload the tokenizer files to the 🤗 Model Hub.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          repo_id (`str`):\n",
            " |              The name of the repository you want to push your tokenizer to. It should contain your organization name\n",
            " |              when pushing to a given organization.\n",
            " |          use_temp_dir (`bool`, *optional*):\n",
            " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
            " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
            " |          commit_message (`str`, *optional*):\n",
            " |              Message to commit while pushing. Will default to `\"Upload tokenizer\"`.\n",
            " |          private (`bool`, *optional*):\n",
            " |              Whether or not the repository created should be private.\n",
            " |          token (`bool` or `str`, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
            " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
            " |              is not specified.\n",
            " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
            " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
            " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
            " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
            " |              Google Colab instances without any CPU OOM issues.\n",
            " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
            " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
            " |          revision (`str`, *optional*):\n",
            " |              Branch to push the uploaded files to.\n",
            " |          commit_description (`str`, *optional*):\n",
            " |              The description of the commit that will be created\n",
            " |          tags (`List[str]`, *optional*):\n",
            " |              List of tags to push on the Hub.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      from transformers import AutoTokenizer\n",
            " |      \n",
            " |      tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
            " |      \n",
            " |      # Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\n",
            " |      tokenizer.push_to_hub(\"my-finetuned-bert\")\n",
            " |      \n",
            " |      # Push the tokenizer to an organization with the name \"my-finetuned-bert\".\n",
            " |      tokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
            " |      ```\n",
            " |  \n",
            " |  save_pretrained(self, save_directory: Union[str, os.PathLike], legacy_format: Optional[bool] = None, filename_prefix: Optional[str] = None, push_to_hub: bool = False, **kwargs) -> Tuple[str]\n",
            " |      Save the full tokenizer state.\n",
            " |      \n",
            " |      \n",
            " |      This method make sure the full tokenizer can then be re-loaded using the\n",
            " |      [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`] class method..\n",
            " |      \n",
            " |      Warning,None This won't save modifications you may have applied to the tokenizer after the instantiation (for\n",
            " |      instance, modifying `tokenizer.do_lower_case` after creation).\n",
            " |      \n",
            " |      Args:\n",
            " |          save_directory (`str` or `os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
            " |          legacy_format (`bool`, *optional*):\n",
            " |              Only applicable for a fast tokenizer. If unset (default), will save the tokenizer in the unified JSON\n",
            " |              format as well as in legacy format if it exists, i.e. with tokenizer specific vocabulary and a separate\n",
            " |              added_tokens files.\n",
            " |      \n",
            " |              If `False`, will only save the tokenizer in the unified JSON format. This format is incompatible with\n",
            " |              \"slow\" tokenizers (not powered by the *tokenizers* library), so the tokenizer will not be able to be\n",
            " |              loaded in the corresponding \"slow\" tokenizer.\n",
            " |      \n",
            " |              If `True`, will save the tokenizer in legacy format. If the \"slow\" tokenizer doesn't exits, a value\n",
            " |              error is raised.\n",
            " |          filename_prefix (`str`, *optional*):\n",
            " |              A prefix to add to the names of the files saved by the tokenizer.\n",
            " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
            " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
            " |              namespace).\n",
            " |          kwargs (`Dict[str, Any]`, *optional*):\n",
            " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tuple of `str`: The files saved.\n",
            " |  \n",
            " |  truncate_sequences(self, ids: List[int], pair_ids: Optional[List[int]] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]\n",
            " |      Truncates a sequence pair in-place following the strategy.\n",
            " |      \n",
            " |      Args:\n",
            " |          ids (`List[int]`):\n",
            " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
            " |              `convert_tokens_to_ids` methods.\n",
            " |          pair_ids (`List[int]`, *optional*):\n",
            " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
            " |              and `convert_tokens_to_ids` methods.\n",
            " |          num_tokens_to_remove (`int`, *optional*, defaults to 0):\n",
            " |              Number of tokens to remove using the truncation strategy.\n",
            " |          truncation_strategy (`str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            " |              The strategy to follow for truncation. Can be:\n",
            " |      \n",
            " |              - `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will truncate\n",
            " |                token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a\n",
            " |                batch of pairs) is provided.\n",
            " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
            " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            " |              - `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths greater\n",
            " |                than the model maximum admissible input size).\n",
            " |          stride (`int`, *optional*, defaults to 0):\n",
            " |              If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
            " |              sequence returned. The value of this argument defines the number of additional tokens.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `Tuple[List[int], List[int], List[int]]`: The truncated `ids`, the truncated `pair_ids` and the list of\n",
            " |          overflowing tokens. Note: The *longest_first* strategy returns empty list of overflowing tokens if a pair\n",
            " |          of sequences (or a batch of pairs) is provided.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |  \n",
            " |  convert_added_tokens(obj: Union[tokenizers.AddedToken, Any], save=False, add_type_field=True) from builtins.type\n",
            " |  \n",
            " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', trust_remote_code=False, **kwargs) from builtins.type\n",
            " |      Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined\n",
            " |      tokenizer.\n",
            " |      \n",
            " |      Args:\n",
            " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
            " |              Can be either:\n",
            " |      \n",
            " |              - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
            " |              - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
            " |                using the [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`] method, e.g.,\n",
            " |                `./my_model_directory/`.\n",
            " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
            " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
            " |                `./my_model_directory/vocab.txt`.\n",
            " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
            " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
            " |              standard cache should not be used.\n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
            " |              exist.\n",
            " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
            " |              exists.\n",
            " |          proxies (`Dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          token (`str` or *bool*, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
            " |              when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
            " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to only rely on local files and not to attempt to download any files.\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
            " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
            " |              identifier allowed by git.\n",
            " |          subfolder (`str`, *optional*):\n",
            " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
            " |              facebook/rag-token-base), specify it here.\n",
            " |          inputs (additional positional arguments, *optional*):\n",
            " |              Will be passed along to the Tokenizer `__init__` method.\n",
            " |          trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
            " |              should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
            " |              execute code present on the Hub on your local machine.\n",
            " |          kwargs (additional keyword arguments, *optional*):\n",
            " |              Will be passed to the Tokenizer `__init__` method. Can be used to set special tokens like `bos_token`,\n",
            " |              `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
            " |              `additional_special_tokens`. See parameters in the `__init__` for more details.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      Passing `token=True` is required when you want to use a private model.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      # We can't instantiate directly the base class *PreTrainedTokenizerBase* so let's show our examples on a derived class: BertTokenizer\n",
            " |      # Download vocabulary from huggingface.co and cache.\n",
            " |      tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
            " |      \n",
            " |      # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
            " |      tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
            " |      \n",
            " |      # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
            " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/\")\n",
            " |      \n",
            " |      # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
            " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/my_vocab.txt\")\n",
            " |      \n",
            " |      # You can link tokens to special vocabulary when instantiating\n",
            " |      tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", unk_token=\"<unk>\")\n",
            " |      # You should be sure '<unk>' is in the vocabulary when doing that.\n",
            " |      # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
            " |      assert tokenizer.unk_token == \"<unk>\"\n",
            " |      ```\n",
            " |  \n",
            " |  register_for_auto_class(auto_class='AutoTokenizer') from builtins.type\n",
            " |      Register this class with a given auto class. This should only be used for custom tokenizers as the ones in the\n",
            " |      library are already mapped with `AutoTokenizer`.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoTokenizer\"`):\n",
            " |              The auto class to register this new tokenizer with.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |  \n",
            " |  clean_up_tokenization(out_string: str) -> str\n",
            " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
            " |      \n",
            " |      Args:\n",
            " |          out_string (`str`): The text to clean up.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `str`: The cleaned-up string.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |  \n",
            " |  default_chat_template\n",
            " |      This template formats inputs in the standard ChatML format. See\n",
            " |      https://github.com/openai/openai-python/blob/main/chatml.md\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |  \n",
            " |  max_len_sentences_pair\n",
            " |      `int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
            " |  \n",
            " |  max_len_single_sentence\n",
            " |      `int`: The maximum length of a sentence that can be fed to the model.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
            " |  \n",
            " |  padding_side = 'right'\n",
            " |  \n",
            " |  truncation_side = 'right'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
            " |  \n",
            " |  add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]], replace_additional_special_tokens=True) -> int\n",
            " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
            " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
            " |      current vocabulary).\n",
            " |      \n",
            " |      When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of the\n",
            " |      model so that its embedding matrix matches the tokenizer.\n",
            " |      \n",
            " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
            " |      \n",
            " |      Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n",
            " |      \n",
            " |      - Special tokens can be skipped when decoding using `skip_special_tokens = True`.\n",
            " |      - Special tokens are carefully handled by the tokenizer (they are never split), similar to `AddedTokens`.\n",
            " |      - You can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This\n",
            " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
            " |      \n",
            " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
            " |      [`BertTokenizer`] `cls_token` is already registered to be :obj*'[CLS]'* and XLM's one is also registered to be\n",
            " |      `'</s>'`).\n",
            " |      \n",
            " |      Args:\n",
            " |          special_tokens_dict (dictionary *str* to *str* or `tokenizers.AddedToken`):\n",
            " |              Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`,\n",
            " |              `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
            " |      \n",
            " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
            " |              assign the index of the `unk_token` to them).\n",
            " |          replace_additional_special_tokens (`bool`, *optional*,, defaults to `True`):\n",
            " |              If `True`, the existing list of additional special tokens will be replaced by the list provided in\n",
            " |              `special_tokens_dict`. Otherwise, `self._additional_special_tokens` is just extended. In the former\n",
            " |              case, the tokens will NOT be removed from the tokenizer's full vocabulary - they are only being flagged\n",
            " |              as non-special tokens. Remember, this only affects which tokens are skipped during decoding, not the\n",
            " |              `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous\n",
            " |              `additional_special_tokens` are still added tokens, and will not be split by the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: Number of tokens added to the vocabulary.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      # Let's see how to add a new classification token to GPT-2\n",
            " |      tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
            " |      model = GPT2Model.from_pretrained(\"openai-community/gpt2\")\n",
            " |      \n",
            " |      special_tokens_dict = {\"cls_token\": \"<CLS>\"}\n",
            " |      \n",
            " |      num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
            " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
            " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
            " |      model.resize_token_embeddings(len(tokenizer))\n",
            " |      \n",
            " |      assert tokenizer.cls_token == \"<CLS>\"\n",
            " |      ```\n",
            " |  \n",
            " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
            " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
            " |      it with indices starting from length of the current vocabulary and and will be isolated before the tokenization\n",
            " |      algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore\n",
            " |      not treated in the same way.\n",
            " |      \n",
            " |      Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix\n",
            " |      of the model so that its embedding matrix matches the tokenizer.\n",
            " |      \n",
            " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
            " |      \n",
            " |      Args:\n",
            " |          new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`):\n",
            " |              Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string\n",
            " |              token to let you personalize its behavior: whether this token should only match against a single word,\n",
            " |              whether this token should strip all potential whitespaces on the left side, whether this token should\n",
            " |              strip all potential whitespaces on the right side, etc.\n",
            " |          special_tokens (`bool`, *optional*, defaults to `False`):\n",
            " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
            " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
            " |      \n",
            " |              See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: Number of tokens added to the vocabulary.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
            " |      tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
            " |      model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
            " |      \n",
            " |      num_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\n",
            " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
            " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
            " |      model.resize_token_embeddings(len(tokenizer))\n",
            " |      ```\n",
            " |  \n",
            " |  sanitize_special_tokens(self) -> int\n",
            " |      The `sanitize_special_tokens` is now deprecated kept for backward compatibility and will be removed in\n",
            " |      transformers v5.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
            " |  \n",
            " |  all_special_ids\n",
            " |      `List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\n",
            " |  \n",
            " |  all_special_tokens\n",
            " |      `List[str]`: A list of the unique special tokens (`'<unk>'`, `'<cls>'`, ..., etc.).\n",
            " |      \n",
            " |      Convert tokens of `tokenizers.AddedToken` type to string.\n",
            " |  \n",
            " |  all_special_tokens_extended\n",
            " |      `List[Union[str, tokenizers.AddedToken]]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.), the order has\n",
            " |      nothing to do with the index of each tokens. If you want to know the correct indices, check\n",
            " |      `self.added_tokens_encoder`. We can't create an order anymore as the keys are `AddedTokens` and not `Strings`.\n",
            " |      \n",
            " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
            " |      special tokens are tokenized.\n",
            " |  \n",
            " |  pad_token_type_id\n",
            " |      `int`: Id of the padding token type in the vocabulary.\n",
            " |  \n",
            " |  special_tokens_map\n",
            " |      `Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (`cls_token`,\n",
            " |      `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
            " |      \n",
            " |      Convert potential tokens of `tokenizers.AddedToken` type to string.\n",
            " |  \n",
            " |  special_tokens_map_extended\n",
            " |      `Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary mapping\n",
            " |      special token class attributes (`cls_token`, `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
            " |      \n",
            " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
            " |      special tokens are tokenized.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  additional_special_tokens\n",
            " |      `List[str]`: All the additional special tokens you may want to use. Log an error if used while not having been\n",
            " |      set.\n",
            " |  \n",
            " |  additional_special_tokens_ids\n",
            " |      `List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not having\n",
            " |      been set.\n",
            " |  \n",
            " |  bos_token\n",
            " |      `str`: Beginning of sentence token. Log an error if used while not having been set.\n",
            " |  \n",
            " |  bos_token_id\n",
            " |      `Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns `None` if the token has not\n",
            " |      been set.\n",
            " |  \n",
            " |  cls_token\n",
            " |      `str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the full\n",
            " |      depth of the model. Log an error if used while not having been set.\n",
            " |  \n",
            " |  cls_token_id\n",
            " |      `Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input sequence\n",
            " |      leveraging self-attention along the full depth of the model.\n",
            " |      \n",
            " |      Returns `None` if the token has not been set.\n",
            " |  \n",
            " |  eos_token\n",
            " |      `str`: End of sentence token. Log an error if used while not having been set.\n",
            " |  \n",
            " |  eos_token_id\n",
            " |      `Optional[int]`: Id of the end of sentence token in the vocabulary. Returns `None` if the token has not been\n",
            " |      set.\n",
            " |  \n",
            " |  mask_token\n",
            " |      `str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while not\n",
            " |      having been set.\n",
            " |  \n",
            " |  mask_token_id\n",
            " |      `Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
            " |      modeling. Returns `None` if the token has not been set.\n",
            " |  \n",
            " |  pad_token\n",
            " |      `str`: Padding token. Log an error if used while not having been set.\n",
            " |  \n",
            " |  pad_token_id\n",
            " |      `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n",
            " |  \n",
            " |  sep_token\n",
            " |      `str`: Separation token, to separate context and query in an input sequence. Log an error if used while not\n",
            " |      having been set.\n",
            " |  \n",
            " |  sep_token_id\n",
            " |      `Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
            " |      sequence. Returns `None` if the token has not been set.\n",
            " |  \n",
            " |  unk_token\n",
            " |      `str`: Unknown token. Log an error if used while not having been set.\n",
            " |  \n",
            " |  unk_token_id\n",
            " |      `Optional[int]`: Id of the unknown token in the vocabulary. Returns `None` if the token has not been set.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
            " |  \n",
            " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"this is awesome\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3AEUMJOc1nd",
        "outputId": "d03c85f9-350e-41d3-93d1-42cfa934db46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 10531, 10124, 56237, 41939, 10627, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Albert tokenization\n",
        "albert_tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n",
        "\n",
        "albert_tokenizer(\"this is awesome.\",\" sencond sentence\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180,
          "referenced_widgets": [
            "6493f02ad3e644dda7b217063d0b45bb",
            "875904bfbe1b4ad09d297a83acb27622",
            "3d6947214d16470bbc2e1e4547292bc0",
            "0645b9158f264822a54cf3a9527ddbbc",
            "05d12aaf5b7841e19f46419bef0030ee",
            "922bc33d383a4a56b44c0972b517ab68",
            "fb4e906f36394329a0e0ac0d5ca58614",
            "a295b2d2798b41e08707fec31d2e3f3d",
            "b098d01b0c774239981535bf2ff97111",
            "131503859e634097a841f66192b34cbc",
            "a7fef936064646349761c8ab40707610",
            "e9d1137d7fb44008b3ce580a6cdbc3d6",
            "0fb8291bc3df4819bfdc8958350db4f3",
            "c71116a9119b4e568cefcd6b8013603e",
            "e5210fd37cab4792af87bdc56d7d3236",
            "7ff4e5b1df43451e9a1a01301aa55c66",
            "711983895ea2473fa054eb1b90413530",
            "bb2bc29c2ba340879cbda20195cb17cb",
            "ef083008d47640d9b68b65ad8491b3a8",
            "5d1105545eb3425b97f4b30f479b9fd5",
            "7555460763f64cdaabff6c09a1814ff6",
            "97e427a387fd46a0a2345fb11ad281a9",
            "0c0d7e35eaad4767a2cdacf393b9b994",
            "13857956f50949f0bf90547dcfd3cd34",
            "d499a5fe2928435692de2a1a08fba4f3",
            "e801ffedab3344e99a8067361b25dc2a",
            "a89449cc0c904de58b9a4d2a6d0de806",
            "a9a820699fa74ca7bc0d01b3e1e783d8",
            "c8de952bf59c44fdb8cdd553d9a856bb",
            "468d7103c43542a98dd6256b583d2fd0",
            "d37c8705051d416a98a7ae1428315e23",
            "32ea2b394ef34146b72fbb03e58390f6",
            "50421cb3f07046bfa349c66e89d6d1f7",
            "85e4b67fd0314938bead37023dbf819f",
            "10b9ebc19bd04c95ab4566d6420ca40f",
            "c1735b17cd7942c0930340e739be904b",
            "6ed292820d6e4fdeb03cf6edae59788b",
            "6ce25c66cadc4f12af9f7d54cc8fe2dc",
            "20119087de364bfab19b6c4ee517bcbd",
            "76f6caf9c6134ca98dec0f94df93f6f4",
            "ce5dd2e167074119976b52a3cd4270e9",
            "311e2f40b0de4b6c89896333fbab0d2d",
            "17f07a93688448218acfcbbdf4130c13",
            "0015b5c0660e425c812a2570bc49c402"
          ]
        },
        "id": "whcGABibc1px",
        "outputId": "51231e0c-b137-46bb-963a-7926d3bbea7a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6493f02ad3e644dda7b217063d0b45bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9d1137d7fb44008b3ce580a6cdbc3d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c0d7e35eaad4767a2cdacf393b9b994"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85e4b67fd0314938bead37023dbf819f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [2, 48, 25, 13706, 9, 3, 8252, 1126, 43, 5123, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = [\"this is the best product, I have seen in my life\",\n",
        "             \"not much good quality\",\n",
        "             \"it is good\"]\n",
        "\n",
        "inputs = tokenizer(text_list, padding=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "5hqhpoGXc1tA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCUd21IYc1wb",
        "outputId": "f1e5ecf2-79b4-484b-eb9e-a87a6330a367"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101, 10531, 10124, 10105, 12504, 21535,   117,   146, 10529, 15652,\n",
              "         10106, 15127, 12103,   102],\n",
              "        [  101, 10472, 13172, 15198, 21905,   102,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0],\n",
              "        [  101, 10271, 10124, 15198,   102,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=AutoModel.from_pretrained(model_name)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584,
          "referenced_widgets": [
            "4edfb6e775f5466b9e1fb068750c9c58",
            "3681476488544e61965bf98c63070b0c",
            "055f8dc311b04c70b65efb51cdd51ca1",
            "94303e647ee54731a9ea72c44b341be8",
            "f21520b665e04421968cfbea30b01882",
            "05b351f8a7724f48a8aa129e60ce4bdc",
            "fae1ef33f8c7427aab2d71fbaac2fae6",
            "4217df52a76c4b59860cf49c9376f277",
            "ded2ecfce1564065a94bfa1c20296868",
            "70c5158870ad42ca8a9f5bdc1b31476c",
            "da7c4160cd62475ba6823349001550b8",
            "3f3afa38ca0044da81aafb35c731ae6f",
            "a913ba9d55054f43bae070d8a650adab",
            "d8ffc63ef61b4331829942e8996bc1f9",
            "dcd79a3f7e5f43608824ebd88f749a91",
            "1b1a1a5d087141568f91f5fc6ff3b404",
            "56e019159b8848a78217f25723dbc046",
            "e202fa6e1ebc40be9548f71f6286a960",
            "6b64b9d61604483c8c33013f7b81f5e4",
            "2abae425e66c46b694d10340a6b4790f",
            "956fed4991144fbbb838eebf942cb9c6",
            "ad39be40f0ee44489c8d890fe5135810"
          ]
        },
        "id": "TI5hSz_vVHGQ",
        "outputId": "fd12216b-713d-49ad-ce78-486281cc8263"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/759 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4edfb6e775f5466b9e1fb068750c9c58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f3afa38ca0044da81aafb35c731ae6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=model(**inputs)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ouK_BpAVHJS",
        "outputId": "eb9f6131-13c1-4d4d-b341-761597a32209"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutput(last_hidden_state=tensor([[[ 0.2185, -0.8947,  0.7776,  ...,  0.4803,  0.1357, -0.1178],\n",
              "         [ 0.2059, -0.8378,  0.7618,  ...,  0.4197,  0.2107, -0.0065],\n",
              "         [ 0.2006, -0.9050,  0.7894,  ...,  0.4194,  0.2098, -0.0875],\n",
              "         ...,\n",
              "         [ 0.2746, -0.7682,  0.8213,  ...,  0.4166,  0.0746,  0.0090],\n",
              "         [ 0.1933, -0.8005,  0.8880,  ...,  0.5430,  0.1639, -0.0665],\n",
              "         [ 0.2792, -0.7374,  0.7649,  ...,  0.3268,  0.0784, -0.0327]],\n",
              "\n",
              "        [[-0.7690, -0.0136, -0.6106,  ...,  0.5343, -0.2228,  0.0343],\n",
              "         [-0.6660,  0.0120, -0.6141,  ...,  0.4998, -0.2488,  0.0520],\n",
              "         [-0.6519,  0.2002, -0.5762,  ...,  0.5481, -0.2708,  0.1586],\n",
              "         ...,\n",
              "         [-0.6753,  0.0586, -0.4098,  ...,  0.5080, -0.2040, -0.0037],\n",
              "         [-0.7185,  0.0427, -0.4284,  ...,  0.5042, -0.2329, -0.0015],\n",
              "         [-0.7051,  0.0708, -0.3990,  ...,  0.4515, -0.1360, -0.0763]],\n",
              "\n",
              "        [[ 0.2307, -1.1705,  0.8783,  ...,  0.4212,  0.3630, -0.4405],\n",
              "         [ 0.1818, -1.1090,  0.9043,  ...,  0.4086,  0.3377, -0.4024],\n",
              "         [ 0.2337, -1.1158,  0.8484,  ...,  0.4106,  0.3784, -0.4184],\n",
              "         ...,\n",
              "         [ 0.3257, -1.1163,  0.8654,  ...,  0.3938,  0.3377, -0.3950],\n",
              "         [ 0.3138, -1.1248,  0.8287,  ...,  0.3429,  0.3375, -0.4261],\n",
              "         [ 0.3086, -1.1258,  0.8554,  ...,  0.3635,  0.3550, -0.4132]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02143lksf0B3",
        "outputId": "6156dd21-fca9-4942-9f67-1deaf046d4aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on BaseModelOutput in module transformers.modeling_outputs object:\n",
            "\n",
            "class BaseModelOutput(transformers.utils.generic.ModelOutput)\n",
            " |  BaseModelOutput(last_hidden_state: torch.FloatTensor = None, hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None, attentions: Optional[Tuple[torch.FloatTensor, ...]] = None) -> None\n",
            " |  \n",
            " |  Base class for model's outputs, with potential hidden states and attentions.\n",
            " |  \n",
            " |  Args:\n",
            " |      last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
            " |          Sequence of hidden-states at the output of the last layer of the model.\n",
            " |      hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
            " |          Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
            " |          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
            " |  \n",
            " |          Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
            " |      attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
            " |          Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
            " |          sequence_length)`.\n",
            " |  \n",
            " |          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
            " |          heads.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      BaseModelOutput\n",
            " |      transformers.utils.generic.ModelOutput\n",
            " |      collections.OrderedDict\n",
            " |      builtins.dict\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __eq__(self, other)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __init__(self, last_hidden_state: torch.FloatTensor = None, hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None, attentions: Optional[Tuple[torch.FloatTensor, ...]] = None) -> None\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'attentions': typing.Optional[typing.Tuple[torch.Fl...\n",
            " |  \n",
            " |  __dataclass_fields__ = {'attentions': Field(name='attentions',type=typ...\n",
            " |  \n",
            " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __match_args__ = ('last_hidden_state', 'hidden_states', 'attentions')\n",
            " |  \n",
            " |  attentions = None\n",
            " |  \n",
            " |  hidden_states = None\n",
            " |  \n",
            " |  last_hidden_state = None\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
            " |  \n",
            " |  __delitem__(self, *args, **kwargs)\n",
            " |      Delete self[key].\n",
            " |  \n",
            " |  __getitem__(self, k)\n",
            " |      x.__getitem__(y) <==> x[y]\n",
            " |  \n",
            " |  __post_init__(self)\n",
            " |      Check the ModelOutput dataclass.\n",
            " |      \n",
            " |      Only occurs if @dataclass decorator has been used.\n",
            " |  \n",
            " |  __reduce__(self)\n",
            " |      Return state information for pickling\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setitem__(self, key, value)\n",
            " |      Set self[key] to value.\n",
            " |  \n",
            " |  pop(self, *args, **kwargs)\n",
            " |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
            " |      \n",
            " |      If the key is not found, return the default if given; otherwise,\n",
            " |      raise a KeyError.\n",
            " |  \n",
            " |  setdefault(self, *args, **kwargs)\n",
            " |      Insert key with a value of default if key is not in the dictionary.\n",
            " |      \n",
            " |      Return the value for key if key is in the dictionary, else default.\n",
            " |  \n",
            " |  to_tuple(self) -> Tuple[Any]\n",
            " |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
            " |  \n",
            " |  update(self, *args, **kwargs)\n",
            " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
            " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
            " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
            " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
            " |  \n",
            " |  __init_subclass__() -> None from builtins.type\n",
            " |      Register subclasses as pytree nodes.\n",
            " |      \n",
            " |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
            " |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from collections.OrderedDict:\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __ior__(self, value, /)\n",
            " |      Return self|=value.\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __or__(self, value, /)\n",
            " |      Return self|value.\n",
            " |  \n",
            " |  __reversed__(...)\n",
            " |      od.__reversed__() <==> reversed(od)\n",
            " |  \n",
            " |  __ror__(self, value, /)\n",
            " |      Return value|self.\n",
            " |  \n",
            " |  __sizeof__(...)\n",
            " |      D.__sizeof__() -> size of D in memory, in bytes\n",
            " |  \n",
            " |  clear(...)\n",
            " |      od.clear() -> None.  Remove all items from od.\n",
            " |  \n",
            " |  copy(...)\n",
            " |      od.copy() -> a shallow copy of od\n",
            " |  \n",
            " |  items(...)\n",
            " |      D.items() -> a set-like object providing a view on D's items\n",
            " |  \n",
            " |  keys(...)\n",
            " |      D.keys() -> a set-like object providing a view on D's keys\n",
            " |  \n",
            " |  move_to_end(self, /, key, last=True)\n",
            " |      Move an existing element to the end (or beginning if last is false).\n",
            " |      \n",
            " |      Raise KeyError if the element does not exist.\n",
            " |  \n",
            " |  popitem(self, /, last=True)\n",
            " |      Remove and return a (key, value) pair from the dictionary.\n",
            " |      \n",
            " |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
            " |  \n",
            " |  values(...)\n",
            " |      D.values() -> an object providing a view on D's values\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from collections.OrderedDict:\n",
            " |  \n",
            " |  fromkeys(iterable, value=None) from builtins.type\n",
            " |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from collections.OrderedDict:\n",
            " |  \n",
            " |  __dict__\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from builtins.dict:\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      True if the dictionary has the specified key, else False.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  get(self, key, default=None, /)\n",
            " |      Return the value for key if key is in the dictionary, else default.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from builtins.dict:\n",
            " |  \n",
            " |  __class_getitem__(...) from builtins.type\n",
            " |      See PEP 585\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from builtins.dict:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.last_hidden_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1noiaFKxf0Eh",
        "outputId": "58400d1f-bb4b-43d5-cd0b-b2cae6a3995c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2185, -0.8947,  0.7776,  ...,  0.4803,  0.1357, -0.1178],\n",
              "         [ 0.2059, -0.8378,  0.7618,  ...,  0.4197,  0.2107, -0.0065],\n",
              "         [ 0.2006, -0.9050,  0.7894,  ...,  0.4194,  0.2098, -0.0875],\n",
              "         ...,\n",
              "         [ 0.2746, -0.7682,  0.8213,  ...,  0.4166,  0.0746,  0.0090],\n",
              "         [ 0.1933, -0.8005,  0.8880,  ...,  0.5430,  0.1639, -0.0665],\n",
              "         [ 0.2792, -0.7374,  0.7649,  ...,  0.3268,  0.0784, -0.0327]],\n",
              "\n",
              "        [[-0.7690, -0.0136, -0.6106,  ...,  0.5343, -0.2228,  0.0343],\n",
              "         [-0.6660,  0.0120, -0.6141,  ...,  0.4998, -0.2488,  0.0520],\n",
              "         [-0.6519,  0.2002, -0.5762,  ...,  0.5481, -0.2708,  0.1586],\n",
              "         ...,\n",
              "         [-0.6753,  0.0586, -0.4098,  ...,  0.5080, -0.2040, -0.0037],\n",
              "         [-0.7185,  0.0427, -0.4284,  ...,  0.5042, -0.2329, -0.0015],\n",
              "         [-0.7051,  0.0708, -0.3990,  ...,  0.4515, -0.1360, -0.0763]],\n",
              "\n",
              "        [[ 0.2307, -1.1705,  0.8783,  ...,  0.4212,  0.3630, -0.4405],\n",
              "         [ 0.1818, -1.1090,  0.9043,  ...,  0.4086,  0.3377, -0.4024],\n",
              "         [ 0.2337, -1.1158,  0.8484,  ...,  0.4106,  0.3784, -0.4184],\n",
              "         ...,\n",
              "         [ 0.3257, -1.1163,  0.8654,  ...,  0.3938,  0.3377, -0.3950],\n",
              "         [ 0.3138, -1.1248,  0.8287,  ...,  0.3429,  0.3375, -0.4261],\n",
              "         [ 0.3086, -1.1258,  0.8554,  ...,  0.3635,  0.3550, -0.4132]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.last_hidden_state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVkeya3Rf0Hc",
        "outputId": "78c40217-457f-411a-b136-926d46423e3c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 14, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "ATqBpAYhf0Lh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(AutoModelForSequenceClassification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJC7-2cfh7bT",
        "outputId": "8b30aa4a-d2e8-4a6a-ee0b-543c02161215"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_model_mapping',\n",
              " 'from_config',\n",
              " 'from_pretrained',\n",
              " 'register']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(AutoModelForSequenceClassification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZxKNdm1f0PJ",
        "outputId": "4db805e6-cdba-481f-c21b-29ec10f26bb6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class AutoModelForSequenceClassification in module transformers.models.auto.modeling_auto:\n",
            "\n",
            "class AutoModelForSequenceClassification(transformers.models.auto.auto_factory._BaseAutoModelClass)\n",
            " |  AutoModelForSequenceClassification(*args, **kwargs)\n",
            " |  \n",
            " |  This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created\n",
            " |  with the [`~AutoModelForSequenceClassification.from_pretrained`] class method or the [`~AutoModelForSequenceClassification.from_config`] class\n",
            " |  method.\n",
            " |  \n",
            " |  This class cannot be instantiated directly using `__init__()` (throws an error).\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      AutoModelForSequenceClassification\n",
            " |      transformers.models.auto.auto_factory._BaseAutoModelClass\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(**kwargs) from builtins.type\n",
            " |      Instantiates one of the model classes of the library (with a sequence classification head) from a configuration.\n",
            " |      \n",
            " |      Note:\n",
            " |          Loading a model from its configuration file does **not** load the model weights. It only affects the\n",
            " |          model's configuration. Use [`~AutoModelForSequenceClassification.from_pretrained`] to load the model weights.\n",
            " |      \n",
            " |      Args:\n",
            " |          config ([`PretrainedConfig`]):\n",
            " |              The model class to instantiate is selected based on the configuration class:\n",
            " |      \n",
            " |              - [`AlbertConfig`] configuration class: [`AlbertForSequenceClassification`] (ALBERT model)\n",
            " |              - [`BartConfig`] configuration class: [`BartForSequenceClassification`] (BART model)\n",
            " |              - [`BertConfig`] configuration class: [`BertForSequenceClassification`] (BERT model)\n",
            " |              - [`BigBirdConfig`] configuration class: [`BigBirdForSequenceClassification`] (BigBird model)\n",
            " |              - [`BigBirdPegasusConfig`] configuration class: [`BigBirdPegasusForSequenceClassification`] (BigBird-Pegasus model)\n",
            " |              - [`BioGptConfig`] configuration class: [`BioGptForSequenceClassification`] (BioGpt model)\n",
            " |              - [`BloomConfig`] configuration class: [`BloomForSequenceClassification`] (BLOOM model)\n",
            " |              - [`CTRLConfig`] configuration class: [`CTRLForSequenceClassification`] (CTRL model)\n",
            " |              - [`CamembertConfig`] configuration class: [`CamembertForSequenceClassification`] (CamemBERT model)\n",
            " |              - [`CanineConfig`] configuration class: [`CanineForSequenceClassification`] (CANINE model)\n",
            " |              - [`ConvBertConfig`] configuration class: [`ConvBertForSequenceClassification`] (ConvBERT model)\n",
            " |              - [`Data2VecTextConfig`] configuration class: [`Data2VecTextForSequenceClassification`] (Data2VecText model)\n",
            " |              - [`DebertaConfig`] configuration class: [`DebertaForSequenceClassification`] (DeBERTa model)\n",
            " |              - [`DebertaV2Config`] configuration class: [`DebertaV2ForSequenceClassification`] (DeBERTa-v2 model)\n",
            " |              - [`DistilBertConfig`] configuration class: [`DistilBertForSequenceClassification`] (DistilBERT model)\n",
            " |              - [`ElectraConfig`] configuration class: [`ElectraForSequenceClassification`] (ELECTRA model)\n",
            " |              - [`ErnieConfig`] configuration class: [`ErnieForSequenceClassification`] (ERNIE model)\n",
            " |              - [`ErnieMConfig`] configuration class: [`ErnieMForSequenceClassification`] (ErnieM model)\n",
            " |              - [`EsmConfig`] configuration class: [`EsmForSequenceClassification`] (ESM model)\n",
            " |              - [`FNetConfig`] configuration class: [`FNetForSequenceClassification`] (FNet model)\n",
            " |              - [`FalconConfig`] configuration class: [`FalconForSequenceClassification`] (Falcon model)\n",
            " |              - [`FlaubertConfig`] configuration class: [`FlaubertForSequenceClassification`] (FlauBERT model)\n",
            " |              - [`FunnelConfig`] configuration class: [`FunnelForSequenceClassification`] (Funnel Transformer model)\n",
            " |              - [`GPT2Config`] configuration class: [`GPT2ForSequenceClassification`] (OpenAI GPT-2 model)\n",
            " |              - [`GPTBigCodeConfig`] configuration class: [`GPTBigCodeForSequenceClassification`] (GPTBigCode model)\n",
            " |              - [`GPTJConfig`] configuration class: [`GPTJForSequenceClassification`] (GPT-J model)\n",
            " |              - [`GPTNeoConfig`] configuration class: [`GPTNeoForSequenceClassification`] (GPT Neo model)\n",
            " |              - [`GPTNeoXConfig`] configuration class: [`GPTNeoXForSequenceClassification`] (GPT NeoX model)\n",
            " |              - [`GemmaConfig`] configuration class: [`GemmaForSequenceClassification`] (Gemma model)\n",
            " |              - [`IBertConfig`] configuration class: [`IBertForSequenceClassification`] (I-BERT model)\n",
            " |              - [`LEDConfig`] configuration class: [`LEDForSequenceClassification`] (LED model)\n",
            " |              - [`LayoutLMConfig`] configuration class: [`LayoutLMForSequenceClassification`] (LayoutLM model)\n",
            " |              - [`LayoutLMv2Config`] configuration class: [`LayoutLMv2ForSequenceClassification`] (LayoutLMv2 model)\n",
            " |              - [`LayoutLMv3Config`] configuration class: [`LayoutLMv3ForSequenceClassification`] (LayoutLMv3 model)\n",
            " |              - [`LiltConfig`] configuration class: [`LiltForSequenceClassification`] (LiLT model)\n",
            " |              - [`LlamaConfig`] configuration class: [`LlamaForSequenceClassification`] (LLaMA model)\n",
            " |              - [`LongformerConfig`] configuration class: [`LongformerForSequenceClassification`] (Longformer model)\n",
            " |              - [`LukeConfig`] configuration class: [`LukeForSequenceClassification`] (LUKE model)\n",
            " |              - [`MBartConfig`] configuration class: [`MBartForSequenceClassification`] (mBART model)\n",
            " |              - [`MPNetConfig`] configuration class: [`MPNetForSequenceClassification`] (MPNet model)\n",
            " |              - [`MT5Config`] configuration class: [`MT5ForSequenceClassification`] (MT5 model)\n",
            " |              - [`MarkupLMConfig`] configuration class: [`MarkupLMForSequenceClassification`] (MarkupLM model)\n",
            " |              - [`MegaConfig`] configuration class: [`MegaForSequenceClassification`] (MEGA model)\n",
            " |              - [`MegatronBertConfig`] configuration class: [`MegatronBertForSequenceClassification`] (Megatron-BERT model)\n",
            " |              - [`MistralConfig`] configuration class: [`MistralForSequenceClassification`] (Mistral model)\n",
            " |              - [`MixtralConfig`] configuration class: [`MixtralForSequenceClassification`] (Mixtral model)\n",
            " |              - [`MobileBertConfig`] configuration class: [`MobileBertForSequenceClassification`] (MobileBERT model)\n",
            " |              - [`MptConfig`] configuration class: [`MptForSequenceClassification`] (MPT model)\n",
            " |              - [`MraConfig`] configuration class: [`MraForSequenceClassification`] (MRA model)\n",
            " |              - [`MvpConfig`] configuration class: [`MvpForSequenceClassification`] (MVP model)\n",
            " |              - [`NezhaConfig`] configuration class: [`NezhaForSequenceClassification`] (Nezha model)\n",
            " |              - [`NystromformerConfig`] configuration class: [`NystromformerForSequenceClassification`] (Nyströmformer model)\n",
            " |              - [`OPTConfig`] configuration class: [`OPTForSequenceClassification`] (OPT model)\n",
            " |              - [`OpenAIGPTConfig`] configuration class: [`OpenAIGPTForSequenceClassification`] (OpenAI GPT model)\n",
            " |              - [`OpenLlamaConfig`] configuration class: [`OpenLlamaForSequenceClassification`] (OpenLlama model)\n",
            " |              - [`PLBartConfig`] configuration class: [`PLBartForSequenceClassification`] (PLBart model)\n",
            " |              - [`PerceiverConfig`] configuration class: [`PerceiverForSequenceClassification`] (Perceiver model)\n",
            " |              - [`PersimmonConfig`] configuration class: [`PersimmonForSequenceClassification`] (Persimmon model)\n",
            " |              - [`PhiConfig`] configuration class: [`PhiForSequenceClassification`] (Phi model)\n",
            " |              - [`QDQBertConfig`] configuration class: [`QDQBertForSequenceClassification`] (QDQBert model)\n",
            " |              - [`Qwen2Config`] configuration class: [`Qwen2ForSequenceClassification`] (Qwen2 model)\n",
            " |              - [`ReformerConfig`] configuration class: [`ReformerForSequenceClassification`] (Reformer model)\n",
            " |              - [`RemBertConfig`] configuration class: [`RemBertForSequenceClassification`] (RemBERT model)\n",
            " |              - [`RoCBertConfig`] configuration class: [`RoCBertForSequenceClassification`] (RoCBert model)\n",
            " |              - [`RoFormerConfig`] configuration class: [`RoFormerForSequenceClassification`] (RoFormer model)\n",
            " |              - [`RobertaConfig`] configuration class: [`RobertaForSequenceClassification`] (RoBERTa model)\n",
            " |              - [`RobertaPreLayerNormConfig`] configuration class: [`RobertaPreLayerNormForSequenceClassification`] (RoBERTa-PreLayerNorm model)\n",
            " |              - [`SqueezeBertConfig`] configuration class: [`SqueezeBertForSequenceClassification`] (SqueezeBERT model)\n",
            " |              - [`StableLmConfig`] configuration class: [`StableLmForSequenceClassification`] (StableLm model)\n",
            " |              - [`T5Config`] configuration class: [`T5ForSequenceClassification`] (T5 model)\n",
            " |              - [`TapasConfig`] configuration class: [`TapasForSequenceClassification`] (TAPAS model)\n",
            " |              - [`TransfoXLConfig`] configuration class: [`TransfoXLForSequenceClassification`] (Transformer-XL model)\n",
            " |              - [`UMT5Config`] configuration class: [`UMT5ForSequenceClassification`] (UMT5 model)\n",
            " |              - [`XLMConfig`] configuration class: [`XLMForSequenceClassification`] (XLM model)\n",
            " |              - [`XLMRobertaConfig`] configuration class: [`XLMRobertaForSequenceClassification`] (XLM-RoBERTa model)\n",
            " |              - [`XLMRobertaXLConfig`] configuration class: [`XLMRobertaXLForSequenceClassification`] (XLM-RoBERTa-XL model)\n",
            " |              - [`XLNetConfig`] configuration class: [`XLNetForSequenceClassification`] (XLNet model)\n",
            " |              - [`XmodConfig`] configuration class: [`XmodForSequenceClassification`] (X-MOD model)\n",
            " |              - [`YosoConfig`] configuration class: [`YosoForSequenceClassification`] (YOSO model)\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import AutoConfig, AutoModelForSequenceClassification\n",
            " |      \n",
            " |      >>> # Download configuration from huggingface.co and cache.\n",
            " |      >>> config = AutoConfig.from_pretrained(\"google-bert/bert-base-cased\")\n",
            " |      >>> model = AutoModelForSequenceClassification.from_config(config)\n",
            " |      ```\n",
            " |  \n",
            " |  from_pretrained(*model_args, **kwargs) from builtins.type\n",
            " |      Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.\n",
            " |      \n",
            " |      The model class to instantiate is selected based on the `model_type` property of the config object (either\n",
            " |      passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n",
            " |      falling back to using pattern matching on `pretrained_model_name_or_path`:\n",
            " |      \n",
            " |          - **albert** -- [`AlbertForSequenceClassification`] (ALBERT model)\n",
            " |          - **bart** -- [`BartForSequenceClassification`] (BART model)\n",
            " |          - **bert** -- [`BertForSequenceClassification`] (BERT model)\n",
            " |          - **big_bird** -- [`BigBirdForSequenceClassification`] (BigBird model)\n",
            " |          - **bigbird_pegasus** -- [`BigBirdPegasusForSequenceClassification`] (BigBird-Pegasus model)\n",
            " |          - **biogpt** -- [`BioGptForSequenceClassification`] (BioGpt model)\n",
            " |          - **bloom** -- [`BloomForSequenceClassification`] (BLOOM model)\n",
            " |          - **camembert** -- [`CamembertForSequenceClassification`] (CamemBERT model)\n",
            " |          - **canine** -- [`CanineForSequenceClassification`] (CANINE model)\n",
            " |          - **code_llama** -- [`LlamaForSequenceClassification`] (CodeLlama model)\n",
            " |          - **convbert** -- [`ConvBertForSequenceClassification`] (ConvBERT model)\n",
            " |          - **ctrl** -- [`CTRLForSequenceClassification`] (CTRL model)\n",
            " |          - **data2vec-text** -- [`Data2VecTextForSequenceClassification`] (Data2VecText model)\n",
            " |          - **deberta** -- [`DebertaForSequenceClassification`] (DeBERTa model)\n",
            " |          - **deberta-v2** -- [`DebertaV2ForSequenceClassification`] (DeBERTa-v2 model)\n",
            " |          - **distilbert** -- [`DistilBertForSequenceClassification`] (DistilBERT model)\n",
            " |          - **electra** -- [`ElectraForSequenceClassification`] (ELECTRA model)\n",
            " |          - **ernie** -- [`ErnieForSequenceClassification`] (ERNIE model)\n",
            " |          - **ernie_m** -- [`ErnieMForSequenceClassification`] (ErnieM model)\n",
            " |          - **esm** -- [`EsmForSequenceClassification`] (ESM model)\n",
            " |          - **falcon** -- [`FalconForSequenceClassification`] (Falcon model)\n",
            " |          - **flaubert** -- [`FlaubertForSequenceClassification`] (FlauBERT model)\n",
            " |          - **fnet** -- [`FNetForSequenceClassification`] (FNet model)\n",
            " |          - **funnel** -- [`FunnelForSequenceClassification`] (Funnel Transformer model)\n",
            " |          - **gemma** -- [`GemmaForSequenceClassification`] (Gemma model)\n",
            " |          - **gpt-sw3** -- [`GPT2ForSequenceClassification`] (GPT-Sw3 model)\n",
            " |          - **gpt2** -- [`GPT2ForSequenceClassification`] (OpenAI GPT-2 model)\n",
            " |          - **gpt_bigcode** -- [`GPTBigCodeForSequenceClassification`] (GPTBigCode model)\n",
            " |          - **gpt_neo** -- [`GPTNeoForSequenceClassification`] (GPT Neo model)\n",
            " |          - **gpt_neox** -- [`GPTNeoXForSequenceClassification`] (GPT NeoX model)\n",
            " |          - **gptj** -- [`GPTJForSequenceClassification`] (GPT-J model)\n",
            " |          - **ibert** -- [`IBertForSequenceClassification`] (I-BERT model)\n",
            " |          - **layoutlm** -- [`LayoutLMForSequenceClassification`] (LayoutLM model)\n",
            " |          - **layoutlmv2** -- [`LayoutLMv2ForSequenceClassification`] (LayoutLMv2 model)\n",
            " |          - **layoutlmv3** -- [`LayoutLMv3ForSequenceClassification`] (LayoutLMv3 model)\n",
            " |          - **led** -- [`LEDForSequenceClassification`] (LED model)\n",
            " |          - **lilt** -- [`LiltForSequenceClassification`] (LiLT model)\n",
            " |          - **llama** -- [`LlamaForSequenceClassification`] (LLaMA model)\n",
            " |          - **longformer** -- [`LongformerForSequenceClassification`] (Longformer model)\n",
            " |          - **luke** -- [`LukeForSequenceClassification`] (LUKE model)\n",
            " |          - **markuplm** -- [`MarkupLMForSequenceClassification`] (MarkupLM model)\n",
            " |          - **mbart** -- [`MBartForSequenceClassification`] (mBART model)\n",
            " |          - **mega** -- [`MegaForSequenceClassification`] (MEGA model)\n",
            " |          - **megatron-bert** -- [`MegatronBertForSequenceClassification`] (Megatron-BERT model)\n",
            " |          - **mistral** -- [`MistralForSequenceClassification`] (Mistral model)\n",
            " |          - **mixtral** -- [`MixtralForSequenceClassification`] (Mixtral model)\n",
            " |          - **mobilebert** -- [`MobileBertForSequenceClassification`] (MobileBERT model)\n",
            " |          - **mpnet** -- [`MPNetForSequenceClassification`] (MPNet model)\n",
            " |          - **mpt** -- [`MptForSequenceClassification`] (MPT model)\n",
            " |          - **mra** -- [`MraForSequenceClassification`] (MRA model)\n",
            " |          - **mt5** -- [`MT5ForSequenceClassification`] (MT5 model)\n",
            " |          - **mvp** -- [`MvpForSequenceClassification`] (MVP model)\n",
            " |          - **nezha** -- [`NezhaForSequenceClassification`] (Nezha model)\n",
            " |          - **nystromformer** -- [`NystromformerForSequenceClassification`] (Nyströmformer model)\n",
            " |          - **open-llama** -- [`OpenLlamaForSequenceClassification`] (OpenLlama model)\n",
            " |          - **openai-gpt** -- [`OpenAIGPTForSequenceClassification`] (OpenAI GPT model)\n",
            " |          - **opt** -- [`OPTForSequenceClassification`] (OPT model)\n",
            " |          - **perceiver** -- [`PerceiverForSequenceClassification`] (Perceiver model)\n",
            " |          - **persimmon** -- [`PersimmonForSequenceClassification`] (Persimmon model)\n",
            " |          - **phi** -- [`PhiForSequenceClassification`] (Phi model)\n",
            " |          - **plbart** -- [`PLBartForSequenceClassification`] (PLBart model)\n",
            " |          - **qdqbert** -- [`QDQBertForSequenceClassification`] (QDQBert model)\n",
            " |          - **qwen2** -- [`Qwen2ForSequenceClassification`] (Qwen2 model)\n",
            " |          - **reformer** -- [`ReformerForSequenceClassification`] (Reformer model)\n",
            " |          - **rembert** -- [`RemBertForSequenceClassification`] (RemBERT model)\n",
            " |          - **roberta** -- [`RobertaForSequenceClassification`] (RoBERTa model)\n",
            " |          - **roberta-prelayernorm** -- [`RobertaPreLayerNormForSequenceClassification`] (RoBERTa-PreLayerNorm model)\n",
            " |          - **roc_bert** -- [`RoCBertForSequenceClassification`] (RoCBert model)\n",
            " |          - **roformer** -- [`RoFormerForSequenceClassification`] (RoFormer model)\n",
            " |          - **squeezebert** -- [`SqueezeBertForSequenceClassification`] (SqueezeBERT model)\n",
            " |          - **stablelm** -- [`StableLmForSequenceClassification`] (StableLm model)\n",
            " |          - **t5** -- [`T5ForSequenceClassification`] (T5 model)\n",
            " |          - **tapas** -- [`TapasForSequenceClassification`] (TAPAS model)\n",
            " |          - **transfo-xl** -- [`TransfoXLForSequenceClassification`] (Transformer-XL model)\n",
            " |          - **umt5** -- [`UMT5ForSequenceClassification`] (UMT5 model)\n",
            " |          - **xlm** -- [`XLMForSequenceClassification`] (XLM model)\n",
            " |          - **xlm-roberta** -- [`XLMRobertaForSequenceClassification`] (XLM-RoBERTa model)\n",
            " |          - **xlm-roberta-xl** -- [`XLMRobertaXLForSequenceClassification`] (XLM-RoBERTa-XL model)\n",
            " |          - **xlnet** -- [`XLNetForSequenceClassification`] (XLNet model)\n",
            " |          - **xmod** -- [`XmodForSequenceClassification`] (X-MOD model)\n",
            " |          - **yoso** -- [`YosoForSequenceClassification`] (YOSO model)\n",
            " |      \n",
            " |      The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are\n",
            " |      deactivated). To train the model, you should first set it back in training mode with `model.train()`\n",
            " |      \n",
            " |      Args:\n",
            " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
            " |              Can be either:\n",
            " |      \n",
            " |                  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
            " |                  - A path to a *directory* containing model weights saved using\n",
            " |                    [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
            " |                  - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
            " |                    this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
            " |                    `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
            " |                    PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
            " |          model_args (additional positional arguments, *optional*):\n",
            " |              Will be passed along to the underlying model `__init__()` method.\n",
            " |          config ([`PretrainedConfig`], *optional*):\n",
            " |              Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
            " |              be automatically loaded when:\n",
            " |      \n",
            " |                  - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
            " |                    model).\n",
            " |                  - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
            " |                    save directory.\n",
            " |                  - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
            " |                    configuration JSON file named *config.json* is found in the directory.\n",
            " |          state_dict (*Dict[str, torch.Tensor]*, *optional*):\n",
            " |              A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
            " |      \n",
            " |              This option can be used if you want to create a model from a pretrained configuration but load your own\n",
            " |              weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
            " |              [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
            " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
            " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
            " |              standard cache should not be used.\n",
            " |          from_tf (`bool`, *optional*, defaults to `False`):\n",
            " |              Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
            " |              `pretrained_model_name_or_path` argument).\n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            " |              cached versions if they exist.\n",
            " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
            " |              file exists.\n",
            " |          proxies (`Dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
            " |              Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
            " |          local_files_only(`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to only look at local files (e.g., not try downloading the model).\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
            " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
            " |              identifier allowed by git.\n",
            " |          trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
            " |              should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
            " |              execute code present on the Hub on your local machine.\n",
            " |          code_revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific revision to use for the code on the Hub, if the code leaves in a different repository than\n",
            " |              the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\n",
            " |              system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\n",
            " |              allowed by git.\n",
            " |          kwargs (additional keyword arguments, *optional*):\n",
            " |              Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
            " |              `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
            " |              automatically loaded:\n",
            " |      \n",
            " |                  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
            " |                    underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
            " |                    already been done)\n",
            " |                  - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
            " |                    initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
            " |                    corresponds to a configuration attribute will be used to override said attribute with the\n",
            " |                    supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
            " |                    will be passed to the underlying model's `__init__` function.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import AutoConfig, AutoModelForSequenceClassification\n",
            " |      \n",
            " |      >>> # Download model and configuration from huggingface.co and cache.\n",
            " |      >>> model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n",
            " |      \n",
            " |      >>> # Update configuration during loading\n",
            " |      >>> model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", output_attentions=True)\n",
            " |      >>> model.config.output_attentions\n",
            " |      True\n",
            " |      \n",
            " |      >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
            " |      >>> config = AutoConfig.from_pretrained(\"./tf_model/bert_tf_model_config.json\")\n",
            " |      >>> model = AutoModelForSequenceClassification.from_pretrained(\n",
            " |      ...     \"./tf_model/bert_tf_checkpoint.ckpt.index\", from_tf=True, config=config\n",
            " |      ... )\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.models.auto.auto_factory._BaseAutoModelClass:\n",
            " |  \n",
            " |  __init__(self, *args, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from transformers.models.auto.auto_factory._BaseAutoModelClass:\n",
            " |  \n",
            " |  register(config_class, model_class, exist_ok=False) from builtins.type\n",
            " |      Register a new model for this class.\n",
            " |      \n",
            " |      Args:\n",
            " |          config_class ([`PretrainedConfig`]):\n",
            " |              The configuration corresponding to the model to register.\n",
            " |          model_class ([`PreTrainedModel`]):\n",
            " |              The model to register.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from transformers.models.auto.auto_factory._BaseAutoModelClass:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_sentiment= AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model_sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1Q7W-nJf0Ry",
        "outputId": "55bc09a8-ab17-44cf-815d-89058baaf7b0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(model_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p6GVMhqilBE",
        "outputId": "b3a6829c-b3bb-421e-bd23-88daee101230"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T_destination',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_apply',\n",
              " '_auto_class',\n",
              " '_autoset_attn_implementation',\n",
              " '_backward_compatibility_gradient_checkpointing',\n",
              " '_backward_hooks',\n",
              " '_backward_pre_hooks',\n",
              " '_buffers',\n",
              " '_call_impl',\n",
              " '_check_and_enable_flash_attn_2',\n",
              " '_check_and_enable_sdpa',\n",
              " '_compiled_call_impl',\n",
              " '_convert_head_mask_to_5d',\n",
              " '_copy_lm_head_original_to_resized',\n",
              " '_create_repo',\n",
              " '_dispatch_accelerate_model',\n",
              " '_expand_inputs_for_generation',\n",
              " '_extract_past_from_model_output',\n",
              " '_forward_hooks',\n",
              " '_forward_hooks_always_called',\n",
              " '_forward_hooks_with_kwargs',\n",
              " '_forward_pre_hooks',\n",
              " '_forward_pre_hooks_with_kwargs',\n",
              " '_from_config',\n",
              " '_get_backward_hooks',\n",
              " '_get_backward_pre_hooks',\n",
              " '_get_candidate_generator',\n",
              " '_get_decoder_start_token_id',\n",
              " '_get_files_timestamps',\n",
              " '_get_generation_mode',\n",
              " '_get_logits_processor',\n",
              " '_get_logits_warper',\n",
              " '_get_name',\n",
              " '_get_no_split_modules',\n",
              " '_get_resized_embeddings',\n",
              " '_get_resized_lm_head',\n",
              " '_get_stopping_criteria',\n",
              " '_hf_peft_config_loaded',\n",
              " '_hook_rss_memory_post_forward',\n",
              " '_hook_rss_memory_pre_forward',\n",
              " '_init_weights',\n",
              " '_initialize_weights',\n",
              " '_is_full_backward_hook',\n",
              " '_is_hf_initialized',\n",
              " '_is_quantized_training_enabled',\n",
              " '_keep_in_fp32_modules',\n",
              " '_keep_in_fp32_modules',\n",
              " '_keys_to_ignore_on_load_missing',\n",
              " '_keys_to_ignore_on_load_unexpected',\n",
              " '_keys_to_ignore_on_save',\n",
              " '_load_from_state_dict',\n",
              " '_load_pretrained_model',\n",
              " '_load_pretrained_model_low_mem',\n",
              " '_load_state_dict_post_hooks',\n",
              " '_load_state_dict_pre_hooks',\n",
              " '_maybe_initialize_input_ids_for_generation',\n",
              " '_maybe_warn_non_full_backward_hook',\n",
              " '_merge_criteria_processor_list',\n",
              " '_modules',\n",
              " '_named_members',\n",
              " '_no_split_modules',\n",
              " '_non_persistent_buffers_set',\n",
              " '_parameters',\n",
              " '_prepare_attention_mask_for_generation',\n",
              " '_prepare_decoder_input_ids_for_generation',\n",
              " '_prepare_encoder_decoder_kwargs_for_generation',\n",
              " '_prepare_model_inputs',\n",
              " '_register_load_state_dict_pre_hook',\n",
              " '_register_state_dict_hook',\n",
              " '_reorder_cache',\n",
              " '_replicate_for_data_parallel',\n",
              " '_resize_token_embeddings',\n",
              " '_save_to_state_dict',\n",
              " '_set_default_torch_dtype',\n",
              " '_set_gradient_checkpointing',\n",
              " '_skip_keys_device_placement',\n",
              " '_slow_forward',\n",
              " '_state_dict_hooks',\n",
              " '_state_dict_pre_hooks',\n",
              " '_supports_cache_class',\n",
              " '_supports_flash_attn_2',\n",
              " '_supports_sdpa',\n",
              " '_temporary_reorder_cache',\n",
              " '_tie_encoder_decoder_weights',\n",
              " '_tie_or_clone_weights',\n",
              " '_tied_weights_keys',\n",
              " '_update_model_kwargs_for_generation',\n",
              " '_upload_modified_files',\n",
              " '_validate_generated_length',\n",
              " '_validate_model_class',\n",
              " '_validate_model_kwargs',\n",
              " '_version',\n",
              " '_wrapped_call_impl',\n",
              " 'active_adapter',\n",
              " 'active_adapters',\n",
              " 'add_adapter',\n",
              " 'add_memory_hooks',\n",
              " 'add_model_tags',\n",
              " 'add_module',\n",
              " 'apply',\n",
              " 'assisted_decoding',\n",
              " 'base_model',\n",
              " 'base_model_prefix',\n",
              " 'beam_sample',\n",
              " 'beam_search',\n",
              " 'bfloat16',\n",
              " 'buffers',\n",
              " 'call_super_init',\n",
              " 'can_generate',\n",
              " 'children',\n",
              " 'classifier',\n",
              " 'compile',\n",
              " 'compute_transition_scores',\n",
              " 'config',\n",
              " 'config_class',\n",
              " 'constrained_beam_search',\n",
              " 'contrastive_search',\n",
              " 'cpu',\n",
              " 'create_extended_attention_mask_for_decoder',\n",
              " 'cuda',\n",
              " 'device',\n",
              " 'disable_adapters',\n",
              " 'disable_input_require_grads',\n",
              " 'distilbert',\n",
              " 'double',\n",
              " 'dropout',\n",
              " 'dtype',\n",
              " 'dummy_inputs',\n",
              " 'dump_patches',\n",
              " 'enable_adapters',\n",
              " 'enable_input_require_grads',\n",
              " 'estimate_tokens',\n",
              " 'eval',\n",
              " 'extra_repr',\n",
              " 'float',\n",
              " 'floating_point_ops',\n",
              " 'forward',\n",
              " 'framework',\n",
              " 'from_pretrained',\n",
              " 'generate',\n",
              " 'generation_config',\n",
              " 'get_adapter_state_dict',\n",
              " 'get_buffer',\n",
              " 'get_extended_attention_mask',\n",
              " 'get_extra_state',\n",
              " 'get_head_mask',\n",
              " 'get_input_embeddings',\n",
              " 'get_memory_footprint',\n",
              " 'get_output_embeddings',\n",
              " 'get_parameter',\n",
              " 'get_position_embeddings',\n",
              " 'get_submodule',\n",
              " 'gradient_checkpointing_disable',\n",
              " 'gradient_checkpointing_enable',\n",
              " 'greedy_search',\n",
              " 'group_beam_search',\n",
              " 'half',\n",
              " 'init_weights',\n",
              " 'invert_attention_mask',\n",
              " 'ipu',\n",
              " 'is_gradient_checkpointing',\n",
              " 'is_parallelizable',\n",
              " 'load_adapter',\n",
              " 'load_state_dict',\n",
              " 'load_tf_weights',\n",
              " 'main_input_name',\n",
              " 'model_tags',\n",
              " 'modules',\n",
              " 'name_or_path',\n",
              " 'named_buffers',\n",
              " 'named_children',\n",
              " 'named_modules',\n",
              " 'named_parameters',\n",
              " 'num_labels',\n",
              " 'num_parameters',\n",
              " 'parameters',\n",
              " 'post_init',\n",
              " 'pre_classifier',\n",
              " 'prepare_inputs_for_generation',\n",
              " 'prune_heads',\n",
              " 'push_to_hub',\n",
              " 'register_backward_hook',\n",
              " 'register_buffer',\n",
              " 'register_for_auto_class',\n",
              " 'register_forward_hook',\n",
              " 'register_forward_pre_hook',\n",
              " 'register_full_backward_hook',\n",
              " 'register_full_backward_pre_hook',\n",
              " 'register_load_state_dict_post_hook',\n",
              " 'register_module',\n",
              " 'register_parameter',\n",
              " 'register_state_dict_pre_hook',\n",
              " 'requires_grad_',\n",
              " 'reset_memory_hooks_state',\n",
              " 'resize_position_embeddings',\n",
              " 'resize_token_embeddings',\n",
              " 'retrieve_modules_from_names',\n",
              " 'reverse_bettertransformer',\n",
              " 'sample',\n",
              " 'save_pretrained',\n",
              " 'set_adapter',\n",
              " 'set_extra_state',\n",
              " 'set_input_embeddings',\n",
              " 'share_memory',\n",
              " 'state_dict',\n",
              " 'supports_gradient_checkpointing',\n",
              " 'tie_weights',\n",
              " 'to',\n",
              " 'to_bettertransformer',\n",
              " 'to_empty',\n",
              " 'train',\n",
              " 'training',\n",
              " 'type',\n",
              " 'warn_if_padding_and_no_attention_mask',\n",
              " 'warnings_issued',\n",
              " 'xpu',\n",
              " 'zero_grad']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(model_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOZRIqZ2ixfj",
        "outputId": "a959393e-1b24-449e-b2da-e57b00895bec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on DistilBertForSequenceClassification in module transformers.models.distilbert.modeling_distilbert object:\n",
            "\n",
            "class DistilBertForSequenceClassification(DistilBertPreTrainedModel)\n",
            " |  DistilBertForSequenceClassification(config: transformers.configuration_utils.PretrainedConfig)\n",
            " |  \n",
            " |  DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n",
            " |  pooled output) e.g. for GLUE tasks.\n",
            " |  \n",
            " |  \n",
            " |  This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
            " |  library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
            " |  etc.)\n",
            " |  \n",
            " |  This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
            " |  Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
            " |  and behavior.\n",
            " |  \n",
            " |  Parameters:\n",
            " |      config ([`DistilBertConfig`]): Model configuration class with all the parameters of the model.\n",
            " |          Initializing with a config file does not load the weights associated with the model, only the\n",
            " |          configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DistilBertForSequenceClassification\n",
            " |      DistilBertPreTrainedModel\n",
            " |      transformers.modeling_utils.PreTrainedModel\n",
            " |      torch.nn.modules.module.Module\n",
            " |      transformers.modeling_utils.ModuleUtilsMixin\n",
            " |      transformers.generation.utils.GenerationMixin\n",
            " |      transformers.utils.hub.PushToHubMixin\n",
            " |      transformers.integrations.peft.PeftAdapterMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, config: transformers.configuration_utils.PretrainedConfig)\n",
            " |      Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
            " |  \n",
            " |  forward(self, input_ids: Optional[torch.Tensor] = None, attention_mask: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor] = None, inputs_embeds: Optional[torch.Tensor] = None, labels: Optional[torch.LongTensor] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None) -> Union[transformers.modeling_outputs.SequenceClassifierOutput, Tuple[torch.Tensor, ...]]\n",
            " |      The [`DistilBertForSequenceClassification`] forward method, overrides the `__call__` special method.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
            " |      instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
            " |      the latter silently ignores them.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            " |              Indices of input sequence tokens in the vocabulary.\n",
            " |      \n",
            " |              Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
            " |              [`PreTrainedTokenizer.__call__`] for details.\n",
            " |      \n",
            " |              [What are input IDs?](../glossary#input-ids)\n",
            " |          attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            " |              Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
            " |      \n",
            " |              - 1 for tokens that are **not masked**,\n",
            " |              - 0 for tokens that are **masked**.\n",
            " |      \n",
            " |              [What are attention masks?](../glossary#attention-mask)\n",
            " |          head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
            " |              Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
            " |      \n",
            " |              - 1 indicates the head is **not masked**,\n",
            " |              - 0 indicates the head is **masked**.\n",
            " |      \n",
            " |          inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
            " |              Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
            " |              is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
            " |              model's internal embedding lookup matrix.\n",
            " |          output_attentions (`bool`, *optional*):\n",
            " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
            " |              tensors for more detail.\n",
            " |          output_hidden_states (`bool`, *optional*):\n",
            " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
            " |              more detail.\n",
            " |          return_dict (`bool`, *optional*):\n",
            " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            " |      \n",
            " |          labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
            " |              Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
            " |              config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
            " |              `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
            " |          \n",
            " |      Returns:\n",
            " |          [`transformers.modeling_outputs.SequenceClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.SequenceClassifierOutput`] or a tuple of\n",
            " |          `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
            " |          elements depending on the configuration ([`DistilBertConfig`]) and inputs.\n",
            " |      \n",
            " |          - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
            " |          - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
            " |          - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
            " |            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
            " |      \n",
            " |            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
            " |          - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
            " |            sequence_length)`.\n",
            " |      \n",
            " |            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
            " |            heads.\n",
            " |      \n",
            " |      Example of single-label classification:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> import torch\n",
            " |      >>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
            " |      \n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
            " |      >>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
            " |      \n",
            " |      >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
            " |      \n",
            " |      >>> with torch.no_grad():\n",
            " |      ...     logits = model(**inputs).logits\n",
            " |      \n",
            " |      >>> predicted_class_id = logits.argmax().item()\n",
            " |      \n",
            " |      >>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
            " |      >>> num_labels = len(model.config.id2label)\n",
            " |      >>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
            " |      \n",
            " |      >>> labels = torch.tensor([1])\n",
            " |      >>> loss = model(**inputs, labels=labels).loss\n",
            " |      ```\n",
            " |      \n",
            " |      Example of multi-label classification:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> import torch\n",
            " |      >>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
            " |      \n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
            " |      >>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\")\n",
            " |      \n",
            " |      >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
            " |      \n",
            " |      >>> with torch.no_grad():\n",
            " |      ...     logits = model(**inputs).logits\n",
            " |      \n",
            " |      >>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
            " |      \n",
            " |      >>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
            " |      >>> num_labels = len(model.config.id2label)\n",
            " |      >>> model = DistilBertForSequenceClassification.from_pretrained(\n",
            " |      ...     \"distilbert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> labels = torch.sum(\n",
            " |      ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
            " |      ... ).to(torch.float)\n",
            " |      >>> loss = model(**inputs, labels=labels).loss\n",
            " |      ```\n",
            " |  \n",
            " |  get_position_embeddings(self) -> torch.nn.modules.sparse.Embedding\n",
            " |      Returns the position embeddings\n",
            " |  \n",
            " |  resize_position_embeddings(self, new_num_position_embeddings: int)\n",
            " |      Resizes position embeddings of the model if `new_num_position_embeddings != config.max_position_embeddings`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          new_num_position_embeddings (`int`):\n",
            " |              The number of new position embedding matrix. If position embeddings are learned, increasing the size\n",
            " |              will add newly initialized vectors at the end, whereas reducing the size will remove vectors from the\n",
            " |              end. If position embeddings are not learned (*e.g.* sinusoidal position embeddings), increasing the\n",
            " |              size will add correct vectors at the end following the position encoding algorithm, whereas reducing\n",
            " |              the size will remove vectors from the end.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from DistilBertPreTrainedModel:\n",
            " |  \n",
            " |  base_model_prefix = 'distilbert'\n",
            " |  \n",
            " |  config_class = <class 'transformers.models.distilbert.configuration_di...\n",
            " |      This is the configuration class to store the configuration of a [`DistilBertModel`] or a [`TFDistilBertModel`]. It\n",
            " |      is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.\n",
            " |      Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT\n",
            " |      [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) architecture.\n",
            " |      \n",
            " |      Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
            " |      documentation from [`PretrainedConfig`] for more information.\n",
            " |      \n",
            " |      Args:\n",
            " |          vocab_size (`int`, *optional*, defaults to 30522):\n",
            " |              Vocabulary size of the DistilBERT model. Defines the number of different tokens that can be represented by\n",
            " |              the `inputs_ids` passed when calling [`DistilBertModel`] or [`TFDistilBertModel`].\n",
            " |          max_position_embeddings (`int`, *optional*, defaults to 512):\n",
            " |              The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
            " |              just in case (e.g., 512 or 1024 or 2048).\n",
            " |          sinusoidal_pos_embds (`boolean`, *optional*, defaults to `False`):\n",
            " |              Whether to use sinusoidal positional embeddings.\n",
            " |          n_layers (`int`, *optional*, defaults to 6):\n",
            " |              Number of hidden layers in the Transformer encoder.\n",
            " |          n_heads (`int`, *optional*, defaults to 12):\n",
            " |              Number of attention heads for each attention layer in the Transformer encoder.\n",
            " |          dim (`int`, *optional*, defaults to 768):\n",
            " |              Dimensionality of the encoder layers and the pooler layer.\n",
            " |          hidden_dim (`int`, *optional*, defaults to 3072):\n",
            " |              The size of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
            " |          dropout (`float`, *optional*, defaults to 0.1):\n",
            " |              The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
            " |          attention_dropout (`float`, *optional*, defaults to 0.1):\n",
            " |              The dropout ratio for the attention probabilities.\n",
            " |          activation (`str` or `Callable`, *optional*, defaults to `\"gelu\"`):\n",
            " |              The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n",
            " |              `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n",
            " |          initializer_range (`float`, *optional*, defaults to 0.02):\n",
            " |              The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
            " |          qa_dropout (`float`, *optional*, defaults to 0.1):\n",
            " |              The dropout probabilities used in the question answering model [`DistilBertForQuestionAnswering`].\n",
            " |          seq_classif_dropout (`float`, *optional*, defaults to 0.2):\n",
            " |              The dropout probabilities used in the sequence classification and the multiple choice model\n",
            " |              [`DistilBertForSequenceClassification`].\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import DistilBertConfig, DistilBertModel\n",
            " |      \n",
            " |      >>> # Initializing a DistilBERT configuration\n",
            " |      >>> configuration = DistilBertConfig()\n",
            " |      \n",
            " |      >>> # Initializing a model (with random weights) from the configuration\n",
            " |      >>> model = DistilBertModel(configuration)\n",
            " |      \n",
            " |      >>> # Accessing the model configuration\n",
            " |      >>> configuration = model.config\n",
            " |      ```\n",
            " |  \n",
            " |  \n",
            " |  load_tf_weights = None\n",
            " |  \n",
            " |  supports_gradient_checkpointing = True\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.modeling_utils.PreTrainedModel:\n",
            " |  \n",
            " |  add_model_tags(self, tags: Union[List[str], str]) -> None\n",
            " |      Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\n",
            " |      not overwrite existing tags in the model.\n",
            " |      \n",
            " |      Args:\n",
            " |          tags (`Union[List[str], str]`):\n",
            " |              The desired tags to inject in the model\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      from transformers import AutoModel\n",
            " |      \n",
            " |      model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n",
            " |      \n",
            " |      model.add_model_tags([\"custom\", \"custom-bert\"])\n",
            " |      \n",
            " |      # Push the model to your namespace with the name \"my-custom-bert\".\n",
            " |      model.push_to_hub(\"my-custom-bert\")\n",
            " |      ```\n",
            " |  \n",
            " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the GPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on GPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  disable_input_require_grads(self)\n",
            " |      Removes the `_require_grads_hook`.\n",
            " |  \n",
            " |  enable_input_require_grads(self)\n",
            " |      Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping\n",
            " |      the model weights fixed.\n",
            " |  \n",
            " |  float(self, *args)\n",
            " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  get_input_embeddings(self) -> torch.nn.modules.module.Module\n",
            " |      Returns the model's input embeddings.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `nn.Module`: A torch module mapping vocabulary to hidden states.\n",
            " |  \n",
            " |  get_memory_footprint(self, return_buffers=True)\n",
            " |      Get the memory footprint of a model. This will return the memory footprint of the current model in bytes.\n",
            " |      Useful to benchmark the memory footprint of the current model and design some tests. Solution inspired from the\n",
            " |      PyTorch discussions: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2\n",
            " |      \n",
            " |      Arguments:\n",
            " |          return_buffers (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether to return the size of the buffer tensors in the computation of the memory footprint. Buffers\n",
            " |              are tensors that do not require gradients and not registered as parameters. E.g. mean and std in batch\n",
            " |              norm layers. Please see: https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2\n",
            " |  \n",
            " |  get_output_embeddings(self) -> torch.nn.modules.module.Module\n",
            " |      Returns the model's output embeddings.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `nn.Module`: A torch module mapping hidden states to vocabulary.\n",
            " |  \n",
            " |  gradient_checkpointing_disable(self)\n",
            " |      Deactivates gradient checkpointing for the current model.\n",
            " |      \n",
            " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
            " |      activations\".\n",
            " |  \n",
            " |  gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None)\n",
            " |      Activates gradient checkpointing for the current model.\n",
            " |      \n",
            " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
            " |      activations\".\n",
            " |      \n",
            " |      We pass the `__call__` method of the modules instead of `forward` because `__call__` attaches all the hooks of\n",
            " |      the module. https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2\n",
            " |      \n",
            " |      Args:\n",
            " |          gradient_checkpointing_kwargs (dict, *optional*):\n",
            " |              Additional keyword arguments passed along to the `torch.utils.checkpoint.checkpoint` function.\n",
            " |  \n",
            " |  half(self, *args)\n",
            " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  init_weights(self)\n",
            " |      If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to implement any\n",
            " |      initialization logic in `_init_weights`.\n",
            " |  \n",
            " |  post_init(self)\n",
            " |      A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n",
            " |      modules properly initialized (such as weight initialization).\n",
            " |  \n",
            " |  prune_heads(self, heads_to_prune: Dict[int, List[int]])\n",
            " |      Prunes heads of the base model.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          heads_to_prune (`Dict[int, List[int]]`):\n",
            " |              Dictionary with keys being selected layer indices (`int`) and associated values being the list of heads\n",
            " |              to prune in said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on\n",
            " |              layer 1 and heads 2 and 3 on layer 2.\n",
            " |  \n",
            " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: str = None, commit_description: str = None, tags: Optional[List[str]] = None, **deprecated_kwargs) -> str\n",
            " |      Upload the model file to the 🤗 Model Hub.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          repo_id (`str`):\n",
            " |              The name of the repository you want to push your model to. It should contain your organization name\n",
            " |              when pushing to a given organization.\n",
            " |          use_temp_dir (`bool`, *optional*):\n",
            " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
            " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
            " |          commit_message (`str`, *optional*):\n",
            " |              Message to commit while pushing. Will default to `\"Upload model\"`.\n",
            " |          private (`bool`, *optional*):\n",
            " |              Whether or not the repository created should be private.\n",
            " |          token (`bool` or `str`, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
            " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
            " |              is not specified.\n",
            " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
            " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
            " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
            " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
            " |              Google Colab instances without any CPU OOM issues.\n",
            " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
            " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
            " |          revision (`str`, *optional*):\n",
            " |              Branch to push the uploaded files to.\n",
            " |          commit_description (`str`, *optional*):\n",
            " |              The description of the commit that will be created\n",
            " |          tags (`List[str]`, *optional*):\n",
            " |              List of tags to push on the Hub.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      from transformers import AutoModel\n",
            " |      \n",
            " |      model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n",
            " |      \n",
            " |      # Push the model to your namespace with the name \"my-finetuned-bert\".\n",
            " |      model.push_to_hub(\"my-finetuned-bert\")\n",
            " |      \n",
            " |      # Push the model to an organization with the name \"my-finetuned-bert\".\n",
            " |      model.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
            " |      ```\n",
            " |  \n",
            " |  resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None) -> torch.nn.modules.sparse.Embedding\n",
            " |      Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n",
            " |      \n",
            " |      Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          new_num_tokens (`int`, *optional*):\n",
            " |              The new number of tokens in the embedding matrix. Increasing the size will add newly initialized\n",
            " |              vectors at the end. Reducing the size will remove vectors from the end. If not provided or `None`, just\n",
            " |              returns a pointer to the input tokens `torch.nn.Embedding` module of the model without doing anything.\n",
            " |          pad_to_multiple_of (`int`, *optional*):\n",
            " |              If set will pad the embedding matrix to a multiple of the provided value.If `new_num_tokens` is set to\n",
            " |              `None` will just pad the embedding to a multiple of `pad_to_multiple_of`.\n",
            " |      \n",
            " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
            " |              `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more\n",
            " |              details about this, or help on choosing the correct value for resizing, refer to this guide:\n",
            " |              https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            " |      \n",
            " |      Return:\n",
            " |          `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n",
            " |  \n",
            " |  retrieve_modules_from_names(self, names, add_prefix=False, remove_prefix=False)\n",
            " |  \n",
            " |  reverse_bettertransformer(self)\n",
            " |      Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original modeling is\n",
            " |      used, for example in order to save the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`PreTrainedModel`]: The model converted back to the original modeling.\n",
            " |  \n",
            " |  save_pretrained(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x7c5d58bff6d0>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', safe_serialization: bool = True, variant: Optional[str] = None, token: Union[str, bool, NoneType] = None, save_peft_format: bool = True, **kwargs)\n",
            " |      Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
            " |      [`~PreTrainedModel.from_pretrained`] class method.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          save_directory (`str` or `os.PathLike`):\n",
            " |              Directory to which to save. Will be created if it doesn't exist.\n",
            " |          is_main_process (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether the process calling this is the main process or not. Useful when in distributed training like\n",
            " |              TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n",
            " |              the main process to avoid race conditions.\n",
            " |          state_dict (nested dictionary of `torch.Tensor`):\n",
            " |              The state dictionary of the model to save. Will default to `self.state_dict()`, but can be used to only\n",
            " |              save parts of the model or if special precautions need to be taken when recovering the state dictionary\n",
            " |              of a model (like when using model parallelism).\n",
            " |          save_function (`Callable`):\n",
            " |              The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n",
            " |              need to replace `torch.save` by another method.\n",
            " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
            " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
            " |              namespace).\n",
            " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
            " |              The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size\n",
            " |              lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5MB\"`).\n",
            " |              We default it to 5GB in order for models to be able to run easily on free-tier google colab instances\n",
            " |              without CPU OOM issues.\n",
            " |      \n",
            " |              <Tip warning={true}>\n",
            " |      \n",
            " |              If a single weight of the model is bigger than `max_shard_size`, it will be in its own checkpoint shard\n",
            " |              which will be bigger than `max_shard_size`.\n",
            " |      \n",
            " |              </Tip>\n",
            " |      \n",
            " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n",
            " |          variant (`str`, *optional*):\n",
            " |              If specified, weights are saved in the format pytorch_model.<variant>.bin.\n",
            " |          token (`str` or `bool`, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n",
            " |              the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
            " |          save_peft_format (`bool`, *optional*, defaults to `True`):\n",
            " |              For backward compatibility with PEFT library, in case adapter weights are attached to the model, all\n",
            " |              keys of the state dict of adapters needs to be pre-pended with `base_model.model`. Advanced users can\n",
            " |              disable this behaviours by setting `save_peft_format` to `False`.\n",
            " |          kwargs (`Dict[str, Any]`, *optional*):\n",
            " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
            " |  \n",
            " |  set_input_embeddings(self, value: torch.nn.modules.module.Module)\n",
            " |      Set model's input embeddings.\n",
            " |      \n",
            " |      Args:\n",
            " |          value (`nn.Module`): A module mapping vocabulary to hidden states.\n",
            " |  \n",
            " |  tie_weights(self)\n",
            " |      Tie the weights between the input embeddings and the output embeddings.\n",
            " |      \n",
            " |      If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n",
            " |      weights instead.\n",
            " |  \n",
            " |  to(self, *args, **kwargs)\n",
            " |      Move and/or cast the parameters and buffers.\n",
            " |      \n",
            " |      This can be called as\n",
            " |      \n",
            " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(dtype, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(tensor, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(memory_format=torch.channels_last)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
            " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
            " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
            " |      (if given). The integral parameters and buffers will be moved\n",
            " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
            " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
            " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
            " |      pinned memory to CUDA devices.\n",
            " |      \n",
            " |      See below for examples.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): the desired device of the parameters\n",
            " |              and buffers in this module\n",
            " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
            " |              the parameters and buffers in this module\n",
            " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
            " |              dtype and device for all parameters and buffers in this module\n",
            " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
            " |              format for 4D parameters and buffers in this module (keyword\n",
            " |              only argument)\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
            " |          >>> linear = nn.Linear(2, 2)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]])\n",
            " |          >>> linear.to(torch.double)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
            " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
            " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
            " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
            " |          >>> cpu = torch.device(\"cpu\")\n",
            " |          >>> linear.to(cpu)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
            " |      \n",
            " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
            " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
            " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
            " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
            " |                  [0.6122+0.j, 0.1150+0.j],\n",
            " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
            " |  \n",
            " |  to_bettertransformer(self) -> 'PreTrainedModel'\n",
            " |      Converts the model to use [PyTorch's native attention\n",
            " |      implementation](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html), integrated to\n",
            " |      Transformers through [Optimum library](https://huggingface.co/docs/optimum/bettertransformer/overview). Only a\n",
            " |      subset of all Transformers models are supported.\n",
            " |      \n",
            " |      PyTorch's attention fastpath allows to speed up inference through kernel fusions and the use of [nested\n",
            " |      tensors](https://pytorch.org/docs/stable/nested.html). Detailed benchmarks can be found in [this blog\n",
            " |      post](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2).\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`PreTrainedModel`]: The model converted to BetterTransformer.\n",
            " |  \n",
            " |  warn_if_padding_and_no_attention_mask(self, input_ids, attention_mask)\n",
            " |      Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from transformers.modeling_utils.PreTrainedModel:\n",
            " |  \n",
            " |  can_generate() -> bool from builtins.type\n",
            " |      Returns whether this model can generate sequences with `.generate()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `bool`: Whether this model can generate sequences with `.generate()`.\n",
            " |  \n",
            " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, config: Union[transformers.configuration_utils.PretrainedConfig, str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', use_safetensors: bool = None, **kwargs) from builtins.type\n",
            " |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
            " |      \n",
            " |      The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n",
            " |      the model, you should first set it back in training mode with `model.train()`.\n",
            " |      \n",
            " |      The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
            " |      pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
            " |      task.\n",
            " |      \n",
            " |      The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n",
            " |      weights are discarded.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
            " |              Can be either:\n",
            " |      \n",
            " |                  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
            " |                  - A path to a *directory* containing model weights saved using\n",
            " |                    [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
            " |                  - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
            " |                    this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
            " |                    `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
            " |                    PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
            " |                  - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n",
            " |                    `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n",
            " |                    `True`.\n",
            " |                  - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n",
            " |                    arguments `config` and `state_dict`).\n",
            " |          model_args (sequence of positional arguments, *optional*):\n",
            " |              All remaining positional arguments will be passed to the underlying model's `__init__` method.\n",
            " |          config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\n",
            " |              Can be either:\n",
            " |      \n",
            " |                  - an instance of a class derived from [`PretrainedConfig`],\n",
            " |                  - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\n",
            " |      \n",
            " |              Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
            " |              be automatically loaded when:\n",
            " |      \n",
            " |                  - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
            " |                    model).\n",
            " |                  - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
            " |                    save directory.\n",
            " |                  - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
            " |                    configuration JSON file named *config.json* is found in the directory.\n",
            " |          state_dict (`Dict[str, torch.Tensor]`, *optional*):\n",
            " |              A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
            " |      \n",
            " |              This option can be used if you want to create a model from a pretrained configuration but load your own\n",
            " |              weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
            " |              [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
            " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
            " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
            " |              standard cache should not be used.\n",
            " |          from_tf (`bool`, *optional*, defaults to `False`):\n",
            " |              Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
            " |              `pretrained_model_name_or_path` argument).\n",
            " |          from_flax (`bool`, *optional*, defaults to `False`):\n",
            " |              Load the model weights from a Flax checkpoint save file (see docstring of\n",
            " |              `pretrained_model_name_or_path` argument).\n",
            " |          ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n",
            " |              as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n",
            " |              checkpoint with 3 labels).\n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            " |              cached versions if they exist.\n",
            " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
            " |              file exists.\n",
            " |          proxies (`Dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
            " |              Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
            " |          local_files_only(`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to only look at local files (i.e., do not try to download the model).\n",
            " |          token (`str` or `bool`, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n",
            " |              the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
            " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
            " |              identifier allowed by git.\n",
            " |      \n",
            " |              <Tip>\n",
            " |      \n",
            " |              To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\n",
            " |      \n",
            " |              </Tip>\n",
            " |      \n",
            " |          mirror (`str`, *optional*):\n",
            " |              Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
            " |              problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
            " |              Please refer to the mirror site for more information.\n",
            " |          _fast_init(`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to disable fast initialization.\n",
            " |      \n",
            " |              <Tip warning={true}>\n",
            " |      \n",
            " |              One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ <\n",
            " |              4.6.0` for seeded model initialization. This argument will be removed at the next major version. See\n",
            " |              [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.\n",
            " |      \n",
            " |              </Tip>\n",
            " |      \n",
            " |          > Parameters for big model inference\n",
            " |      \n",
            " |          low_cpu_mem_usage(`bool`, *optional*):\n",
            " |              Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
            " |              This is an experimental feature and a subject to change at any moment.\n",
            " |          torch_dtype (`str` or `torch.dtype`, *optional*):\n",
            " |              Override the default `torch.dtype` and load the model under a specific `dtype`. The different options\n",
            " |              are:\n",
            " |      \n",
            " |              1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified\n",
            " |                `dtype`, ignoring the model's `config.torch_dtype` if one exists. If not specified\n",
            " |                - the model will get loaded in `torch.float` (fp32).\n",
            " |      \n",
            " |              2. `\"auto\"` - A `torch_dtype` entry in the `config.json` file of the model will be\n",
            " |                attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in\n",
            " |                the checkpoint that's of a floating point type and use that as `dtype`. This will load the model\n",
            " |                using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how\n",
            " |                the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.\n",
            " |      \n",
            " |              <Tip>\n",
            " |      \n",
            " |              For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or\n",
            " |              reach out to the authors and ask them to add this information to the model's card and to insert the\n",
            " |              `torch_dtype` entry in `config.json` on the hub.\n",
            " |      \n",
            " |              </Tip>\n",
            " |      \n",
            " |          device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n",
            " |              A map that specifies where each submodule should go. It doesn't need to be refined to each\n",
            " |              parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n",
            " |              same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n",
            " |              like `1`) on which the model will be allocated, the device map will map the entire model to this\n",
            " |              device. Passing `device_map = 0` means put the whole model on GPU 0.\n",
            " |      \n",
            " |              To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n",
            " |              more information about each option see [designing a device\n",
            " |              map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
            " |          max_memory (`Dict`, *optional*):\n",
            " |              A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\n",
            " |              GPU and the available CPU RAM if unset.\n",
            " |          offload_folder (`str` or `os.PathLike`, *optional*):\n",
            " |              If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n",
            " |          offload_state_dict (`bool`, *optional*):\n",
            " |              If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU\n",
            " |              RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to\n",
            " |              `True` when there is some disk offload.\n",
            " |          quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):\n",
            " |              A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g\n",
            " |              bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and\n",
            " |              `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes\n",
            " |              quantizations and not preferred. consider inserting all such arguments into quantization_config\n",
            " |              instead.\n",
            " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
            " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n",
            " |              specify the folder name here.\n",
            " |          variant (`str`, *optional*):\n",
            " |              If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is\n",
            " |              ignored when using `from_tf` or `from_flax`.\n",
            " |          use_safetensors (`bool`, *optional*, defaults to `None`):\n",
            " |              Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`\n",
            " |              is not installed, it will be set to `False`.\n",
            " |      \n",
            " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
            " |              Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
            " |              `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
            " |              automatically loaded:\n",
            " |      \n",
            " |                  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
            " |                    underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
            " |                    already been done)\n",
            " |                  - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
            " |                    initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
            " |                    corresponds to a configuration attribute will be used to override said attribute with the\n",
            " |                    supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
            " |                    will be passed to the underlying model's `__init__` function.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n",
            " |      use this method in a firewalled environment.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import BertConfig, BertModel\n",
            " |      \n",
            " |      >>> # Download model and configuration from huggingface.co and cache.\n",
            " |      >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
            " |      >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n",
            " |      >>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n",
            " |      >>> # Update configuration during loading.\n",
            " |      >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True)\n",
            " |      >>> assert model.config.output_attentions == True\n",
            " |      >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n",
            " |      >>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\n",
            " |      >>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\n",
            " |      >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n",
            " |      >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", from_flax=True)\n",
            " |      ```\n",
            " |      \n",
            " |      * `low_cpu_mem_usage` algorithm:\n",
            " |      \n",
            " |      This is an experimental function that loads the model using ~1x model size CPU memory\n",
            " |      \n",
            " |      Here is how it works:\n",
            " |      \n",
            " |      1. save which state_dict keys we have\n",
            " |      2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory\n",
            " |      3. after the model has been instantiated switch to the meta device all params/buffers that\n",
            " |      are going to be replaced from the loaded state_dict\n",
            " |      4. load state_dict 2nd time\n",
            " |      5. replace the params/buffers from the state_dict\n",
            " |      \n",
            " |      Currently, it can't handle deepspeed ZeRO stage 3 and ignores loading errors\n",
            " |  \n",
            " |  register_for_auto_class(auto_class='AutoModel') from builtins.type\n",
            " |      Register this class with a given auto class. This should only be used for custom models as the ones in the\n",
            " |      library are already mapped with an auto class.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoModel\"`):\n",
            " |              The auto class to register this new model with.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from transformers.modeling_utils.PreTrainedModel:\n",
            " |  \n",
            " |  base_model\n",
            " |      `torch.nn.Module`: The main body of the model.\n",
            " |  \n",
            " |  dummy_inputs\n",
            " |      `Dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\n",
            " |  \n",
            " |  framework\n",
            " |      :str: Identifies that this is a PyTorch model.\n",
            " |  \n",
            " |  is_gradient_checkpointing\n",
            " |      Whether gradient checkpointing is activated for this model or not.\n",
            " |      \n",
            " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
            " |      activations\".\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from transformers.modeling_utils.PreTrainedModel:\n",
            " |  \n",
            " |  is_parallelizable = False\n",
            " |  \n",
            " |  main_input_name = 'input_ids'\n",
            " |  \n",
            " |  model_tags = None\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __dir__(self)\n",
            " |      Default dir() implementation.\n",
            " |  \n",
            " |  __getattr__(self, name: str) -> Any\n",
            " |      # On the return type:\n",
            " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
            " |      # This is done for better interop with various type checkers for the end users.\n",
            " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
            " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
            " |      # See full discussion on the problems with returning `Union` here\n",
            " |      # https://github.com/microsoft/pyright/issues/4213\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
            " |      Add a child module to the current module.\n",
            " |      \n",
            " |      The module can be accessed as an attribute using the given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (str): name of the child module. The child module can be\n",
            " |              accessed from this module using the given name\n",
            " |          module (Module): child module to be added to the module.\n",
            " |  \n",
            " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
            " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
            " |      \n",
            " |      Typical use includes initializing the parameters of a model\n",
            " |      (see also :ref:`nn-init-doc`).\n",
            " |      \n",
            " |      Args:\n",
            " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> @torch.no_grad()\n",
            " |          >>> def init_weights(m):\n",
            " |          >>>     print(m)\n",
            " |          >>>     if type(m) == nn.Linear:\n",
            " |          >>>         m.weight.fill_(1.0)\n",
            " |          >>>         print(m.weight)\n",
            " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
            " |          >>> net.apply(init_weights)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[1., 1.],\n",
            " |                  [1., 1.]], requires_grad=True)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[1., 1.],\n",
            " |                  [1., 1.]], requires_grad=True)\n",
            " |          Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |  \n",
            " |  bfloat16(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
            " |      Return an iterator over module buffers.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          torch.Tensor: module buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for buf in model.buffers():\n",
            " |          >>>     print(type(buf), buf.size())\n",
            " |          <class 'torch.Tensor'> (20L,)\n",
            " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  children(self) -> Iterator[ForwardRef('Module')]\n",
            " |      Return an iterator over immediate children modules.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a child module\n",
            " |  \n",
            " |  compile(self, *args, **kwargs)\n",
            " |      Compile this Module's forward using :func:`torch.compile`.\n",
            " |      \n",
            " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
            " |      to :func:`torch.compile`.\n",
            " |      \n",
            " |      See :func:`torch.compile` for details on the arguments for this function.\n",
            " |  \n",
            " |  cpu(self: ~T) -> ~T\n",
            " |      Move all model parameters and buffers to the CPU.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  double(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  eval(self: ~T) -> ~T\n",
            " |      Set the module in evaluation mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |      \n",
            " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
            " |      \n",
            " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
            " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  extra_repr(self) -> str\n",
            " |      Set the extra representation of the module.\n",
            " |      \n",
            " |      To print customized extra information, you should re-implement\n",
            " |      this method in your own modules. Both single-line and multi-line\n",
            " |      strings are acceptable.\n",
            " |  \n",
            " |  get_buffer(self, target: str) -> 'Tensor'\n",
            " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      See the docstring for ``get_submodule`` for a more detailed\n",
            " |      explanation of this method's functionality as well as how to\n",
            " |      correctly specify ``target``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the buffer\n",
            " |              to look for. (See ``get_submodule`` for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.Tensor: The buffer referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not a\n",
            " |              buffer\n",
            " |  \n",
            " |  get_extra_state(self) -> Any\n",
            " |      Return any extra state to include in the module's state_dict.\n",
            " |      \n",
            " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
            " |      if you need to store extra state. This function is called when building the\n",
            " |      module's `state_dict()`.\n",
            " |      \n",
            " |      Note that extra state should be picklable to ensure working serialization\n",
            " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
            " |      for serializing Tensors; other objects may break backwards compatibility if\n",
            " |      their serialized pickled form changes.\n",
            " |      \n",
            " |      Returns:\n",
            " |          object: Any extra state to store in the module's state_dict\n",
            " |  \n",
            " |  get_parameter(self, target: str) -> 'Parameter'\n",
            " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      See the docstring for ``get_submodule`` for a more detailed\n",
            " |      explanation of this method's functionality as well as how to\n",
            " |      correctly specify ``target``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the Parameter\n",
            " |              to look for. (See ``get_submodule`` for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Parameter``\n",
            " |  \n",
            " |  get_submodule(self, target: str) -> 'Module'\n",
            " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
            " |      looks like this:\n",
            " |      \n",
            " |      .. code-block:: text\n",
            " |      \n",
            " |          A(\n",
            " |              (net_b): Module(\n",
            " |                  (net_c): Module(\n",
            " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
            " |                  )\n",
            " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
            " |              )\n",
            " |          )\n",
            " |      \n",
            " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
            " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
            " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
            " |      \n",
            " |      To check whether or not we have the ``linear`` submodule, we\n",
            " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
            " |      we have the ``conv`` submodule, we would call\n",
            " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
            " |      \n",
            " |      The runtime of ``get_submodule`` is bounded by the degree\n",
            " |      of module nesting in ``target``. A query against\n",
            " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
            " |      the number of transitive modules. So, for a simple check to see\n",
            " |      if some submodule exists, ``get_submodule`` should always be\n",
            " |      used.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the submodule\n",
            " |              to look for. (See above example for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.nn.Module: The submodule referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Module``\n",
            " |  \n",
            " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the IPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on IPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
            " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
            " |      \n",
            " |      If :attr:`strict` is ``True``, then\n",
            " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
            " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
            " |          the call to :attr:`load_state_dict`.\n",
            " |      \n",
            " |      Args:\n",
            " |          state_dict (dict): a dict containing parameters and\n",
            " |              persistent buffers.\n",
            " |          strict (bool, optional): whether to strictly enforce that the keys\n",
            " |              in :attr:`state_dict` match the keys returned by this module's\n",
            " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
            " |          assign (bool, optional): whether to assign items in the state\n",
            " |              dictionary to their corresponding keys in the module instead\n",
            " |              of copying them inplace into the module's current parameters and buffers.\n",
            " |              When ``False``, the properties of the tensors in the current\n",
            " |              module are preserved while when ``True``, the properties of the\n",
            " |              Tensors in the state dict are preserved.\n",
            " |              Default: ``False``\n",
            " |      \n",
            " |      Returns:\n",
            " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
            " |              * **missing_keys** is a list of str containing the missing keys\n",
            " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
            " |      \n",
            " |      Note:\n",
            " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
            " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
            " |          ``RuntimeError``.\n",
            " |  \n",
            " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
            " |      Return an iterator over all modules in the network.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a module in the network\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.modules()):\n",
            " |          ...     print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
            " |  \n",
            " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
            " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all buffer names.\n",
            " |          recurse (bool, optional): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module. Defaults to True.\n",
            " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for name, buf in self.named_buffers():\n",
            " |          >>>     if name in ['running_var']:\n",
            " |          >>>         print(buf.size())\n",
            " |  \n",
            " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
            " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, Module): Tuple containing a name and child module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for name, module in model.named_children():\n",
            " |          >>>     if name in ['conv4', 'conv5']:\n",
            " |          >>>         print(module)\n",
            " |  \n",
            " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
            " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          memo: a memo to store the set of modules already added to the result\n",
            " |          prefix: a prefix that will be added to the name of the module\n",
            " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
            " |              or not\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, Module): Tuple of name and module\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.named_modules()):\n",
            " |          ...     print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> ('', Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          ))\n",
            " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
            " |  \n",
            " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
            " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all parameter names.\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
            " |              parameters in the result. Defaults to True.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, Parameter): Tuple containing the name and parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for name, param in self.named_parameters():\n",
            " |          >>>     if name in ['bias']:\n",
            " |          >>>         print(param.size())\n",
            " |  \n",
            " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
            " |      Return an iterator over module parameters.\n",
            " |      \n",
            " |      This is typically passed to an optimizer.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Parameter: module parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for param in model.parameters():\n",
            " |          >>>     print(type(param), param.size())\n",
            " |          <class 'torch.Tensor'> (20L,)\n",
            " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a backward hook on the module.\n",
            " |      \n",
            " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
            " |      the behavior of this function will change in future versions.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
            " |      Add a buffer to the module.\n",
            " |      \n",
            " |      This is typically used to register a buffer that should not to be\n",
            " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
            " |      is not a parameter, but is part of the module's state. Buffers, by\n",
            " |      default, are persistent and will be saved alongside parameters. This\n",
            " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
            " |      only difference between a persistent buffer and a non-persistent buffer\n",
            " |      is that the latter will not be a part of this module's\n",
            " |      :attr:`state_dict`.\n",
            " |      \n",
            " |      Buffers can be accessed as attributes using given names.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (str): name of the buffer. The buffer can be accessed\n",
            " |              from this module using the given name\n",
            " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
            " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
            " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
            " |          persistent (bool): whether the buffer is part of this module's\n",
            " |              :attr:`state_dict`.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
            " |  \n",
            " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a forward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time after :func:`forward` has computed an output.\n",
            " |      \n",
            " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
            " |      the positional arguments given to the module. Keyword arguments won't be\n",
            " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
            " |      output. It can modify the input inplace but it will not have effect on\n",
            " |      forward since this is called after :func:`forward` is called. The hook\n",
            " |      should have the following signature::\n",
            " |      \n",
            " |          hook(module, args, output) -> None or modified output\n",
            " |      \n",
            " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
            " |      ``kwargs`` given to the forward function and be expected to return the\n",
            " |      output possibly modified. The hook should have the following signature::\n",
            " |      \n",
            " |          hook(module, args, kwargs, output) -> None or modified output\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user defined hook to be registered.\n",
            " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
            " |              before all existing ``forward`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
            " |              this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``forward`` hooks registered with\n",
            " |              :func:`register_module_forward_hook` will fire before all hooks\n",
            " |              registered by this method.\n",
            " |              Default: ``False``\n",
            " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
            " |              kwargs given to the forward function.\n",
            " |              Default: ``False``\n",
            " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
            " |              whether an exception is raised while calling the Module.\n",
            " |              Default: ``False``\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a forward pre-hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time before :func:`forward` is invoked.\n",
            " |      \n",
            " |      \n",
            " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
            " |      the positional arguments given to the module. Keyword arguments won't be\n",
            " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
            " |      input. User can either return a tuple or a single modified value in the\n",
            " |      hook. We will wrap the value into a tuple if a single value is returned\n",
            " |      (unless that value is already a tuple). The hook should have the\n",
            " |      following signature::\n",
            " |      \n",
            " |          hook(module, args) -> None or modified input\n",
            " |      \n",
            " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
            " |      kwargs given to the forward function. And if the hook modifies the\n",
            " |      input, both the args and kwargs should be returned. The hook should have\n",
            " |      the following signature::\n",
            " |      \n",
            " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user defined hook to be registered.\n",
            " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
            " |              all existing ``forward_pre`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
            " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``forward_pre`` hooks registered with\n",
            " |              :func:`register_module_forward_pre_hook` will fire before all\n",
            " |              hooks registered by this method.\n",
            " |              Default: ``False``\n",
            " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
            " |              given to the forward function.\n",
            " |              Default: ``False``\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a backward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time the gradients with respect to a module\n",
            " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
            " |      respect to module outputs are computed. The hook should have the following\n",
            " |      signature::\n",
            " |      \n",
            " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
            " |      \n",
            " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
            " |      with respect to the inputs and outputs respectively. The hook should\n",
            " |      not modify its arguments, but it can optionally return a new gradient with\n",
            " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
            " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
            " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
            " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
            " |      arguments.\n",
            " |      \n",
            " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
            " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
            " |      of each Tensor returned by the Module's forward function.\n",
            " |      \n",
            " |      .. warning ::\n",
            " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
            " |          will raise an error.\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user-defined hook to be registered.\n",
            " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
            " |              all existing ``backward`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
            " |              this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``backward`` hooks registered with\n",
            " |              :func:`register_module_full_backward_hook` will fire before\n",
            " |              all hooks registered by this method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a backward pre-hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time the gradients for the module are computed.\n",
            " |      The hook should have the following signature::\n",
            " |      \n",
            " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
            " |      \n",
            " |      The :attr:`grad_output` is a tuple. The hook should\n",
            " |      not modify its arguments, but it can optionally return a new gradient with\n",
            " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
            " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
            " |      all non-Tensor arguments.\n",
            " |      \n",
            " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
            " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
            " |      of each Tensor returned by the Module's forward function.\n",
            " |      \n",
            " |      .. warning ::\n",
            " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
            " |          will raise an error.\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user-defined hook to be registered.\n",
            " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
            " |              all existing ``backward_pre`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
            " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``backward_pre`` hooks registered with\n",
            " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
            " |              all hooks registered by this method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_load_state_dict_post_hook(self, hook)\n",
            " |      Register a post hook to be run after module's ``load_state_dict`` is called.\n",
            " |      \n",
            " |      It should have the following signature::\n",
            " |          hook(module, incompatible_keys) -> None\n",
            " |      \n",
            " |      The ``module`` argument is the current module that this hook is registered\n",
            " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
            " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
            " |      is a ``list`` of ``str`` containing the missing keys and\n",
            " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
            " |      \n",
            " |      The given incompatible_keys can be modified inplace if needed.\n",
            " |      \n",
            " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
            " |      ``strict=True`` are affected by modifications the hook makes to\n",
            " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
            " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
            " |      clearing out both missing and unexpected keys will avoid an error.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
            " |      Alias for :func:`add_module`.\n",
            " |  \n",
            " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
            " |      Add a parameter to the module.\n",
            " |      \n",
            " |      The parameter can be accessed as an attribute using given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (str): name of the parameter. The parameter can be accessed\n",
            " |              from this module using the given name\n",
            " |          param (Parameter or None): parameter to be added to the module. If\n",
            " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
            " |              are ignored. If ``None``, the parameter is **not** included in the\n",
            " |              module's :attr:`state_dict`.\n",
            " |  \n",
            " |  register_state_dict_pre_hook(self, hook)\n",
            " |      Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.\n",
            " |      \n",
            " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
            " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
            " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
            " |      call is made.\n",
            " |  \n",
            " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
            " |      Change if autograd should record operations on parameters in this module.\n",
            " |      \n",
            " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
            " |      in-place.\n",
            " |      \n",
            " |      This method is helpful for freezing part of the module for finetuning\n",
            " |      or training parts of a model individually (e.g., GAN training).\n",
            " |      \n",
            " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
            " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
            " |      \n",
            " |      Args:\n",
            " |          requires_grad (bool): whether autograd should record operations on\n",
            " |                                parameters in this module. Default: ``True``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  set_extra_state(self, state: Any)\n",
            " |      Set extra state contained in the loaded `state_dict`.\n",
            " |      \n",
            " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
            " |      found within the `state_dict`. Implement this function and a corresponding\n",
            " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
            " |      `state_dict`.\n",
            " |      \n",
            " |      Args:\n",
            " |          state (dict): Extra state from the `state_dict`\n",
            " |  \n",
            " |  share_memory(self: ~T) -> ~T\n",
            " |      See :meth:`torch.Tensor.share_memory_`.\n",
            " |  \n",
            " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
            " |      Return a dictionary containing references to the whole state of the module.\n",
            " |      \n",
            " |      Both parameters and persistent buffers (e.g. running averages) are\n",
            " |      included. Keys are corresponding parameter and buffer names.\n",
            " |      Parameters and buffers set to ``None`` are not included.\n",
            " |      \n",
            " |      .. note::\n",
            " |          The returned object is a shallow copy. It contains references\n",
            " |          to the module's parameters and buffers.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          Currently ``state_dict()`` also accepts positional arguments for\n",
            " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
            " |          this is being deprecated and keyword arguments will be enforced in\n",
            " |          future releases.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          Please avoid the use of argument ``destination`` as it is not\n",
            " |          designed for end-users.\n",
            " |      \n",
            " |      Args:\n",
            " |          destination (dict, optional): If provided, the state of module will\n",
            " |              be updated into the dict and the same object is returned.\n",
            " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
            " |              Default: ``None``.\n",
            " |          prefix (str, optional): a prefix added to parameter and buffer\n",
            " |              names to compose the keys in state_dict. Default: ``''``.\n",
            " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
            " |              returned in the state dict are detached from autograd. If it's\n",
            " |              set to ``True``, detaching will not be performed.\n",
            " |              Default: ``False``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          dict:\n",
            " |              a dictionary containing a whole state of the module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> module.state_dict().keys()\n",
            " |          ['bias', 'weight']\n",
            " |  \n",
            " |  to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
            " |      Move the parameters and buffers to the specified device without copying storage.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): The desired device of the parameters\n",
            " |              and buffers in this module.\n",
            " |          recurse (bool): Whether parameters and buffers of submodules should\n",
            " |              be recursively moved to the specified device.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  train(self: ~T, mode: bool = True) -> ~T\n",
            " |      Set the module in training mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |      \n",
            " |      Args:\n",
            " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
            " |                       mode (``False``). Default: ``True``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
            " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          dst_type (type or string): the desired type\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the XPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on XPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  zero_grad(self, set_to_none: bool = True) -> None\n",
            " |      Reset gradients of all model parameters.\n",
            " |      \n",
            " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
            " |      \n",
            " |      Args:\n",
            " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
            " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  T_destination = ~T_destination\n",
            " |  \n",
            " |  call_super_init = False\n",
            " |  \n",
            " |  dump_patches = False\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
            " |  \n",
            " |  add_memory_hooks(self)\n",
            " |      Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.\n",
            " |      \n",
            " |      Increase in memory consumption is stored in a `mem_rss_diff` attribute for each module and can be reset to zero\n",
            " |      with `model.reset_memory_hooks_state()`.\n",
            " |  \n",
            " |  estimate_tokens(self, input_dict: Dict[str, Union[torch.Tensor, Any]]) -> int\n",
            " |      Helper function to estimate the total number of tokens from the model inputs.\n",
            " |      \n",
            " |      Args:\n",
            " |          inputs (`dict`): The model inputs.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: The total number of tokens.\n",
            " |  \n",
            " |  floating_point_ops(self, input_dict: Dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True) -> int\n",
            " |      Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n",
            " |      batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\n",
            " |      tokens (valid if `12 * d_model << sequence_length`) as laid out in [this\n",
            " |      paper](https://arxiv.org/pdf/2001.08361.pdf) section 2.1. Should be overridden for transformers with parameter\n",
            " |      re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.\n",
            " |      \n",
            " |      Args:\n",
            " |          batch_size (`int`):\n",
            " |              The batch size for the forward pass.\n",
            " |      \n",
            " |          sequence_length (`int`):\n",
            " |              The number of tokens in each line of the batch.\n",
            " |      \n",
            " |          exclude_embeddings (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to count embedding and softmax operations.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: The number of floating-point operations.\n",
            " |  \n",
            " |  get_extended_attention_mask(self, attention_mask: torch.Tensor, input_shape: Tuple[int], device: torch.device = None, dtype: torch.float32 = None) -> torch.Tensor\n",
            " |      Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          attention_mask (`torch.Tensor`):\n",
            " |              Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
            " |          input_shape (`Tuple[int]`):\n",
            " |              The shape of the input to the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n",
            " |  \n",
            " |  get_head_mask(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor\n",
            " |      Prepare the head mask if needed.\n",
            " |      \n",
            " |      Args:\n",
            " |          head_mask (`torch.Tensor` with shape `[num_heads]` or `[num_hidden_layers x num_heads]`, *optional*):\n",
            " |              The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).\n",
            " |          num_hidden_layers (`int`):\n",
            " |              The number of hidden layers in the model.\n",
            " |          is_attention_chunked (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not the attentions scores are computed by chunks or not.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `torch.Tensor` with shape `[num_hidden_layers x batch x num_heads x seq_length x seq_length]` or list with\n",
            " |          `[None]` for each layer.\n",
            " |  \n",
            " |  invert_attention_mask(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor\n",
            " |      Invert an attention mask (e.g., switches 0. and 1.).\n",
            " |      \n",
            " |      Args:\n",
            " |          encoder_attention_mask (`torch.Tensor`): An attention mask.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `torch.Tensor`: The inverted attention mask.\n",
            " |  \n",
            " |  num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int\n",
            " |      Get number of (optionally, trainable or non-embeddings) parameters in the module.\n",
            " |      \n",
            " |      Args:\n",
            " |          only_trainable (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return only the number of trainable parameters\n",
            " |      \n",
            " |          exclude_embeddings (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return only the number of non-embeddings parameters\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: The number of parameters.\n",
            " |  \n",
            " |  reset_memory_hooks_state(self)\n",
            " |      Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory_hooks`]).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
            " |  \n",
            " |  create_extended_attention_mask_for_decoder(input_shape, attention_mask, device=None)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
            " |  \n",
            " |  device\n",
            " |      `torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n",
            " |      device).\n",
            " |  \n",
            " |  dtype\n",
            " |      `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.generation.utils.GenerationMixin:\n",
            " |  \n",
            " |  assisted_decoding(self, input_ids: torch.LongTensor, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None, candidate_generator: Optional[ForwardRef('CandidateGenerator')] = None, do_sample: bool = False, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, logits_warper: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, pad_token_id: Optional[int] = None, eos_token_id: Union[int, List[int], NoneType] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, output_logits: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: bool = False, streamer: Optional[ForwardRef('BaseStreamer')] = None, **model_kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, torch.LongTensor]\n",
            " |      Generates sequences of token ids for models with a language modeling head using **greedy decoding** or\n",
            " |      **sample** (depending on `do_sample`), assisted by candidate sequences. Assisted generation is an example of a\n",
            " |      candidate decoding strategy. Can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text\n",
            " |      models.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      In most cases, you do not need to call [`~generation.GenerationMixin.candidate_decoding`] directly. Use\n",
            " |      generate() instead. For an overview of generation strategies and code examples, check the [following\n",
            " |      guide](../generation_strategies).\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Parameters:\n",
            " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            " |              The sequence used as a prompt for the generation.\n",
            " |          candidate_generator (`CandidateGenerator`, *optional*):\n",
            " |              A derived instance of [`CandidateGenerator`] that defines how candidate sequences are generated. For\n",
            " |              more information, the documentation of [`CandidateGenerator`] should be read. Only one of `assistant_model` or `candidate_generator` should be passed as input to this function.\n",
            " |          assistant_model (`PreTrainedModel`, *optional*):\n",
            " |              An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
            " |              same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
            " |              is much faster than running generation with the model you're calling generate from. As such, the\n",
            " |              assistant model should be much smaller.\n",
            " |          do_sample (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to use sampling ; use greedy decoding otherwise.\n",
            " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
            " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
            " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
            " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
            " |              sampling at each generation step.\n",
            " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
            " |              used to tell if the generation loop should stop.\n",
            " |          pad_token_id (`int`, *optional*):\n",
            " |              The id of the *padding* token.\n",
            " |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
            " |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
            " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            " |              returned tensors for more details.\n",
            " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            " |              for more details.\n",
            " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            " |          output_logits (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the raw prediction logit scores. See `logits` under returned tensors for\n",
            " |              more details.\n",
            " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            " |          streamer (`BaseStreamer`, *optional*):\n",
            " |              Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
            " |              through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
            " |          model_kwargs:\n",
            " |              Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n",
            " |              If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
            " |      \n",
            " |      Return:\n",
            " |          [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`] or\n",
            " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
            " |          [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
            " |          `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n",
            " |          `model.config.is_encoder_decoder=True`.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import (\n",
            " |      ...     AutoTokenizer,\n",
            " |      ...     AutoModelForCausalLM,\n",
            " |      ...     LogitsProcessorList,\n",
            " |      ...     MinLengthLogitsProcessor,\n",
            " |      ...     StoppingCriteriaList,\n",
            " |      ...     MaxLengthCriteria,\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
            " |      >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
            " |      >>> assistant_model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
            " |      >>> # set pad_token_id to eos_token_id because GPT2 does not have a PAD token\n",
            " |      >>> model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
            " |      >>> input_prompt = \"It might be possible to\"\n",
            " |      >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
            " |      >>> # instantiate logits processors\n",
            " |      >>> logits_processor = LogitsProcessorList(\n",
            " |      ...     [\n",
            " |      ...         MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),\n",
            " |      ...     ]\n",
            " |      ... )\n",
            " |      >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
            " |      >>> outputs = model.assisted_decoding(\n",
            " |      ...     input_ids,\n",
            " |      ...     assistant_model=assistant_model,\n",
            " |      ...     logits_processor=logits_processor,\n",
            " |      ...     stopping_criteria=stopping_criteria,\n",
            " |      ... )\n",
            " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            " |      [\"It might be possible to get a better understanding of the nature of the problem, but it's not\"]\n",
            " |      ```\n",
            " |  \n",
            " |  beam_sample(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation.beam_search.BeamScorer, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, logits_warper: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Union[int, List[int], NoneType] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, output_logits: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: bool = False, **model_kwargs) -> Union[transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor]\n",
            " |      Generates sequences of token ids for models with a language modeling head using **beam search multinomial\n",
            " |      sampling** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      In most cases, you do not need to call [`~generation.GenerationMixin.beam_sample`] directly. Use generate()\n",
            " |      instead. For an overview of generation strategies and code examples, check the [following\n",
            " |      guide](../generation_strategies).\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Parameters:\n",
            " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            " |              The sequence used as a prompt for the generation.\n",
            " |          beam_scorer (`BeamScorer`):\n",
            " |              A derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
            " |              sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
            " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
            " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
            " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
            " |              used to tell if the generation loop should stop.\n",
            " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
            " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
            " |              sampling at each generation step.\n",
            " |          max_length (`int`, *optional*, defaults to 20):\n",
            " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
            " |              tokens. The maximum length of the sequence to be generated.\n",
            " |          pad_token_id (`int`, *optional*):\n",
            " |              The id of the *padding* token.\n",
            " |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
            " |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
            " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            " |              returned tensors for more details.\n",
            " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            " |              for more details.\n",
            " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            " |          output_logits (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the raw prediction logit scores. See `logits` under returned tensors for\n",
            " |              more details.\n",
            " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            " |          model_kwargs:\n",
            " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
            " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
            " |      \n",
            " |      Return:\n",
            " |          [`~generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] or\n",
            " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
            " |          [`~generation.GenerateBeamDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
            " |          `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if\n",
            " |          `model.config.is_encoder_decoder=True`.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import (\n",
            " |      ...     AutoTokenizer,\n",
            " |      ...     AutoModelForSeq2SeqLM,\n",
            " |      ...     LogitsProcessorList,\n",
            " |      ...     MinLengthLogitsProcessor,\n",
            " |      ...     TopKLogitsWarper,\n",
            " |      ...     TemperatureLogitsWarper,\n",
            " |      ...     BeamSearchScorer,\n",
            " |      ... )\n",
            " |      >>> import torch\n",
            " |      \n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
            " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
            " |      \n",
            " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
            " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
            " |      \n",
            " |      >>> # lets run beam search using 3 beams\n",
            " |      >>> num_beams = 3\n",
            " |      >>> # define decoder start token ids\n",
            " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
            " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
            " |      \n",
            " |      >>> # add encoder_outputs to model keyword arguments\n",
            " |      >>> model_kwargs = {\n",
            " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
            " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
            " |      ...     )\n",
            " |      ... }\n",
            " |      \n",
            " |      >>> # instantiate beam scorer\n",
            " |      >>> beam_scorer = BeamSearchScorer(\n",
            " |      ...     batch_size=1,\n",
            " |      ...     max_length=model.config.max_length,\n",
            " |      ...     num_beams=num_beams,\n",
            " |      ...     device=model.device,\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> # instantiate logits processors\n",
            " |      >>> logits_processor = LogitsProcessorList(\n",
            " |      ...     [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]\n",
            " |      ... )\n",
            " |      >>> # instantiate logits processors\n",
            " |      >>> logits_warper = LogitsProcessorList(\n",
            " |      ...     [\n",
            " |      ...         TopKLogitsWarper(50),\n",
            " |      ...         TemperatureLogitsWarper(0.7),\n",
            " |      ...     ]\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> outputs = model.beam_sample(\n",
            " |      ...     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            " |      ['Wie alt bist du?']\n",
            " |      ```\n",
            " |  \n",
            " |  beam_search(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation.beam_search.BeamScorer, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Union[int, List[int], NoneType] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, output_logits: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: bool = False, sequential: Optional[bool] = None, **model_kwargs) -> Union[transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor]\n",
            " |      Generates sequences of token ids for models with a language modeling head using **beam search decoding** and\n",
            " |      can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      In most cases, you do not need to call [`~generation.GenerationMixin.beam_search`] directly. Use generate()\n",
            " |      instead. For an overview of generation strategies and code examples, check the [following\n",
            " |      guide](../generation_strategies).\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Parameters:\n",
            " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            " |              The sequence used as a prompt for the generation.\n",
            " |          beam_scorer (`BeamScorer`):\n",
            " |              An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
            " |              sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
            " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
            " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
            " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
            " |              used to tell if the generation loop should stop.\n",
            " |          max_length (`int`, *optional*, defaults to 20):\n",
            " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
            " |              tokens. The maximum length of the sequence to be generated.\n",
            " |          pad_token_id (`int`, *optional*):\n",
            " |              The id of the *padding* token.\n",
            " |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
            " |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
            " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            " |              returned tensors for more details.\n",
            " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            " |              for more details.\n",
            " |          output_logits (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the raw prediction logit scores. See `logits` under returned tensors for\n",
            " |              more details.\n",
            " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            " |          sequential (`bool`, defaults to `False`):\n",
            " |              By default, beam search has `batch_size * num_beams` as effective batch size (see `beam_search()` for\n",
            " |              more details). This flag will avoid parallelizing the beam search and will instead run beam search\n",
            " |              sequentially.\n",
            " |          model_kwargs:\n",
            " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
            " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
            " |      \n",
            " |      Return:\n",
            " |          [`generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] or\n",
            " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
            " |          [`~generation.GenerateBeamDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
            " |          `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if\n",
            " |          `model.config.is_encoder_decoder=True`.\n",
            " |      \n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import (\n",
            " |      ...     AutoTokenizer,\n",
            " |      ...     AutoModelForSeq2SeqLM,\n",
            " |      ...     LogitsProcessorList,\n",
            " |      ...     MinLengthLogitsProcessor,\n",
            " |      ...     BeamSearchScorer,\n",
            " |      ... )\n",
            " |      >>> import torch\n",
            " |      \n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
            " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
            " |      \n",
            " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
            " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
            " |      \n",
            " |      \n",
            " |      >>> # lets run beam search using 3 beams\n",
            " |      >>> num_beams = 3\n",
            " |      >>> # define decoder start token ids\n",
            " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
            " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
            " |      \n",
            " |      >>> # add encoder_outputs to model keyword arguments\n",
            " |      >>> model_kwargs = {\n",
            " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
            " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
            " |      ...     )\n",
            " |      ... }\n",
            " |      \n",
            " |      >>> # instantiate beam scorer\n",
            " |      >>> beam_scorer = BeamSearchScorer(\n",
            " |      ...     batch_size=1,\n",
            " |      ...     num_beams=num_beams,\n",
            " |      ...     device=model.device,\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> # instantiate logits processors\n",
            " |      >>> logits_processor = LogitsProcessorList(\n",
            " |      ...     [\n",
            " |      ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
            " |      ...     ]\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
            " |      \n",
            " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            " |      ['Wie alt bist du?']\n",
            " |      ```\n",
            " |  \n",
            " |  compute_transition_scores(self, sequences: torch.Tensor, scores: Tuple[torch.Tensor], beam_indices: Optional[torch.Tensor] = None, normalize_logits: bool = False) -> torch.Tensor\n",
            " |      Computes the transition scores of sequences given the generation scores (and beam indices, if beam search was\n",
            " |      used). This is a convenient method to quicky obtain the scores of the selected tokens at generation time.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          sequences (`torch.LongTensor`):\n",
            " |              The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or\n",
            " |              shorter if all batches finished early due to the `eos_token_id`.\n",
            " |          scores (`tuple(torch.FloatTensor)`):\n",
            " |              Transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n",
            " |              of log probabilities of tokens conditioned on log softmax of previously generated tokens Tuple of\n",
            " |              `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token), with\n",
            " |              each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n",
            " |          beam_indices (`torch.LongTensor`, *optional*):\n",
            " |              Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n",
            " |              `(batch_size*num_return_sequences, sequence_length)`. Only required if a `num_beams>1` at\n",
            " |              generate-time.\n",
            " |          normalize_logits (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to normalize the logits (which, for legacy reasons, may be unnormalized).\n",
            " |      \n",
            " |      Return:\n",
            " |          `torch.Tensor`: A `torch.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)` containing\n",
            " |              the transition scores (logits)\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
            " |      >>> import numpy as np\n",
            " |      \n",
            " |      >>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
            " |      >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
            " |      >>> tokenizer.pad_token_id = tokenizer.eos_token_id\n",
            " |      >>> inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
            " |      \n",
            " |      >>> # Example 1: Print the scores for each token generated with Greedy Search\n",
            " |      >>> outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n",
            " |      >>> transition_scores = model.compute_transition_scores(\n",
            " |      ...     outputs.sequences, outputs.scores, normalize_logits=True\n",
            " |      ... )\n",
            " |      >>> # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
            " |      >>> # encoder-decoder models, like BART or T5.\n",
            " |      >>> input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
            " |      >>> generated_tokens = outputs.sequences[:, input_length:]\n",
            " |      >>> for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
            " |      ...     # | token | token string | log probability | probability\n",
            " |      ...     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n",
            " |      |   262 |  the     | -1.414 | 24.33%\n",
            " |      |  1110 |  day     | -2.609 | 7.36%\n",
            " |      |   618 |  when    | -2.010 | 13.40%\n",
            " |      |   356 |  we      | -1.859 | 15.58%\n",
            " |      |   460 |  can     | -2.508 | 8.14%\n",
            " |      \n",
            " |      >>> # Example 2: Reconstruct the sequence scores from Beam Search\n",
            " |      >>> outputs = model.generate(\n",
            " |      ...     **inputs,\n",
            " |      ...     max_new_tokens=5,\n",
            " |      ...     num_beams=4,\n",
            " |      ...     num_return_sequences=4,\n",
            " |      ...     return_dict_in_generate=True,\n",
            " |      ...     output_scores=True,\n",
            " |      ... )\n",
            " |      >>> transition_scores = model.compute_transition_scores(\n",
            " |      ...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
            " |      ... )\n",
            " |      >>> # If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
            " |      >>> # Tip 1: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n",
            " |      >>> # use case, you might want to recompute it with `normalize_logits=True`.\n",
            " |      >>> # Tip 2: the output length does NOT include the input length\n",
            " |      >>> output_length = np.sum(transition_scores.numpy() < 0, axis=1)\n",
            " |      >>> length_penalty = model.generation_config.length_penalty\n",
            " |      >>> reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
            " |      >>> print(np.allclose(outputs.sequences_scores, reconstructed_scores))\n",
            " |      True\n",
            " |      ```\n",
            " |  \n",
            " |  constrained_beam_search(self, input_ids: torch.LongTensor, constrained_beam_scorer: transformers.generation.beam_search.ConstrainedBeamSearchScorer, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Union[int, List[int], NoneType] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, output_logits: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = None, **model_kwargs) -> Union[transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor]\n",
            " |      Generates sequences of token ids for models with a language modeling head using **constrained beam search\n",
            " |      decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      In most cases, you do not need to call [`~generation.GenerationMixin.constrained_beam_search`] directly. Use\n",
            " |      generate() instead. For an overview of generation strategies and code examples, check the [following\n",
            " |      guide](../generation_strategies).\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Parameters:\n",
            " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            " |              The sequence used as a prompt for the generation.\n",
            " |          constrained_beam_scorer (`ConstrainedBeamSearchScorer`):\n",
            " |              A derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
            " |              sorted during generation, while satisfying a list of positive constraints. For more information, the\n",
            " |              documentation of [`ConstrainedBeamSearchScorer`] should be read.\n",
            " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
            " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
            " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
            " |              used to tell if the generation loop should stop.\n",
            " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
            " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
            " |              sampling at each generation step.\n",
            " |          max_length (`int`, *optional*, defaults to 20):\n",
            " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
            " |              tokens. The maximum length of the sequence to be generated.\n",
            " |          pad_token_id (`int`, *optional*):\n",
            " |              The id of the *padding* token.\n",
            " |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
            " |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
            " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            " |              returned tensors for more details.\n",
            " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            " |              for more details.\n",
            " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            " |          output_logits (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the raw prediction logit scores. See `logits` under returned tensors for\n",
            " |              more details.\n",
            " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            " |          model_kwargs:\n",
            " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
            " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
            " |      \n",
            " |      Return:\n",
            " |          [`~generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] or\n",
            " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
            " |          [`~generation.GenerateBeamDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
            " |          `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if\n",
            " |          `model.config.is_encoder_decoder=True`.\n",
            " |      \n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import (\n",
            " |      ...     AutoTokenizer,\n",
            " |      ...     AutoModelForSeq2SeqLM,\n",
            " |      ...     LogitsProcessorList,\n",
            " |      ...     MinLengthLogitsProcessor,\n",
            " |      ...     ConstrainedBeamSearchScorer,\n",
            " |      ...     PhrasalConstraint,\n",
            " |      ... )\n",
            " |      >>> import torch\n",
            " |      \n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
            " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
            " |      \n",
            " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
            " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
            " |      \n",
            " |      \n",
            " |      >>> # lets run beam search using 3 beams\n",
            " |      >>> num_beams = 3\n",
            " |      >>> # define decoder start token ids\n",
            " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
            " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
            " |      \n",
            " |      >>> # add encoder_outputs to model keyword arguments\n",
            " |      >>> model_kwargs = {\n",
            " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
            " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
            " |      ...     )\n",
            " |      ... }\n",
            " |      \n",
            " |      >>> constraint_str = \"Sie\"\n",
            " |      >>> constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token\n",
            " |      >>> constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]\n",
            " |      \n",
            " |      \n",
            " |      >>> # instantiate beam scorer\n",
            " |      >>> beam_scorer = ConstrainedBeamSearchScorer(\n",
            " |      ...     batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> # instantiate logits processors\n",
            " |      >>> logits_processor = LogitsProcessorList(\n",
            " |      ...     [\n",
            " |      ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
            " |      ...     ]\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> outputs = model.constrained_beam_search(\n",
            " |      ...     input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            " |      ['Wie alt sind Sie?']\n",
            " |      ```\n",
            " |  \n",
            " |  contrastive_search(self, input_ids: torch.LongTensor, top_k: Optional[int] = 1, penalty_alpha: Optional[float] = 0, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, logits_warper: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, pad_token_id: Optional[int] = None, eos_token_id: Union[int, List[int], NoneType] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, output_logits: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: bool = False, streamer: Optional[ForwardRef('BaseStreamer')] = None, sequential: Optional[bool] = None, **model_kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, torch.LongTensor]\n",
            " |      Generates sequences of token ids for models with a language modeling head using **contrastive search** and can\n",
            " |      be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      In most cases, you do not need to call [`~generation.GenerationMixin.contrastive_search`] directly. Use\n",
            " |      generate() instead. For an overview of generation strategies and code examples, check the [following\n",
            " |      guide](../generation_strategies).\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Parameters:\n",
            " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            " |              The sequence used as a prompt for the generation.\n",
            " |          top_k (`int`, *optional*, defaults to 1):\n",
            " |              The size of the candidate set that is used to re-rank for contrastive search\n",
            " |          penalty_alpha (`float`, *optional*, defaults to 0):\n",
            " |              The degeneration penalty for contrastive search; activate when it is larger than 0\n",
            " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
            " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
            " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
            " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
            " |              sampling at each generation step.\n",
            " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
            " |              used to tell if the generation loop should stop.\n",
            " |          pad_token_id (`int`, *optional*):\n",
            " |              The id of the *padding* token.\n",
            " |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
            " |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
            " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            " |              returned tensors for more details.\n",
            " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            " |              for more details.\n",
            " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            " |          output_logits (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the raw prediction logit scores. See `logits` under returned tensors\n",
            " |              for more details.\n",
            " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            " |          streamer (`BaseStreamer`, *optional*):\n",
            " |              Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
            " |              through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
            " |          sequential (`bool`, *optional*):\n",
            " |              Switches topk hidden state computation from parallel to sequential to reduce memory if True.\n",
            " |          model_kwargs:\n",
            " |              Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n",
            " |              If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
            " |      \n",
            " |      Return:\n",
            " |          [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`]\n",
            " |          or `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
            " |          [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
            " |          `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n",
            " |          `model.config.is_encoder_decoder=True`.\n",
            " |      \n",
            " |      Examples:\n",
            " |      ```python\n",
            " |      >>> from transformers import (\n",
            " |      ...     AutoTokenizer,\n",
            " |      ...     AutoModelForCausalLM,\n",
            " |      ...     StoppingCriteriaList,\n",
            " |      ...     MaxLengthCriteria,\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
            " |      >>> model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
            " |      >>> # set pad_token_id to eos_token_id because OPT does not have a PAD token\n",
            " |      >>> model.config.pad_token_id = model.config.eos_token_id\n",
            " |      >>> input_prompt = \"DeepMind Company is\"\n",
            " |      >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\")\n",
            " |      >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=64)])\n",
            " |      >>> outputs = model.contrastive_search(\n",
            " |      ...     **input_ids, penalty_alpha=0.6, top_k=4, stopping_criteria=stopping_criteria\n",
            " |      ... )\n",
            " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            " |      ['DeepMind Company is a company that focuses on the development and commercialization of artificial intelligence (AI). DeepMind’s mission is to help people understand and solve problems that are difficult to solve in the world today.\\n\\nIn this post, we talk about the benefits of deep learning in business and how it']\n",
            " |      ```\n",
            " |  \n",
            " |  generate(self, inputs: Optional[torch.Tensor] = None, generation_config: Optional[transformers.generation.configuration_utils.GenerationConfig] = None, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None, streamer: Optional[ForwardRef('BaseStreamer')] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask: Optional[torch.Tensor] = None, **kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor]\n",
            " |      Generates sequences of token ids for models with a language modeling head.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
            " |      model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
            " |      parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
            " |      \n",
            " |      For an overview of generation strategies and code examples, check out the [following\n",
            " |      guide](../generation_strategies).\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Parameters:\n",
            " |          inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
            " |              The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
            " |              method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
            " |              should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
            " |              `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
            " |          generation_config (`~generation.GenerationConfig`, *optional*):\n",
            " |              The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
            " |              passed to generate matching the attributes of `generation_config` will override them. If\n",
            " |              `generation_config` is not provided, the default will be used, which had the following loading\n",
            " |              priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
            " |              configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
            " |              default values, whose documentation should be checked to parameterize generation.\n",
            " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
            " |              Custom logits processors that complement the default logits processors built from arguments and\n",
            " |              generation config. If a logit processor is passed that is already created with the arguments or a\n",
            " |              generation config an error is thrown. This feature is intended for advanced users.\n",
            " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            " |              Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
            " |              generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
            " |              generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
            " |              sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
            " |              intended for advanced users.\n",
            " |          prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
            " |              If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
            " |              provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
            " |              `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
            " |              on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
            " |              for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
            " |              Retrieval](https://arxiv.org/abs/2010.00904).\n",
            " |          synced_gpus (`bool`, *optional*):\n",
            " |              Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
            " |              `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
            " |              generating before other GPUs. Otherwise it'll be set to `False`.\n",
            " |          assistant_model (`PreTrainedModel`, *optional*):\n",
            " |              An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
            " |              same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
            " |              is much faster than running generation with the model you're calling generate from. As such, the\n",
            " |              assistant model should be much smaller.\n",
            " |          streamer (`BaseStreamer`, *optional*):\n",
            " |              Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
            " |              through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
            " |          negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            " |              The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
            " |              size. This is an experimental feature, subject to breaking API changes in future versions.\n",
            " |          negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            " |              Attention_mask for `negative_prompt_ids`.\n",
            " |          kwargs (`Dict[str, Any]`, *optional*):\n",
            " |              Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n",
            " |              forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
            " |              specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
            " |      \n",
            " |      Return:\n",
            " |          [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
            " |          or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
            " |      \n",
            " |              If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
            " |              [`~utils.ModelOutput`] types are:\n",
            " |      \n",
            " |                  - [`~generation.GenerateDecoderOnlyOutput`],\n",
            " |                  - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
            " |      \n",
            " |              If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
            " |              [`~utils.ModelOutput`] types are:\n",
            " |      \n",
            " |                  - [`~generation.GenerateEncoderDecoderOutput`],\n",
            " |                  - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
            " |  \n",
            " |  greedy_search(self, input_ids: torch.LongTensor, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Union[int, List[int], NoneType] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, output_logits: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: bool = False, streamer: Optional[ForwardRef('BaseStreamer')] = None, **model_kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, torch.LongTensor]\n",
            " |      Generates sequences of token ids for models with a language modeling head using **greedy decoding** and can be\n",
            " |      used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      In most cases, you do not need to call [`~generation.GenerationMixin.greedy_search`] directly. Use generate()\n",
            " |      instead. For an overview of generation strategies and code examples, check the [following\n",
            " |      guide](../generation_strategies).\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      \n",
            " |      Parameters:\n",
            " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            " |              The sequence used as a prompt for the generation.\n",
            " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
            " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
            " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
            " |              used to tell if the generation loop should stop.\n",
            " |      \n",
            " |          max_length (`int`, *optional*, defaults to 20):\n",
            " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
            " |              tokens. The maximum length of the sequence to be generated.\n",
            " |          pad_token_id (`int`, *optional*):\n",
            " |              The id of the *padding* token.\n",
            " |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
            " |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
            " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            " |              returned tensors for more details.\n",
            " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            " |              for more details.\n",
            " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            " |          output_logits (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the raw prediction logit scores. See `logits` under returned tensors\n",
            " |              for more details.\n",
            " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            " |          streamer (`BaseStreamer`, *optional*):\n",
            " |              Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
            " |              through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
            " |          model_kwargs:\n",
            " |              Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n",
            " |              If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
            " |      \n",
            " |      Return:\n",
            " |          [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`] or\n",
            " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
            " |          [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
            " |          `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n",
            " |          `model.config.is_encoder_decoder=True`.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import (\n",
            " |      ...     AutoTokenizer,\n",
            " |      ...     AutoModelForCausalLM,\n",
            " |      ...     LogitsProcessorList,\n",
            " |      ...     MinLengthLogitsProcessor,\n",
            " |      ...     StoppingCriteriaList,\n",
            " |      ...     MaxLengthCriteria,\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
            " |      >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
            " |      \n",
            " |      >>> # set pad_token_id to eos_token_id because GPT2 does not have a PAD token\n",
            " |      >>> model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
            " |      \n",
            " |      >>> input_prompt = \"It might be possible to\"\n",
            " |      >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
            " |      \n",
            " |      >>> # instantiate logits processors\n",
            " |      >>> logits_processor = LogitsProcessorList(\n",
            " |      ...     [\n",
            " |      ...         MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),\n",
            " |      ...     ]\n",
            " |      ... )\n",
            " |      >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
            " |      \n",
            " |      >>> outputs = model.greedy_search(\n",
            " |      ...     input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            " |      [\"It might be possible to get a better understanding of the nature of the problem, but it's not\"]\n",
            " |      ```\n",
            " |  \n",
            " |  group_beam_search(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation.beam_search.BeamScorer, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Union[int, List[int], NoneType] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, output_logits: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: bool = False, **model_kwargs)\n",
            " |      Generates sequences of token ids for models with a language modeling head using **diverse beam search\n",
            " |      decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      In most cases, you do not need to call [`~generation.GenerationMixin.group_beam_search`] directly. Use\n",
            " |      generate() instead. For an overview of generation strategies and code examples, check the [following\n",
            " |      guide](../generation_strategies).\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Parameters:\n",
            " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            " |              The sequence used as a prompt for the generation.\n",
            " |          beam_scorer (`BeamScorer`):\n",
            " |              An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
            " |              sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
            " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
            " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
            " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
            " |              used to tell if the generation loop should stop.\n",
            " |          max_length (`int`, *optional*, defaults to 20):\n",
            " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
            " |              tokens. The maximum length of the sequence to be generated.\n",
            " |          pad_token_id (`int`, *optional*):\n",
            " |              The id of the *padding* token.\n",
            " |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
            " |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
            " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            " |              returned tensors for more details.\n",
            " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            " |              for more details.\n",
            " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            " |          output_logits (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the raw prediction logit scores. See `logits` under returned tensors for\n",
            " |              more details.\n",
            " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            " |      \n",
            " |          model_kwargs:\n",
            " |              Additional model specific kwargs that will be forwarded to the `forward` function of the model. If\n",
            " |              model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
            " |      \n",
            " |      Return:\n",
            " |          [`~generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] or\n",
            " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
            " |          [`~generation.GenerateBeamDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
            " |          `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if\n",
            " |          `model.config.is_encoder_decoder=True`.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import (\n",
            " |      ...     AutoTokenizer,\n",
            " |      ...     AutoModelForSeq2SeqLM,\n",
            " |      ...     LogitsProcessorList,\n",
            " |      ...     MinLengthLogitsProcessor,\n",
            " |      ...     HammingDiversityLogitsProcessor,\n",
            " |      ...     BeamSearchScorer,\n",
            " |      ... )\n",
            " |      >>> import torch\n",
            " |      \n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
            " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
            " |      \n",
            " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
            " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
            " |      \n",
            " |      \n",
            " |      >>> # lets run diverse beam search using 6 beams\n",
            " |      >>> num_beams = 6\n",
            " |      >>> # define decoder start token ids\n",
            " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
            " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
            " |      \n",
            " |      >>> # add encoder_outputs to model keyword arguments\n",
            " |      >>> model_kwargs = {\n",
            " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
            " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
            " |      ...     )\n",
            " |      ... }\n",
            " |      \n",
            " |      >>> # instantiate beam scorer\n",
            " |      >>> beam_scorer = BeamSearchScorer(\n",
            " |      ...     batch_size=1,\n",
            " |      ...     max_length=model.config.max_length,\n",
            " |      ...     num_beams=num_beams,\n",
            " |      ...     device=model.device,\n",
            " |      ...     num_beam_groups=3,\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> # instantiate logits processors\n",
            " |      >>> logits_processor = LogitsProcessorList(\n",
            " |      ...     [\n",
            " |      ...         HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),\n",
            " |      ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
            " |      ...     ]\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> outputs = model.group_beam_search(\n",
            " |      ...     input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            " |      ['Wie alt bist du?']\n",
            " |      ```\n",
            " |  \n",
            " |  prepare_inputs_for_generation(self, *args, **kwargs)\n",
            " |  \n",
            " |  sample(self, input_ids: torch.LongTensor, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, logits_warper: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Union[int, List[int], NoneType] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, output_logits: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: bool = False, streamer: Optional[ForwardRef('BaseStreamer')] = None, **model_kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, torch.LongTensor]\n",
            " |      Generates sequences of token ids for models with a language modeling head using **multinomial sampling** and\n",
            " |      can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      In most cases, you do not need to call [`~generation.GenerationMixin.sample`] directly. Use generate() instead.\n",
            " |      For an overview of generation strategies and code examples, check the [following\n",
            " |      guide](../generation_strategies).\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Parameters:\n",
            " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
            " |              The sequence used as a prompt for the generation.\n",
            " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
            " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
            " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
            " |              used to tell if the generation loop should stop.\n",
            " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
            " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
            " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
            " |              sampling at each generation step.\n",
            " |          max_length (`int`, *optional*, defaults to 20):\n",
            " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
            " |              tokens. The maximum length of the sequence to be generated.\n",
            " |          pad_token_id (`int`, *optional*):\n",
            " |              The id of the *padding* token.\n",
            " |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
            " |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
            " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            " |              returned tensors for more details.\n",
            " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            " |              for more details.\n",
            " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            " |          output_logits (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return the raw prediction logit scores. See `logits` under returned tensors for\n",
            " |              more details.\n",
            " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            " |          streamer (`BaseStreamer`, *optional*):\n",
            " |              Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
            " |              through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
            " |          model_kwargs:\n",
            " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
            " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
            " |      \n",
            " |      Return:\n",
            " |          [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`] or `torch.LongTensor`:\n",
            " |          A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
            " |          [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
            " |          `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n",
            " |          `model.config.is_encoder_decoder=True`.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from transformers import (\n",
            " |      ...     AutoTokenizer,\n",
            " |      ...     AutoModelForCausalLM,\n",
            " |      ...     LogitsProcessorList,\n",
            " |      ...     MinLengthLogitsProcessor,\n",
            " |      ...     TopKLogitsWarper,\n",
            " |      ...     TemperatureLogitsWarper,\n",
            " |      ...     StoppingCriteriaList,\n",
            " |      ...     MaxLengthCriteria,\n",
            " |      ... )\n",
            " |      >>> import torch\n",
            " |      \n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
            " |      >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
            " |      \n",
            " |      >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
            " |      >>> model.config.pad_token_id = model.config.eos_token_id\n",
            " |      >>> model.generation_config.pad_token_id = model.config.eos_token_id\n",
            " |      \n",
            " |      >>> input_prompt = \"Today is a beautiful day, and\"\n",
            " |      >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
            " |      \n",
            " |      >>> # instantiate logits processors\n",
            " |      >>> logits_processor = LogitsProcessorList(\n",
            " |      ...     [\n",
            " |      ...         MinLengthLogitsProcessor(15, eos_token_id=model.generation_config.eos_token_id),\n",
            " |      ...     ]\n",
            " |      ... )\n",
            " |      >>> # instantiate logits processors\n",
            " |      >>> logits_warper = LogitsProcessorList(\n",
            " |      ...     [\n",
            " |      ...         TopKLogitsWarper(50),\n",
            " |      ...         TemperatureLogitsWarper(0.7),\n",
            " |      ...     ]\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
            " |      \n",
            " |      >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
            " |      >>> outputs = model.sample(\n",
            " |      ...     input_ids,\n",
            " |      ...     logits_processor=logits_processor,\n",
            " |      ...     logits_warper=logits_warper,\n",
            " |      ...     stopping_criteria=stopping_criteria,\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            " |      ['Today is a beautiful day, and we must do everything possible to make it a day of celebration.']\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from transformers.integrations.peft.PeftAdapterMixin:\n",
            " |  \n",
            " |  active_adapter(self) -> str\n",
            " |  \n",
            " |  active_adapters(self) -> List[str]\n",
            " |      If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n",
            " |      official documentation: https://huggingface.co/docs/peft\n",
            " |      \n",
            " |      Gets the current active adapters of the model. In case of multi-adapter inference (combining multiple adapters\n",
            " |      for inference) returns the list of all active adapters so that users can deal with them accordingly.\n",
            " |      \n",
            " |      For previous PEFT versions (that does not support multi-adapter inference), `module.active_adapter` will return\n",
            " |      a single string.\n",
            " |  \n",
            " |  add_adapter(self, adapter_config, adapter_name: Optional[str] = None) -> None\n",
            " |      If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n",
            " |      official documentation: https://huggingface.co/docs/peft\n",
            " |      \n",
            " |      Adds a fresh new adapter to the current model for training purpose. If no adapter name is passed, a default\n",
            " |      name is assigned to the adapter to follow the convention of PEFT library (in PEFT we use \"default\" as the\n",
            " |      default adapter name).\n",
            " |      \n",
            " |      Args:\n",
            " |          adapter_config (`~peft.PeftConfig`):\n",
            " |              The configuration of the adapter to add, supported adapters are non-prefix tuning and adaption prompts\n",
            " |              methods\n",
            " |          adapter_name (`str`, *optional*, defaults to `\"default\"`):\n",
            " |              The name of the adapter to add. If no name is passed, a default name is assigned to the adapter.\n",
            " |  \n",
            " |  disable_adapters(self) -> None\n",
            " |      If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n",
            " |      official documentation: https://huggingface.co/docs/peft\n",
            " |      \n",
            " |      Disable all adapters that are attached to the model. This leads to inferring with the base model only.\n",
            " |  \n",
            " |  enable_adapters(self) -> None\n",
            " |      If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n",
            " |      official documentation: https://huggingface.co/docs/peft\n",
            " |      \n",
            " |      Enable adapters that are attached to the model. The model will use `self.active_adapter()`\n",
            " |  \n",
            " |  get_adapter_state_dict(self, adapter_name: Optional[str] = None) -> dict\n",
            " |      If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n",
            " |      official documentation: https://huggingface.co/docs/peft\n",
            " |      \n",
            " |      Gets the adapter state dict that should only contain the weights tensors of the specified adapter_name adapter.\n",
            " |      If no adapter_name is passed, the active adapter is used.\n",
            " |      \n",
            " |      Args:\n",
            " |          adapter_name (`str`, *optional*):\n",
            " |              The name of the adapter to get the state dict from. If no name is passed, the active adapter is used.\n",
            " |  \n",
            " |  load_adapter(self, peft_model_id: Optional[str] = None, adapter_name: Optional[str] = None, revision: Optional[str] = None, token: Optional[str] = None, device_map: Optional[str] = 'auto', max_memory: Optional[str] = None, offload_folder: Optional[str] = None, offload_index: Optional[int] = None, peft_config: Dict[str, Any] = None, adapter_state_dict: Optional[Dict[str, ForwardRef('torch.Tensor')]] = None, adapter_kwargs: Optional[Dict[str, Any]] = None) -> None\n",
            " |      Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT methods, we\n",
            " |      invite you to read more about them on PEFT official documentation: https://huggingface.co/docs/peft\n",
            " |      \n",
            " |      Requires peft as a backend to load the adapter weights.\n",
            " |      \n",
            " |      Args:\n",
            " |          peft_model_id (`str`, *optional*):\n",
            " |              The identifier of the model to look for on the Hub, or a local path to the saved adapter config file\n",
            " |              and adapter weights.\n",
            " |          adapter_name (`str`, *optional*):\n",
            " |              The adapter name to use. If not set, will use the default adapter.\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
            " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
            " |              identifier allowed by git.\n",
            " |      \n",
            " |              <Tip>\n",
            " |      \n",
            " |              To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\n",
            " |      \n",
            " |              </Tip>\n",
            " |      \n",
            " |          token (`str`, `optional`):\n",
            " |              Whether to use authentication token to load the remote folder. Userful to load private repositories\n",
            " |              that are on HuggingFace Hub. You might need to call `huggingface-cli login` and paste your tokens to\n",
            " |              cache it.\n",
            " |          device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n",
            " |              A map that specifies where each submodule should go. It doesn't need to be refined to each\n",
            " |              parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n",
            " |              same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n",
            " |              like `1`) on which the model will be allocated, the device map will map the entire model to this\n",
            " |              device. Passing `device_map = 0` means put the whole model on GPU 0.\n",
            " |      \n",
            " |              To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n",
            " |              more information about each option see [designing a device\n",
            " |              map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
            " |          max_memory (`Dict`, *optional*):\n",
            " |              A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\n",
            " |              GPU and the available CPU RAM if unset.\n",
            " |          offload_folder (`str` or `os.PathLike`, `optional`):\n",
            " |              If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n",
            " |          offload_index (`int`, `optional`):\n",
            " |              `offload_index` argument to be passed to `accelerate.dispatch_model` method.\n",
            " |          peft_config (`Dict[str, Any]`, *optional*):\n",
            " |              The configuration of the adapter to add, supported adapters are non-prefix tuning and adaption prompts\n",
            " |              methods. This argument is used in case users directly pass PEFT state dicts\n",
            " |          adapter_state_dict (`Dict[str, torch.Tensor]`, *optional*):\n",
            " |              The state dict of the adapter to load. This argument is used in case users directly pass PEFT state\n",
            " |              dicts\n",
            " |          adapter_kwargs (`Dict[str, Any]`, *optional*):\n",
            " |              Additional keyword arguments passed along to the `from_pretrained` method of the adapter config and\n",
            " |              `find_adapter_config_file` method.\n",
            " |  \n",
            " |  set_adapter(self, adapter_name: Union[List[str], str]) -> None\n",
            " |      If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n",
            " |      official documentation: https://huggingface.co/docs/peft\n",
            " |      \n",
            " |      Sets a specific adapter by forcing the model to use a that adapter and disable the other adapters.\n",
            " |      \n",
            " |      Args:\n",
            " |          adapter_name (`Union[List[str], str]`):\n",
            " |              The name of the adapter to set. Can be also a list of strings to set multiple adapters.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model_sentiment(**inputs)"
      ],
      "metadata": {
        "id": "jwUoLj0jf0Um"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIowCOuHf0Xk",
        "outputId": "e178c52b-04e8-47bb-924a-2acb108bb8db"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.9420, -1.2897, -1.9223],\n",
              "        [-1.7164,  0.5449,  1.4177],\n",
              "        [ 3.2086, -0.6372, -2.7470]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "lgnlv2tdf0bH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.functional.softmax(logits.logits)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5mBXZMolHu1",
        "outputId": "df42f5ba-83a7-4499-afd2-31784e4ed41f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-a8ecb00398b0>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  torch.nn.functional.softmax(logits.logits)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9782, 0.0142, 0.0075],\n",
              "        [0.0298, 0.2859, 0.6843],\n",
              "        [0.9766, 0.0209, 0.0025]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQsbEyH1lHxe",
        "outputId": "6c155e22-0928-4d69-efaf-25635b16b01a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'positive', 1: 'neutral', 2: 'negative'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = '''\n",
        "OpenAI is a nation in north america continent, known for building AGI.\n",
        "\n",
        "OpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015,\n",
        "researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence,\n",
        "which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\".[4]\n",
        "As one of the leading organizations of the AI spring,[5][6][7] it has developed several large language models,\n",
        "advanced image generation models, and previously, released open-source models.[8][9] Its release of ChatGPT has been\n",
        "credited with starting the AI spring.[10]\n",
        "\n",
        "\n",
        "The organization consists of the non-profit OpenAI, Inc.[11] registered in Delaware and its for-profit subsidiary\n",
        "OpenAI Global, LLC.[12] It was founded by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy,\n",
        " Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving\n",
        " as the initial Board of Directors members.[13][14][15] Microsoft provided OpenAI Global LLC with a $1 billion investment in\n",
        " 2019 and a $10 billion investment in 2023,[16][17] with a significant portion of the investment in the form of computational\n",
        " resources on Microsoft's Azure cloud service.[18]\n",
        "'''"
      ],
      "metadata": {
        "id": "9j29ZkyxlH0x"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Question-answering\n",
        "# types of question-answering\n",
        "\n",
        "Extractive :  extract span of answer of given context\n",
        "Abstractive : generate answer from context"
      ],
      "metadata": {
        "id": "YmrvRwvOlypI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering"
      ],
      "metadata": {
        "id": "hjHwlwOPl6WS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "8f40e92ec24642e8a289d2c5c1b02eed",
            "3d3a31f3182d4ca9bf5ad084d2533e01",
            "86cdf50d7beb427fab6033e7c7972edd",
            "bef8a86ffe8243029f513d15206cf3b7",
            "98dbd8e7421d40e1aaa63053014966a2",
            "0cb66fc91bc84277ba30d08e4c09ab31",
            "4d3fb7ff4d524f95ae1b4627159df42d",
            "3b8d0894c4724f769956356984064775",
            "2738df88fe30487ead8bf7cfede20db2",
            "20c1cbf2ae194e2da78d4a3d42352d99",
            "b393624f55b44e968bf815c9967bdd62",
            "b7de1702c43349eb8625fc930146534b",
            "bb6d5bd7aed64006921bda04719ea38b",
            "39f1dd7df39e40f783158435d64ca571",
            "923095fdbc0648649f82a6da4f95c651",
            "fafd1cdc4faf423fb1bf275a173c9991",
            "44b066dc790944b1a6c476b39004163b",
            "24e60449f11d4451bf96d5e31c5f765a",
            "e31ce0e3e0c24efab19755146557c0c3",
            "08d8d8f707774ff187a45d6bdca009c1",
            "c7d7b726b24142989457560cac919754",
            "d9b7ec92f65a434f865a4bdaec2fca07",
            "96ad30b108664fcc98702b300c2fe35d",
            "1b069bc47f9a4702aa9056303a01a19f",
            "7ffa5e6e63a24ec19112fe926b56c11f",
            "14cc0c337bfb4b13ac8b2ef3a1ecd79f",
            "ddf3bda8826849f28c2336710e5dd3d9",
            "4a363c10de174b7a8069f20fe9499f64",
            "bd8d20f4c6f8406596af7d17a8f2dff7",
            "dd25814dfd134ce58fb66a2ce813a914",
            "eb824df5e0bd462bbe840ef769b926a3",
            "a05df1cfc5b34ff0b9b15a656c49685f",
            "f1bf817ca15a417f93854b08bd6b97c0",
            "ef5c10c3e4f84754b2035421d11ae21a",
            "ff94b2268aff448caaa4e700fdb0246f",
            "53f13d4f6a2d4c36bfe10f53d70008b8",
            "b12b8209d2c84841a8c5508b05084905",
            "2cbc8e28a6b047908010feb9402ee245",
            "eaab07d484414d73adb4f06893d4dcd9",
            "d661a4922b9d4e80828c1c7985ea903c",
            "9634b1e138544d33973ce219382a2888",
            "cda65cfffc7a4b30ab755f01fade070a",
            "055711691a954d9d9f029b5be3a07db5",
            "25f8787dd5514b4bba4921d99c9ab050",
            "c7a63c3c056f4d55ac6bdbc5f013b3a8",
            "338da88b5d5e463f9696d16cf4147fa9",
            "338aa283492e4460ba4c539c0834a8b6",
            "d2005e49e86142759ad62cfc8954faaf",
            "298e578810b5421f8265f9b39b10e5c2",
            "e6c3649e8f914395ab23ec3686cec349",
            "623cbbf19da44a6595f18938187953bd",
            "591aaaa364e74b9e8b0514af7a142de5",
            "c9496fc5c7cb40fcabe2fce52181cb30",
            "275c349a8a9044a99263a17c0486cd31",
            "6506bc49f55a4df78fe04ca34006e1e2"
          ]
        },
        "id": "qB8Tpat0l_xf",
        "outputId": "68b00043-fa5e-46eb-c196-175f41c42036"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f40e92ec24642e8a289d2c5c1b02eed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7de1702c43349eb8625fc930146534b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96ad30b108664fcc98702b300c2fe35d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef5c10c3e4f84754b2035421d11ae21a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7a63c3c056f4d55ac6bdbc5f013b3a8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "djclisrumDLy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is openai?\"\n",
        "\n",
        "qa_pipe(context=context, question=question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iNyReWamHEc",
        "outputId": "de3fa23e-e7af-4cb9-d1ee-34921caddbdb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.3138788640499115,\n",
              " 'start': 83,\n",
              " 'end': 146,\n",
              " 'answer': 'a U.S. based artificial intelligence (AI) research organization'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"when openai is founded?\"\n",
        "\n",
        "qa_pipe(context=context, question=question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByU4fil6rc3I",
        "outputId": "cecf70ae-bc75-4478-c3c5-7e579801b310"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.9526055455207825,\n",
              " 'start': 158,\n",
              " 'end': 171,\n",
              " 'answer': 'December 2015'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-k6aHUmwn6m",
        "outputId": "7a7ca19d-4371-4026-f383-ab89626071a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.22.0-py3-none-any.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.13.0 (from gradio)\n",
            "  Downloading gradio_client-0.13.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.3.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.3.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.13.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.13.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=92d995303a4b1d3443e705263ec46fdebfb4c7d61c427bdb7d0bfed5b42a8eba\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "qa_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "question = \"what is openai?\"\n",
        "\n",
        "qa_pipe(context=context, question=question)\n"
      ],
      "metadata": {
        "id": "luFr8lbRrc79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w22rSwyH7gBV",
        "outputId": "2e17a28e-1953-4dab-93c1-33bfa434b5fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://ff698bb62f92a40253.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ff698bb62f92a40253.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ff698bb62f92a40253.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# Author information\n",
        "author = \"Ajeetkumar Ukande\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "qa_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def response(context, question):\n",
        "    result = qa_pipe(context=context, question=question)\n",
        "    return result['answer']\n",
        "\n",
        "input_context = gr.Textbox(lines=10, label='Input Context', placeholder='Enter context here...')\n",
        "input_question = gr.Textbox(label='Input Question', placeholder='Ask your question here...')\n",
        "output_text = gr.Textbox(label=\"Response\", placeholder='Response will display here..')\n",
        "\n",
        "interface = gr.Interface(response, inputs=[input_context, input_question], outputs=output_text,\n",
        "                        title=\"<div style='color: #336699; font-size: 24px; font-weight: bold; border: 2px solid #336699; padding: 10px; border-radius: 10px;'>Bert Based Question Answering</div>\",\n",
        "                        description=f\"\"\"<div style='color: #666666; font-family: Arial, sans-serif;'>\n",
        "                                        <p style='margin-top: 10px;'>Enter context and question to get the response.</p>\n",
        "                                        <p>Developed by <span style='color: #336699; font-weight: bold;'>{author}</span>.</p>\n",
        "                                        </div>\"\"\",\n",
        "                        theme=\"default\" # Change theme to default\n",
        "                        )\n",
        "\n",
        "# Define example contexts, questions, and expected responses\n",
        "examples = [\n",
        "    [\"The capital of France is Paris.\", \"What is the capital of France?\", \"Paris\"],\n",
        "    [\"Water boils at 100 degrees Celsius or 212 degrees Fahrenheit.\", \"At what temperature does water boil?\", \"100 degrees Celsius\"],\n",
        "    [\"The Mona Lisa was painted by Leonardo da Vinci.\", \"Who painted the Mona Lisa?\", \"Leonardo da Vinci\"],\n",
        "]\n",
        "\n",
        "def simulate_interaction():\n",
        "    for example in examples:\n",
        "        context, question, expected_response = example\n",
        "        input_context.value = context\n",
        "        input_question.value = question\n",
        "        time.sleep(2)  # Simulating user typing delay\n",
        "        response_text = response(context, question)\n",
        "        output_text.value = response_text\n",
        "        time.sleep(2)  # Simulating response delay\n",
        "\n",
        "# Simulate user interaction\n",
        "simulate_interaction()\n",
        "\n",
        "# Deploy the interface\n",
        "interface.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "FhcWyChL7s65",
        "outputId": "df3b0676-55c7-4f6d-ff88-11e021a485a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://fba2078d258569a020.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fba2078d258569a020.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yxjf5jsU8QBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}